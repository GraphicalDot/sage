Project Path: sage

Source Tree:

```
sage
├── CODE_OF_CONDUCT.md
├── LICENSE
├── requirements.txt
├── pyproject.toml
├── tests
│   ├── test_data_manager.py
│   ├── conftest.py
│   ├── test_chunker.py
│   ├── test_github.py
│   ├── __init__.py
│   ├── assets
│   │   ├── sample-script.ts
│   │   ├── sample-script.tsx
│   │   └── sample-notebook.ipynb
│   └── test_vector_store.py
├── MANIFEST.in
├── README.md
├── setup.py
├── CONTRIBUTING.md
├── benchmarks
│   └── retrieval
│       ├── requirements.txt
│       ├── retrieve_kaggle.py
│       ├── README.md
│       ├── sample.json
│       ├── retrieve.py
│       └── assets
│           ├── markdown.png
│           ├── embeddings.png
│           ├── chunks.png
│           ├── retrievers.png
│           └── rerankers.png
├── assets
│   ├── chat_screenshot.png
│   ├── chat_screenshot2.png
│   ├── storia-logo.png
│   └── sage.gif
└── sage
    ├── config.py
    ├── reranker.py
    ├── index.py
    ├── chunker.py
    ├── vector_store.py
    ├── constants.py
    ├── retriever.py
    ├── __init__.py
    ├── llm.py
    ├── sample-exclude.txt
    ├── chat.py
    ├── configs
    │   ├── remote.yaml
    │   └── local.yaml
    ├── embedder.py
    ├── data_manager.py
    ├── github.py
    └── code_symbols.py

```

`/Users/sauravverma/programs/pyano/sage/CODE_OF_CONDUCT.md`:

```md
# Contributor Covenant Code of Conduct

## Our Pledge

We as members, contributors, and leaders pledge to make participation in our
community a harassment-free experience for everyone, regardless of age, body
size, visible or invisible disability, ethnicity, sex characteristics, gender
identity and expression, level of experience, education, socio-economic status,
nationality, personal appearance, race, religion, or sexual identity
and orientation.

We pledge to act and interact in ways that contribute to an open, welcoming,
diverse, inclusive, and healthy community.

## Our Standards

Examples of behavior that contributes to a positive environment for our
community include:

* Demonstrating empathy and kindness toward other people
* Being respectful of differing opinions, viewpoints, and experiences
* Giving and gracefully accepting constructive feedback
* Accepting responsibility and apologizing to those affected by our mistakes,
  and learning from the experience
* Focusing on what is best not just for us as individuals, but for the
  overall community

Examples of unacceptable behavior include:

* The use of sexualized language or imagery, and sexual attention or
  advances of any kind
* Trolling, insulting or derogatory comments, and personal or political attacks
* Public or private harassment
* Publishing others' private information, such as a physical or email
  address, without their explicit permission
* Other conduct which could reasonably be considered inappropriate in a
  professional setting

## Enforcement Responsibilities

Community leaders are responsible for clarifying and enforcing our standards of
acceptable behavior and will take appropriate and fair corrective action in
response to any behavior that they deem inappropriate, threatening, offensive,
or harmful.

Community leaders have the right and responsibility to remove, edit, or reject
comments, commits, code, wiki edits, issues, and other contributions that are
not aligned to this Code of Conduct, and will communicate reasons for moderation
decisions when appropriate.

## Scope

This Code of Conduct applies within all community spaces, and also applies when
an individual is officially representing the community in public spaces.
Examples of representing our community include using an official e-mail address,
posting via an official social media account, or acting as an appointed
representative at an online or offline event.

## Enforcement

Instances of abusive, harassing, or otherwise unacceptable behavior may be
reported to the community leaders responsible for enforcement at
founders@storia.ai.
All complaints will be reviewed and investigated promptly and fairly.

All community leaders are obligated to respect the privacy and security of the
reporter of any incident.

## Enforcement Guidelines

Community leaders will follow these Community Impact Guidelines in determining
the consequences for any action they deem in violation of this Code of Conduct:

### 1. Correction

**Community Impact**: Use of inappropriate language or other behavior deemed
unprofessional or unwelcome in the community.

**Consequence**: A private, written warning from community leaders, providing
clarity around the nature of the violation and an explanation of why the
behavior was inappropriate. A public apology may be requested.

### 2. Warning

**Community Impact**: A violation through a single incident or series
of actions.

**Consequence**: A warning with consequences for continued behavior. No
interaction with the people involved, including unsolicited interaction with
those enforcing the Code of Conduct, for a specified period of time. This
includes avoiding interactions in community spaces as well as external channels
like social media. Violating these terms may lead to a temporary or
permanent ban.

### 3. Temporary Ban

**Community Impact**: A serious violation of community standards, including
sustained inappropriate behavior.

**Consequence**: A temporary ban from any sort of interaction or public
communication with the community for a specified period of time. No public or
private interaction with the people involved, including unsolicited interaction
with those enforcing the Code of Conduct, is allowed during this period.
Violating these terms may lead to a permanent ban.

### 4. Permanent Ban

**Community Impact**: Demonstrating a pattern of violation of community
standards, including sustained inappropriate behavior,  harassment of an
individual, or aggression toward or disparagement of classes of individuals.

**Consequence**: A permanent ban from any sort of public interaction within
the community.

## Attribution

This Code of Conduct is adapted from the [Contributor Covenant][homepage],
version 2.0, available at
https://www.contributor-covenant.org/version/2/0/code_of_conduct.html.

Community Impact Guidelines were inspired by [Mozilla's code of conduct
enforcement ladder](https://github.com/mozilla/diversity).

[homepage]: https://www.contributor-covenant.org

For answers to common questions about this code of conduct, see the FAQ at
https://www.contributor-covenant.org/faq. Translations are available at
https://www.contributor-covenant.org/translations.

```

`/Users/sauravverma/programs/pyano/sage/LICENSE`:

```
                                 Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      "License" shall mean the terms and conditions for use, reproduction,
      and distribution as defined by Sections 1 through 9 of this document.

      "Licensor" shall mean the copyright owner or entity authorized by
      the copyright owner that is granting the License.

      "Legal Entity" shall mean the union of the acting entity and all
      other entities that control, are controlled by, or are under common
      control with that entity. For the purposes of this definition,
      "control" means (i) the power, direct or indirect, to cause the
      direction or management of such entity, whether by contract or
      otherwise, or (ii) ownership of fifty percent (50%) or more of the
      outstanding shares, or (iii) beneficial ownership of such entity.

      "You" (or "Your") shall mean an individual or Legal Entity
      exercising permissions granted by this License.

      "Source" form shall mean the preferred form for making modifications,
      including but not limited to software source code, documentation
      source, and configuration files.

      "Object" form shall mean any form resulting from mechanical
      transformation or translation of a Source form, including but
      not limited to compiled object code, generated documentation,
      and conversions to other media types.

      "Work" shall mean the work of authorship, whether in Source or
      Object form, made available under the License, as indicated by a
      copyright notice that is included in or attached to the work
      (an example is provided in the Appendix below).

      "Derivative Works" shall mean any work, whether in Source or Object
      form, that is based on (or derived from) the Work and for which the
      editorial revisions, annotations, elaborations, or other modifications
      represent, as a whole, an original work of authorship. For the purposes
      of this License, Derivative Works shall not include works that remain
      separable from, or merely link (or bind by name) to the interfaces of,
      the Work and Derivative Works thereof.

      "Contribution" shall mean any work of authorship, including
      the original version of the Work and any modifications or additions
      to that Work or Derivative Works thereof, that is intentionally
      submitted to Licensor for inclusion in the Work by the copyright owner
      or by an individual or Legal Entity authorized to submit on behalf of
      the copyright owner. For the purposes of this definition, "submitted"
      means any form of electronic, verbal, or written communication sent
      to the Licensor or its representatives, including but not limited to
      communication on electronic mailing lists, source code control systems,
      and issue tracking systems that are managed by, or on behalf of, the
      Licensor for the purpose of discussing and improving the Work, but
      excluding communication that is conspicuously marked or otherwise
      designated in writing by the copyright owner as "Not a Contribution."

      "Contributor" shall mean Licensor and any individual or Legal Entity
      on behalf of whom a Contribution has been received by Licensor and
      subsequently incorporated within the Work.

   2. Grant of Copyright License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      copyright license to reproduce, prepare Derivative Works of,
      publicly display, publicly perform, sublicense, and distribute the
      Work and such Derivative Works in Source or Object form.

   3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

   4. Redistribution. You may reproduce and distribute copies of the
      Work or Derivative Works thereof in any medium, with or without
      modifications, and in Source or Object form, provided that You
      meet the following conditions:

      (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

      (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

      (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

      (d) If the Work includes a "NOTICE" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

      You may add Your own copyright statement to Your modifications and
      may provide additional or different license terms and conditions
      for use, reproduction, or distribution of Your modifications, or
      for any such Derivative Works as a whole, provided Your use,
      reproduction, and distribution of the Work otherwise complies with
      the conditions stated in this License.

   5. Submission of Contributions. Unless You explicitly state otherwise,
      any Contribution intentionally submitted for inclusion in the Work
      by You to the Licensor shall be under the terms and conditions of
      this License, without any additional terms or conditions.
      Notwithstanding the above, nothing herein shall supersede or modify
      the terms of any separate license agreement you may have executed
      with Licensor regarding such Contributions.

   6. Trademarks. This License does not grant permission to use the trade
      names, trademarks, service marks, or product names of the Licensor,
      except as required for reasonable and customary use in describing the
      origin of the Work and reproducing the content of the NOTICE file.

   7. Disclaimer of Warranty. Unless required by applicable law or
      agreed to in writing, Licensor provides the Work (and each
      Contributor provides its Contributions) on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
      implied, including, without limitation, any warranties or conditions
      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
      PARTICULAR PURPOSE. You are solely responsible for determining the
      appropriateness of using or redistributing the Work and assume any
      risks associated with Your exercise of permissions under this License.

   8. Limitation of Liability. In no event and under no legal theory,
      whether in tort (including negligence), contract, or otherwise,
      unless required by applicable law (such as deliberate and grossly
      negligent acts) or agreed to in writing, shall any Contributor be
      liable to You for damages, including any direct, indirect, special,
      incidental, or consequential damages of any character arising as a
      result of this License or out of the use or inability to use the
      Work (including but not limited to damages for loss of goodwill,
      work stoppage, computer failure or malfunction, or any and all
      other commercial damages or losses), even if such Contributor
      has been advised of the possibility of such damages.

   9. Accepting Warranty or Additional Liability. While redistributing
      the Work or Derivative Works thereof, You may choose to offer,
      and charge a fee for, acceptance of support, warranty, indemnity,
      or other liability obligations and/or rights consistent with this
      License. However, in accepting such obligations, You may act only
      on Your own behalf and on Your sole responsibility, not on behalf
      of any other Contributor, and only if You agree to indemnify,
      defend, and hold each Contributor harmless for any liability
      incurred by, or claims asserted against, such Contributor by reason
      of your accepting any such warranty or additional liability.

   END OF TERMS AND CONDITIONS

   APPENDIX: How to apply the Apache License to your work.

      To apply the Apache License to your work, attach the following
      boilerplate notice, with the fields enclosed by brackets "[]"
      replaced with your own identifying information. (Don't include
      the brackets!)  The text should be enclosed in the appropriate
      comment syntax for the file format. We also recommend that a
      file or class name and description of purpose be included on the
      same "printed page" as the copyright notice for easier
      identification within third-party archives.

   Copyright [yyyy] [name of copyright owner]

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.

```

`/Users/sauravverma/programs/pyano/sage/requirements.txt`:

```txt
aiofiles==23.2.1
aiohappyeyeballs==2.4.4
aiohttp==3.11.11
aiolimiter==1.2.1
aiosignal==1.3.2
annotated-types==0.7.0
anthropic==0.43.0
anyio==4.8.0
anytree==2.12.1
asgiref==3.8.1
attrs==24.3.0
backoff==2.2.1
bcrypt==4.2.1
build==1.2.2.post1
cachetools==5.5.0
certifi==2024.12.14
charset-normalizer==3.4.1
chroma-hnswlib==0.7.6
chromadb==0.5.23
click==8.1.8
cohere==5.13.8
coloredlogs==15.0.1
ConfigArgParse==1.7
dataclasses-json==0.6.7
defusedxml==0.7.1
Deprecated==1.2.15
dill==0.3.9
distro==1.9.0
durationpy==0.9
faiss-cpu==1.9.0.post1
fastapi==0.115.6
fastavro==1.10.0
fastjsonschema==2.21.1
ffmpy==0.5.0
filelock==3.16.1
filetype==1.2.0
flatbuffers==24.12.23
frozenlist==1.5.0
fsspec==2024.12.0
gitdb==4.0.12
GitPython==3.1.44
google-ai-generativelanguage==0.6.10
google-api-core==2.24.0
google-api-python-client==2.159.0
google-auth==2.37.0
google-auth-httplib2==0.2.0
google-generativeai==0.8.3
googleapis-common-protos==1.66.0
gradio==5.12.0
gradio_client==1.5.4
grpcio==1.69.0
grpcio-status==1.69.0
grpcio-tools==1.69.0
h11==0.14.0
h2==4.1.0
hpack==4.0.0
httpcore==1.0.7
httplib2==0.22.0
httptools==0.6.4
httpx==0.27.2
httpx-sse==0.4.0
huggingface-hub==0.27.1
humanfriendly==10.0
hyperframe==6.0.1
idna==3.10
importlib_metadata==8.5.0
importlib_resources==6.5.2
Jinja2==3.1.5
jiter==0.8.2
joblib==1.4.2
jsonpatch==1.33
jsonpointer==3.0.0
jsonschema==4.23.0
jsonschema-specifications==2024.10.1
jupyter_core==5.7.2
kubernetes==31.0.0
langchain==0.3.14
langchain-anthropic==0.3.1
langchain-chroma==0.2.0
langchain-cohere==0.3.4
langchain-community==0.3.14
langchain-core==0.3.29
langchain-experimental==0.3.4
langchain-google-genai==2.0.8
langchain-milvus==0.1.8
langchain-nvidia-ai-endpoints==0.3.7
langchain-ollama==0.2.2
langchain-openai==0.3.0
langchain-qdrant==0.2.0
langchain-text-splitters==0.3.5
langchain-together==0.3.0
langchain-voyageai==0.1.4
langsmith==0.2.10
Levenshtein==0.26.1
markdown-it-py==3.0.0
MarkupSafe==2.1.5
marqo==3.9.2
marshmallow==3.25.1
mdurl==0.1.2
milvus-lite==2.4.11
mmh3==4.1.0
monotonic==1.6
mpire==2.10.2
mpmath==1.3.0
multidict==6.1.0
multiprocess==0.70.17
mypy-extensions==1.0.0
nbformat==5.10.4
networkx==3.4.2
nltk==3.9.1
numpy==1.26.4
oauthlib==3.2.2
ollama==0.4.6
onnxruntime==1.20.1
openai==1.59.7
opentelemetry-api==1.29.0
opentelemetry-exporter-otlp-proto-common==1.29.0
opentelemetry-exporter-otlp-proto-grpc==1.29.0
opentelemetry-instrumentation==0.50b0
opentelemetry-instrumentation-asgi==0.50b0
opentelemetry-instrumentation-fastapi==0.50b0
opentelemetry-proto==1.29.0
opentelemetry-sdk==1.29.0
opentelemetry-semantic-conventions==0.50b0
opentelemetry-util-http==0.50b0
orjson==3.10.14
overrides==7.7.0
packaging==24.2
pandas==2.2.3
parameterized==0.9.0
pillow==10.4.0
pinecone==5.4.2
pinecone-plugin-inference==3.1.0
pinecone-plugin-interface==0.0.7
pinecone-text==0.9.0
platformdirs==4.3.6
portalocker==2.10.1
posthog==3.8.3
propcache==0.2.1
proto-plus==1.25.0
protobuf==5.29.3
pyasn1==0.6.1
pyasn1_modules==0.4.1
pydantic==2.10.5
pydantic-settings==2.7.1
pydantic_core==2.27.2
pydub==0.25.1
Pygments==2.19.1
pymilvus==2.5.3
pyparsing==3.2.1
PyPika==0.48.9
pyproject_hooks==1.2.0
python-dateutil==2.9.0.post0
python-dotenv==1.0.1
python-Levenshtein==0.26.1
python-multipart==0.0.20
pytz==2024.2
PyYAML==6.0.2
qdrant-client==1.12.2
RapidFuzz==3.11.0
referencing==0.35.1
regex==2024.11.6
requests==2.32.3
requests-oauthlib==2.0.0
requests-toolbelt==1.0.0
rich==13.9.4
rpds-py==0.22.3
rsa==4.9
ruff==0.9.1
safehttpx==0.1.6
safetensors==0.5.2
scikit-learn==1.6.1
scipy==1.15.1
semantic-version==2.10.0
semchunk==3.0.1
sentence-transformers==3.3.1
setuptools==75.8.0
shellingham==1.5.4
six==1.17.0
smmap==5.0.2
sniffio==1.3.1
SQLAlchemy==2.0.37
starlette==0.41.3
sympy==1.13.1
tabulate==0.9.0
tenacity==9.0.0
threadpoolctl==3.5.0
tiktoken==0.8.0
tokenizers==0.21.0
tomlkit==0.13.2
torch==2.5.1
tqdm==4.67.1
traitlets==5.14.3
transformers==4.48.0
tree-sitter==0.23.2
tree-sitter-c-sharp==0.23.1
tree-sitter-embedded-template==0.23.2
tree-sitter-language-pack==0.2.0
tree-sitter-php==0.23.11
tree-sitter-typescript==0.23.2
tree-sitter-xml==0.7.0
tree-sitter-yaml==0.7.0
typer==0.15.1
types-requests==2.32.0.20241016
typing-inspect==0.9.0
typing_extensions==4.12.2
tzdata==2024.2
ujson==5.10.0
uritemplate==4.1.1
urllib3==2.3.0
uvicorn==0.34.0
uvloop==0.21.0
voyageai==0.3.2
watchfiles==1.0.4
websocket-client==1.8.0
websockets==14.1
wget==3.2
wrapt==1.17.2
yarl==1.18.3
zipp==3.21.0

```

`/Users/sauravverma/programs/pyano/sage/pyproject.toml`:

```toml
[build-system]
requires = ["setuptools>=42", "wheel"]
build-backend = "setuptools.build_meta"

[tool.black]
line-length = 120

[project]
name = "storia-sage"
version = "0.1.0"
description = "A library to index a code repository and chat with it via LLMs."
readme = "README.md"
requires-python = ">=3.9,<=3.13"
authors = [
    { name = "Julia Turc", email = "founders@storia.ai" },
]
classifiers = [
    "Programming Language :: Python :: 3",
    "License :: OSI Approved :: MIT License",
    "Operating System :: OS Independent"
]
dependencies = [
    "GitPython==3.1.43",
    "Pygments==2.18.0",
    "anytree==2.12.1",
    "cohere==5.9.2",
    "configargparse",
    "faiss-cpu==1.9.0",
    "fastapi==0.112.2",
    "google-ai-generativelanguage==0.6.6",
    "gradio>=4.26.0",
    "langchain==0.2.16",
    "langchain-anthropic==0.1.23",
    "langchain-cohere==0.2.4",
    "langchain-community==0.2.17",
    "langchain-core==0.2.41",
    "langchain-experimental==0.0.65",
    "langchain-google-genai",
    "langchain-nvidia-ai-endpoints==0.2.2",
    "langchain-ollama==0.1.3",
    "langchain-openai==0.1.25",
    "langchain-text-splitters==0.2.4",
    "langchain-voyageai==0.1.1",
    "langchain-milvus==0.1.6",
    "langchain-chroma==0.1.4",
    "langchain-qdrant==0.1.4",
    "marqo==3.7.0",
    "nbformat==5.10.4",
    "openai==1.42.0",
    "pinecone==5.0.1",
    "pinecone-text==0.9.0",
    "python-dotenv==1.0.1",
    "python-Levenshtein==0.26.0",
    "requests==2.32.3",
    "semchunk==2.2.0",
    "sentence-transformers==3.1.0",
    "tenacity==8.5.0",
    "tiktoken==0.7.0",
    "tokenizers==0.19.1",
    "transformers==4.44.2",
    "tree-sitter==0.22.3",
    "tree-sitter-typescript==0.21.2",
    "tree-sitter-language-pack==0.2.0",
    "voyageai==0.2.3",
    "setuptools"  # Added from the setup.py install_requires
]

[project.optional-dependencies]
dev = ["black"]

[project.scripts]
sage-index = "sage.index:main"
sage-chat = "sage.chat:main"

[tool.setuptools.package-data]
"sage" = ["sample-exclude.txt"]

[tool.setuptools.packages]
find = {}

```

`/Users/sauravverma/programs/pyano/sage/tests/test_data_manager.py`:

```py
""" 
This section does the following:

Here, one can find a set of unit tests for GitHubRepoManager class from sage.data_manager module. These tests made use of unit testing in python with unittest framework and mock patching to reinforce several scenarios and correct functioning of the different methods of the GitHubRepoManager. The following methods were focused on testing:

1. In the test_download_clone_success method, the ability to successfully clone a github repo by using the clone_from method is explored.
2. In the test_is_public_repository method, the visibility of the repository is determined by mimicking a successful Call to GitHub API for the isPublic field.
3. In the test_is_private_repository method, the availability status of a repository as private is also tested by impersonating a 404 return status of the github API.
4. In the test_default_branch method, the possession of the repository which provides the default branch is confirmed.
5. In the test_parse_filter_file method, the ability of the program to read the rules and whether any including or excluding rules are appropriately set in the filter file is verified.
6. In the test_walk_included_files method, the replication of walking through the file structure of the repository was useful particularly in ensuring the returned files are exclusively restricted to included files.
7. In the test_read_file method, the functionality of the method in reading the appropriate uniform resource locator is checked to ensure the contents were well read.
8. In the test_create_log_directories method, the implementation of the method that ensures all the log files are created in the right directories in the repository. The test suite is designed with controlled isolation of the methods under test by means of different mock objects to ensure

"""

import os
import unittest
from unittest.mock import MagicMock, patch

from sage.data_manager import GitHubRepoManager


class TestGitHubRepoManager(unittest.TestCase):
    @patch("git.Repo.clone_from")
    def test_download_clone_success(self, mock_clone):
        """Test the download() method of GitHubRepoManager by mocking the cloning process."""
        repo_manager = GitHubRepoManager(repo_id="Storia-AI/sage", local_dir="/tmp/test_repo")
        mock_clone.return_value = MagicMock()
        result = repo_manager.download()
        mock_clone.assert_called_once_with(
            "https://github.com/Storia-AI/sage.git", "/tmp/test_repo/Storia-AI/sage", depth=1, single_branch=True
        )
        self.assertTrue(result)

    @patch("sage.data_manager.requests.get")
    def test_is_public_repository(self, mock_get):
        """Test the is_public property to check if a repository is public."""
        mock_get.return_value.status_code = 200
        repo_manager = GitHubRepoManager(repo_id="Storia-AI/sage")
        self.assertTrue(repo_manager.is_public)
        mock_get.assert_called_once_with("https://api.github.com/repos/Storia-AI/sage", timeout=10)

    @patch("sage.data_manager.requests.get")
    def test_is_private_repository(self, mock_get):
        """Test the is_public property to check if a repository is private."""
        mock_get.return_value.status_code = 404
        repo_manager = GitHubRepoManager(repo_id="Storia-AI/sage")
        self.assertFalse(repo_manager.is_public)
        mock_get.assert_called_once_with("https://api.github.com/repos/Storia-AI/sage", timeout=10)

    @patch("sage.data_manager.requests.get")
    def test_default_branch(self, mock_get):
        """Test the default_branch property to fetch the default branch of the repository."""
        mock_get.return_value.status_code = 200
        mock_get.return_value.json.return_value = {"default_branch": "main"}
        repo_manager = GitHubRepoManager(repo_id="Storia-AI/sage")
        self.assertEqual(repo_manager.default_branch, "main")
        mock_get.assert_called_once_with(
            "https://api.github.com/repos/Storia-AI/sage", headers={"Accept": "application/vnd.github.v3+json"}
        )

    @patch("builtins.open", new_callable=unittest.mock.mock_open, read_data="ext:.py\nfile:test.py\ndir:test_dir\n")
    def test_parse_filter_file(self, mock_file):
        """Test the _parse_filter_file method for correct parsing of inclusion/exclusion files."""
        repo_manager = GitHubRepoManager(repo_id="Storia-AI/sage", inclusion_file="dummy_path")
        expected = {"ext": [".py"], "file": ["test.py"], "dir": ["test_dir"]}
        result = repo_manager._parse_filter_file("dummy_path")
        self.assertEqual(result, expected)

    @patch("os.path.exists")
    @patch("os.remove")
    @patch("builtins.open", new_callable=unittest.mock.mock_open, read_data="dummy content")
    def test_walk_included_files(self, mock_open, mock_remove, mock_exists):
        """Test the walk method to ensure it only includes specified files."""
        mock_exists.return_value = True
        repo_manager = GitHubRepoManager(repo_id="Storia-AI/sage", local_dir="/tmp/test_repo")
        with patch(
            "os.walk",
            return_value=[
                ("/tmp/test_repo", ("subdir",), ("included_file.py", "excluded_file.txt")),
            ],
        ):
            included_files = list(repo_manager.walk())
            print("Included files:", included_files)
            self.assertTrue(any(file[1]["file_path"] == "included_file.py" for file in included_files))

    def test_read_file(self):
        """Test the read_file method to read the content of a file."""
        mock_file_path = "/tmp/test_repo/test_file.txt"
        with patch("builtins.open", new_callable=unittest.mock.mock_open, read_data="Hello, World!"):
            repo_manager = GitHubRepoManager(repo_id="Storia-AI/sage", local_dir="/tmp/test_repo")
            content = repo_manager.read_file("test_file.txt")
            self.assertEqual(content, "Hello, World!")

    @patch("os.makedirs")
    def test_create_log_directories(self, mock_makedirs):
        """Test that log directories are created."""
        repo_manager = GitHubRepoManager(repo_id="Storia-AI/sage", local_dir="/tmp/test_repo")

        with self.assertRaises(AttributeError):
            repo_manager.create_log_directories()


if __name__ == "main":
    unittest.main()

```

`/Users/sauravverma/programs/pyano/sage/tests/conftest.py`:

```py
import os
import sys

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "../sage")))

```

`/Users/sauravverma/programs/pyano/sage/tests/test_chunker.py`:

```py
"""Unit tests for the classes under chunker.py.

These are minimal happy-path tests to ensure that the chunkers don't crash.

Dependencies:
pip install pytest
pip install pytest-mock
"""

import os

from pytest import mark, param

import sage.chunker


def test_text_chunker_happy_path():
    """Tests the happy path for the TextFileChunker."""
    chunker = sage.chunker.TextFileChunker(max_tokens=100)

    file_path = os.path.join(os.path.dirname(__file__), "../README.md")
    with open(file_path, "r") as file:
        content = file.read()
    metadata = {"file_path": file_path}
    chunks = chunker.chunk(content, metadata)

    assert len(chunks) >= 1


def test_code_chunker_happy_path():
    """Tests the happy path for the CodeFileChunker."""
    chunker = sage.chunker.CodeFileChunker(max_tokens=100)

    file_path = os.path.join(os.path.dirname(__file__), "../sage/chunker.py")
    with open(file_path, "r") as file:
        content = file.read()
    metadata = {"file_path": file_path}
    chunks = chunker.chunk(content, metadata)

    assert len(chunks) >= 1


@mark.parametrize("filename", [param("assets/sample-script.ts"), param("assets/sample-script.tsx")])
def test_code_chunker_typescript_happy_path(filename):
    """Tests the happy path for the CodeFileChunker on .ts and .tsx files."""
    file_path = os.path.join(os.path.dirname(__file__), filename)
    with open(file_path, "r") as file:
        content = file.read()
    metadata = {"file_path": file_path}

    chunker = sage.chunker.CodeFileChunker(max_tokens=100)
    chunks = chunker.chunk(content, metadata)

    assert len(chunks) >= 1


def test_ipynb_chunker_happy_path():
    """Tests the happy path for the IPynbChunker."""
    code_chunker = sage.chunker.CodeFileChunker(max_tokens=100)
    chunker = sage.chunker.IpynbFileChunker(code_chunker)

    file_path = os.path.join(os.path.dirname(__file__), "assets/sample-notebook.ipynb")
    with open(file_path, "r") as file:
        content = file.read()
    metadata = {"file_path": file_path}
    chunks = chunker.chunk(content, metadata)

    assert len(chunks) >= 1

```

`/Users/sauravverma/programs/pyano/sage/tests/test_github.py`:

```py
"""
Unit tests for the classes under github.py.

These tests ensure the functionality of GitHub issue management,
issue chunking, and issue comments, including happy-path scenarios.

Dependencies:
pip install pytest
pip install pytest-mock
"""

from unittest.mock import MagicMock, patch

import pytest
import requests

from sage.github import (
    GitHubIssue,
    GitHubIssueComment,
    GitHubIssuesChunker,
    GitHubIssuesManager,
    IssueChunk,
)


class TestGitHubIssuesManager:

    @pytest.fixture(autouse=True)
    def setup_method(self):
        """Fixture to create a GitHubIssuesManager instance for each test."""
        self.github_manager = GitHubIssuesManager(repo_id="random/random-repo", access_token="fake_token")

    @staticmethod
    def mock_issue_response():
        """A mock response for GitHub issues."""
        return MagicMock(
            json=lambda: [
                {
                    "url": "https://api.github.com/repos/random/random-repo/issues/1",
                    "html_url": "https://github.com/random/random-repo/issues/1",
                    "title": "Found a bug",
                    "body": "I'm having a problem with this.",
                    "comments_url": "https://api.github.com/repos/random/random-repo/issues/1/comments",
                    "comments": 2,
                }
            ]
        )

    @staticmethod
    def mock_comment_response():
        """Create a mock response for GitHub issue comments."""
        return MagicMock(
            json=lambda: [
                {
                    "url": "https://api.github.com/repos/random/random-repo/issues/comments/1",
                    "html_url": "https://github.com/random/random-repo/issues/comments/1",
                    "body": "This is a comment.",
                }
            ]
        )

    @patch("github.requests.get")
    def test_download_issues(self, mock_get):
        """Test the download of issues from GitHub."""
        mock_get.side_effect = [self.mock_issue_response(), self.mock_comment_response()]

        self.github_manager.download()

        assert len(self.github_manager.issues) == 1
        assert self.github_manager.issues[0].title == "Found a bug"
        assert self.github_manager.issues[0].body == "I'm having a problem with this."
        assert self.github_manager.issues[0].url == "https://api.github.com/repos/random/random-repo/issues/1"

    @patch("github.requests.get")
    def test_walk_issues(self, mock_get):
        """Test the walking through downloaded issues."""
        self.github_manager.issues = [
            GitHubIssue(url="issue_url", html_url="html_issue_url", title="Test Issue", body="Test Body", comments=[]),
            GitHubIssue(
                url="issue_url_2",
                html_url="html_issue_url_2",
                title="Another Test Issue",
                body="Another Test Body",
                comments=[],
            ),
        ]

        issues = list(self.github_manager.walk())

        assert len(issues) == 2
        assert issues[0][0].title == "Test Issue"
        assert issues[1][0].title == "Another Test Issue"

    @patch("github.requests.get")
    def test_get_page_of_issues(self, mock_get):
        """Test fetching a page of issues."""
        mock_response = MagicMock()
        mock_response.json.return_value = [
            {
                "url": "https://api.github.com/repos/random/random-repo/issues/1",
                "html_url": "https://github.com/random/random-repo/issues/1",
                "title": "Found a bug",
                "body": "I'm having a problem with this.",
                "comments_url": "https://api.github.com/repos/random/random-repo/issues/1/comments",
                "comments": 2,
            }
        ]
        mock_get.return_value = mock_response

        issues = self.github_manager._get_page_of_issues(
            "https://api.github.com/repos/random/random-repo/issues?page=1"
        ).json()

        assert len(issues) == 1

    @patch("github.requests.get")
    def test_get_comments(self, mock_get):
        """Test retrieving comments for an issue."""
        mock_get.return_value.json.return_value = self.mock_comment_response().json()

        comments = self.github_manager._get_comments("comments_url")
        assert len(comments) == 1
        assert comments[0].body == "This is a comment."

    def test_chunker(self):
        """Test chunking of an issue into smaller parts."""
        issue = GitHubIssue(
            url="issue_url",
            html_url="html_issue_url",
            title="Test Issue",
            body="This is a long body of the issue that needs to be chunked.",
            comments=[
                GitHubIssueComment(url="comment_url_1", html_url="html_comment_url_1", body="First comment."),
                GitHubIssueComment(url="comment_url_2", html_url="html_comment_url_2", body="Second comment."),
            ],
        )

        chunker = GitHubIssuesChunker(max_tokens=50)
        chunks = chunker.chunk(content=issue, metadata={})

        assert len(chunks) > 0
        assert all(isinstance(chunk, IssueChunk) for chunk in chunks)
        assert chunks[0].issue.title == "Test Issue"
        assert chunks[0].start_comment == 0


class TestGitHubIssueComment:

    def test_initialization(self):
        """Test the initialization of the GitHubIssueComment class."""
        comment = GitHubIssueComment(url="comment_url", html_url="html_comment_url", body="Sample comment")
        assert comment.url == "comment_url"
        assert comment.html_url == "html_comment_url"
        assert comment.body == "Sample comment"

    def test_pretty_property(self):
        """Test the pretty property of the GitHubIssueComment class."""
        comment = GitHubIssueComment(url="comment_url", html_url="html_comment_url", body="Sample comment")
        assert comment.pretty == "## Comment: Sample comment"


class TestGitHubIssue:

    def test_initialization(self):
        """Test the initialization of the GitHubIssue class."""
        issue = GitHubIssue(
            url="issue_url", html_url="html_issue_url", title="Test Issue", body="Test Body", comments=[]
        )
        assert issue.url == "issue_url"
        assert issue.html_url == "html_issue_url"
        assert issue.title == "Test Issue"
        assert issue.body == "Test Body"
        assert issue.comments == []

    def test_pretty_property(self):
        """Test the pretty property of the GitHubIssue class."""
        issue = GitHubIssue(
            url="issue_url", html_url="html_issue_url", title="Test Issue", body="Test Body", comments=[]
        )
        assert issue.pretty == "# Issue: Test Issue\nTest Body"


class TestIssueChunk:

    def test_initialization(self):
        """Test the initialization of the IssueChunk class."""
        issue = GitHubIssue(
            url="issue_url", html_url="html_issue_url", title="Test Issue", body="Test Body", comments=[]
        )
        chunk = IssueChunk(issue=issue, start_comment=0, end_comment=1)
        assert chunk.issue == issue
        assert chunk.start_comment == 0
        assert chunk.end_comment == 1

    def test_content_property(self):
        """Test the content property of the IssueChunk class."""
        issue = GitHubIssue(
            url="issue_url", html_url="html_issue_url", title="Test Issue", body="Test Body", comments=[]
        )
        chunk = IssueChunk(issue=issue, start_comment=0, end_comment=1)
        assert chunk.content == "# Issue: Test Issue\nTest Body\n\n"

    def test_metadata_property(self):
        """Test the metadata property of the IssueChunk class."""
        issue = GitHubIssue(
            url="issue_url", html_url="html_issue_url", title="Test Issue", body="Test Body", comments=[]
        )
        chunk = IssueChunk(issue=issue, start_comment=0, end_comment=1)
        expected_metadata = {
            "id": "html_issue_url_0_1",
            "url": "html_issue_url",
            "start_comment": 0,
            "end_comment": 1,
            "text": "# Issue: Test Issue\nTest Body\n\n",
        }
        assert chunk.metadata == expected_metadata

    def test_num_tokens_property(self):
        """Test the num_tokens property of the IssueChunk class."""
        issue = GitHubIssue(
            url="issue_url", html_url="html_issue_url", title="Test Issue", body="This is a test body.", comments=[]
        )
        chunk = IssueChunk(issue=issue, start_comment=0, end_comment=1)
        assert chunk.num_tokens == 12


class TestGitHubIssuesChunker:

    def test_initialization(self):
        """Test the initialization of the GitHubIssuesChunker class."""
        chunker = GitHubIssuesChunker(max_tokens=50)
        assert chunker.max_tokens == 50

    def test_chunk_method(self):
        """Test the chunk method of the GitHubIssuesChunker class."""
        issue = GitHubIssue(
            url="issue_url",
            html_url="html_issue_url",
            title="Test Issue",
            body="This is a long body of the issue that needs to be chunked.",
            comments=[],
        )

        chunker = GitHubIssuesChunker(max_tokens=50)
        chunks = chunker.chunk(content=issue, metadata={})

        assert len(chunks) > 0

        assert all(isinstance(chunk, IssueChunk) for chunk in chunks)


if __name__ == "__main__":
    pytest.main()

```

`/Users/sauravverma/programs/pyano/sage/tests/assets/sample-script.ts`:

```ts
function bubbleSort(arr: number[]): number[] {
    let n = arr.length;
    let swapped: boolean;

    // Outer loop for traversing the array
    for (let i = 0; i < n; i++) {
        swapped = false;

        // Inner loop for comparing adjacent elements
        for (let j = 0; j < n - i - 1; j++) {
            if (arr[j] > arr[j + 1]) {
                // Swap the elements if they are in the wrong order
                let temp = arr[j];
                arr[j] = arr[j + 1];
                arr[j + 1] = temp;
                swapped = true;
            }
        }

        // If no elements were swapped in the inner loop, break out of the loop
        if (!swapped) {
            break;
        }
    }

    return arr;
}

// Example usage
let arr = [64, 34, 25, 12, 22, 11, 90];
console.log("Sorted array:", bubbleSort(arr));

function mergeSort(arr: number[]): number[] {
    // Base case: if the array has only one element or is empty, return it
    if (arr.length <= 1) {
        return arr;
    }

    // Find the middle point of the array
    const middle = Math.floor(arr.length / 2);

    // Divide the array into left and right halves
    const left = arr.slice(0, middle);
    const right = arr.slice(middle);

    // Recursively sort both halves and then merge them
    return merge(mergeSort(left), mergeSort(right));
}

// Function to merge two sorted arrays
function merge(left: number[], right: number[]): number[] {
    let resultArray: number[] = [];
    let leftIndex = 0;
    let rightIndex = 0;

    // Compare the elements in the left and right arrays and merge them in sorted order
    while (leftIndex < left.length && rightIndex < right.length) {
        if (left[leftIndex] < right[rightIndex]) {
            resultArray.push(left[leftIndex]);
            leftIndex++;
        } else {
            resultArray.push(right[rightIndex]);
            rightIndex++;
        }
    }

    // Concatenate any remaining elements in the left or right arrays
    return resultArray.concat(left.slice(leftIndex)).concat(right.slice(rightIndex));
}

// Example usage
let arr2 = [38, 27, 43, 3, 9, 82, 10];
console.log("Sorted array:", mergeSort(arr2));


```

`/Users/sauravverma/programs/pyano/sage/tests/assets/sample-script.tsx`:

```tsx
import React, { useState } from 'react';

// Define the types for the props
interface MyComponentProps {
  title: string;
  subtitle?: string; // Optional prop
}

const MyComponent: React.FC<MyComponentProps> = ({ title, subtitle }) => {
  // Define a state variable with an initial value
  const [count, setCount] = useState<number>(0);

  // Function to handle button click
  const handleButtonClick = () => {
    setCount(count + 1);
  };

  return (
    <div style={{ padding: '20px', border: '1px solid #ccc', borderRadius: '8px' }}>
      <h1>{title}</h1>
      {subtitle && <h2>{subtitle}</h2>}
      <p>Current count: {count}</p>
      <button onClick={handleButtonClick}>Increase count</button>
    </div>
  );
};

export default MyComponent;

```

`/Users/sauravverma/programs/pyano/sage/tests/assets/sample-notebook.ipynb`:

```ipynb
{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5norOZI0mA6s"
      },
      "outputs": [],
      "source": [
        "# Copyright 2023 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNPE46X8mJj4"
      },
      "source": [
        "# Use Retrieval Augmented Generation (RAG) with Codey APIs\n",
        "\n",
        "<table align=\"left\">\n",
        "\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/language/code/code_retrieval_augmented_generation.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Flanguage%2Fcode%2Fcode_retrieval_augmented_generation.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://cloud.google.com/ml-engine/images/colab-enterprise-logo-32px.png\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/language/code/code_retrieval_augmented_generation.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/language/code/code_retrieval_augmented_generation.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrLtlKPFqSxB"
      },
      "source": [
        "| | |\n",
        "|-|-|\n",
        "|Author(s) | [Lavi Nigam](https://github.com/lavinigam-gcp), [Polong Lin](https://github.com/polong-lin) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNAEdYNFmQcP"
      },
      "source": [
        "### Objective\n",
        "\n",
        "This notebook demonstrates how you augment output from Gemini APIs by bringing in external knowledge. An example is provided using Code Retrieval Augmented Generation(RAG) pattern using [Google Cloud's Generative AI github repository](https://github.com/GoogleCloudPlatform/generative-ai) as external knowledge. The notebook uses [Vertex AI Gemini API](https://ai.google.dev/gemini-api), [Embeddings for Text API](https://cloud.google.com/vertex-ai/docs/generative-ai/embeddings/get-text-embeddings), FAISS vector store and [LangChain 🦜️🔗](https://python.langchain.com/en/latest/).\n",
        "\n",
        "### Overview\n",
        "\n",
        "Here is overview of what we'll go over.\n",
        "\n",
        "Index Creation:\n",
        "\n",
        "1. Recursively list the files(.ipynb) in github repo\n",
        "2. Extract code and markdown from the files\n",
        "3. Chunk & generate embeddings for each code strings and add initialize the vector store\n",
        "\n",
        "Runtime:\n",
        "\n",
        "4. User enters a prompt or asks a question as a prompt\n",
        "5. Try zero-shot prompt\n",
        "6. Run prompt using RAG Chain & compare results.To generate response we use **gemini-1.5-pro**\n",
        "\n",
        "### Cost\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "- Vertex AI Gemini APIs offered by Google Cloud\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing) and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage.\n",
        "\n",
        "**Note:** We are using local vector store(FAISS) for this example however recommend managed highly scalable vector store for production usage such as [Vertex AI Vector Search](https://cloud.google.com/vertex-ai/docs/vector-search/overview) or [AlloyDB for PostgreSQL](https://cloud.google.com/alloydb/docs/ai/work-with-embeddings) or [Cloud SQL for PostgreSQL](https://cloud.google.com/sql/docs/postgres/features)  using pgvector extension."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cab0c8509c9"
      },
      "source": [
        "## Get started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b56b5a5d28c1"
      },
      "source": [
        "### Install Vertex AI SDK for Python and other required packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHaqV20Csqkt"
      },
      "outputs": [],
      "source": [
        "!pip3 install --upgrade --user -q google-cloud-aiplatform \\\n",
        "                                langchain \\\n",
        "                                langchain_google_vertexai \\\n",
        "                                langchain-community \\\n",
        "                                faiss-cpu \\\n",
        "                                nbformat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VUWOgz6M1rZ"
      },
      "source": [
        "### Restart runtime (Colab only)\n",
        "\n",
        "To use the newly installed packages, you must restart the runtime on Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIS8EYgkMy8T"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0af13c10a26a"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>⚠️ The kernel is going to restart. Wait until it's finished before continuing to the next step. ⚠️</b>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZcP9WBENG0e"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "Authenticate your environment on Google Colab.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1S_HgQXQNcbz"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVmxMr43Nhoo"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-Tljm5asMBc"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from typing import List, Optional\n",
        "\n",
        "from google.cloud import aiplatform\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema.document import Document\n",
        "from langchain.text_splitter import Language, RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "# LangChain\n",
        "from langchain_google_vertexai import VertexAI, VertexAIEmbeddings\n",
        "import nbformat\n",
        "import requests\n",
        "\n",
        "# Vertex AI\n",
        "import vertexai\n",
        "\n",
        "# Print the version of Vertex AI SDK for Python\n",
        "print(f\"Vertex AI SDK version: {aiplatform.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f872cd812d0"
      },
      "source": [
        "### Set Google Cloud project information and initialize Vertex AI SDK for Python\n",
        "\n",
        "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com). Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNGEcBKG0iK-"
      },
      "outputs": [],
      "source": [
        "# Initialize project\n",
        "# Define project information\n",
        "PROJECT_ID = \"YOUR_PROJECT_ID\"  # @param {type:\"string\"}\n",
        "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
        "\n",
        "# Code Generation\n",
        "code_llm = VertexAI(\n",
        "    model_name=\"gemini-1.5-pro\",\n",
        "    max_output_tokens=2048,\n",
        "    temperature=0.1,\n",
        "    verbose=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o537exyZk9DI"
      },
      "source": [
        "Next we need to create a GitHub personal token to be able to list all files in a repository.\n",
        "\n",
        "- Follow [this link](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens) to create GitHub token with repo->public_repo scope and update `GITHUB_TOKEN` variable below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bt9IVDSqk7y4"
      },
      "outputs": [],
      "source": [
        "# provide GitHub personal access token\n",
        "GITHUB_TOKEN = \"YOUR_GITHUB_TOKEN\"  # @param {type:\"string\"}\n",
        "GITHUB_REPO = \"GoogleCloudPlatform/generative-ai\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqq3GeEbOJbU"
      },
      "source": [
        "# Index Creation\n",
        "\n",
        "We use the Google Cloud Generative AI github repository as the data source. First list all Jupyter Notebook files in the repo and store it in a text file.\n",
        "\n",
        "You can skip this step(#1) if you have executed it once and generated the output text file.\n",
        "\n",
        "### 1. Recursively list the files(.ipynb) in the github repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTA1Jt0uOX8y"
      },
      "outputs": [],
      "source": [
        "# Crawls a GitHub repository and returns a list of all ipynb files in the repository\n",
        "def crawl_github_repo(url: str, is_sub_dir: bool, access_token: str = GITHUB_TOKEN):\n",
        "    ignore_list = [\"__init__.py\"]\n",
        "\n",
        "    if not is_sub_dir:\n",
        "        api_url = f\"https://api.github.com/repos/{url}/contents\"\n",
        "\n",
        "    else:\n",
        "        api_url = url\n",
        "\n",
        "    headers = {\n",
        "        \"Accept\": \"application/vnd.github.v3+json\",\n",
        "        \"Authorization\": f\"Bearer {access_token}\",\n",
        "    }\n",
        "\n",
        "    response = requests.get(api_url, headers=headers)\n",
        "    response.raise_for_status()  # Check for any request errors\n",
        "\n",
        "    files = []\n",
        "\n",
        "    contents = response.json()\n",
        "\n",
        "    for item in contents:\n",
        "        if (\n",
        "            item[\"type\"] == \"file\"\n",
        "            and item[\"name\"] not in ignore_list\n",
        "            and (item[\"name\"].endswith(\".py\") or item[\"name\"].endswith(\".ipynb\"))\n",
        "        ):\n",
        "            files.append(item[\"html_url\"])\n",
        "        elif item[\"type\"] == \"dir\" and not item[\"name\"].startswith(\".\"):\n",
        "            sub_files = crawl_github_repo(item[\"url\"], True)\n",
        "            time.sleep(0.1)\n",
        "            files.extend(sub_files)\n",
        "\n",
        "    return files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vaKaxcGO_R6"
      },
      "outputs": [],
      "source": [
        "code_files_urls = crawl_github_repo(GITHUB_REPO, False, GITHUB_TOKEN)\n",
        "\n",
        "# Write list to a file so you do not have to download each time\n",
        "with open(\"code_files_urls.txt\", \"w\") as f:\n",
        "    for item in code_files_urls:\n",
        "        f.write(item + \"\\n\")\n",
        "\n",
        "len(code_files_urls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5hoNYJ5byMJ"
      },
      "outputs": [],
      "source": [
        "code_files_urls[0:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFNVieLnR8Ie"
      },
      "source": [
        "### 2. Extract code from the Jupyter notebooks.\n",
        "\n",
        "You could also include .py file, shell scripts etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZsM1M4hn4cBu"
      },
      "outputs": [],
      "source": [
        "# Extracts the python code from an ipynb file from github\n",
        "def extract_python_code_from_ipynb(github_url, cell_type=\"code\"):\n",
        "    raw_url = github_url.replace(\"github.com\", \"raw.githubusercontent.com\").replace(\n",
        "        \"/blob/\", \"/\"\n",
        "    )\n",
        "\n",
        "    response = requests.get(raw_url)\n",
        "    response.raise_for_status()  # Check for any request errors\n",
        "\n",
        "    notebook_content = response.text\n",
        "\n",
        "    notebook = nbformat.reads(notebook_content, as_version=nbformat.NO_CONVERT)\n",
        "\n",
        "    python_code = None\n",
        "\n",
        "    for cell in notebook.cells:\n",
        "        if cell.cell_type == cell_type:\n",
        "            if not python_code:\n",
        "                python_code = cell.source\n",
        "            else:\n",
        "                python_code += \"\\n\" + cell.source\n",
        "\n",
        "    return python_code\n",
        "\n",
        "\n",
        "def extract_python_code_from_py(github_url):\n",
        "    raw_url = github_url.replace(\"github.com\", \"raw.githubusercontent.com\").replace(\n",
        "        \"/blob/\", \"/\"\n",
        "    )\n",
        "\n",
        "    response = requests.get(raw_url)\n",
        "    response.raise_for_status()  # Check for any request errors\n",
        "\n",
        "    python_code = response.text\n",
        "\n",
        "    return python_code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCRp5Xtb48is"
      },
      "outputs": [],
      "source": [
        "with open(\"code_files_urls.txt\") as f:\n",
        "    code_files_urls = f.read().splitlines()\n",
        "len(code_files_urls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Y9SMO7H4xgF"
      },
      "outputs": [],
      "source": [
        "code_strings = []\n",
        "\n",
        "for i in range(0, len(code_files_urls)):\n",
        "    if code_files_urls[i].endswith(\".ipynb\"):\n",
        "        content = extract_python_code_from_ipynb(code_files_urls[i], \"code\")\n",
        "        doc = Document(\n",
        "            page_content=content, metadata={\"url\": code_files_urls[i], \"file_index\": i}\n",
        "        )\n",
        "        code_strings.append(doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1AF3fhBSLOm"
      },
      "source": [
        "### 3. Chunk & generate embeddings for each code strings & initialize the vector store\n",
        "\n",
        "We need to split code into usable chunks that the LLM can use for code generation. Therefore it's crucial to use the right chunking approach and chunk size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rj1cCA2fqx64"
      },
      "outputs": [],
      "source": [
        "# Utility functions for Embeddings API with rate limiting\n",
        "def rate_limit(max_per_minute):\n",
        "    period = 60 / max_per_minute\n",
        "    print(\"Waiting\")\n",
        "    while True:\n",
        "        before = time.time()\n",
        "        yield\n",
        "        after = time.time()\n",
        "        elapsed = after - before\n",
        "        sleep_time = max(0, period - elapsed)\n",
        "        if sleep_time > 0:\n",
        "            print(\".\", end=\"\")\n",
        "            time.sleep(sleep_time)\n",
        "\n",
        "\n",
        "class CustomVertexAIEmbeddings(VertexAIEmbeddings):\n",
        "    requests_per_minute: int\n",
        "    num_instances_per_batch: int\n",
        "    model_name: str\n",
        "\n",
        "    # Overriding embed_documents method\n",
        "    def embed_documents(\n",
        "        self, texts: List[str], batch_size: Optional[int] = None\n",
        "    ) -> List[List[float]]:\n",
        "        limiter = rate_limit(self.requests_per_minute)\n",
        "        results = []\n",
        "        docs = list(texts)\n",
        "\n",
        "        while docs:\n",
        "            # Working in batches because the API accepts maximum 5\n",
        "            # documents per request to get embeddings\n",
        "            head, docs = (\n",
        "                docs[: self.num_instances_per_batch],\n",
        "                docs[self.num_instances_per_batch :],\n",
        "            )\n",
        "            chunk = self.client.get_embeddings(head)\n",
        "            results.extend(chunk)\n",
        "            next(limiter)\n",
        "\n",
        "        return [r.values for r in results]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oae37l-pvzZ6"
      },
      "outputs": [],
      "source": [
        "# Chunk code strings\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_language(\n",
        "    language=Language.PYTHON, chunk_size=2000, chunk_overlap=200\n",
        ")\n",
        "\n",
        "\n",
        "texts = text_splitter.split_documents(code_strings)\n",
        "print(len(texts))\n",
        "\n",
        "# Initialize Embedding API\n",
        "EMBEDDING_QPM = 100\n",
        "EMBEDDING_NUM_BATCH = 5\n",
        "embeddings = CustomVertexAIEmbeddings(\n",
        "    requests_per_minute=EMBEDDING_QPM,\n",
        "    num_instances_per_batch=EMBEDDING_NUM_BATCH,\n",
        "    model_name=\"textembedding-gecko@latest\",\n",
        ")\n",
        "\n",
        "# Create Index from embedded code chunks\n",
        "db = FAISS.from_documents(texts, embeddings)\n",
        "\n",
        "# Init your retriever.\n",
        "retriever = db.as_retriever(\n",
        "    search_type=\"similarity\",  # Also test \"similarity\", \"mmr\"\n",
        "    search_kwargs={\"k\": 5},\n",
        ")\n",
        "\n",
        "retriever"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_gn89IyuHIT"
      },
      "source": [
        "# Runtime\n",
        "### 4. User enters a prompt or asks a question as a prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vrvTkO7uFNi"
      },
      "outputs": [],
      "source": [
        "user_question = \"Create a Python function that takes a prompt and predicts using langchain.llms interface with Vertex AI text-bison model\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azbvOUFRvEp5"
      },
      "outputs": [],
      "source": [
        "# Define prompt templates\n",
        "\n",
        "# Zero Shot prompt template\n",
        "prompt_zero_shot = \"\"\"\n",
        "    You are a proficient python developer. Respond with the syntactically correct & concise code for to the question below.\n",
        "\n",
        "    Question:\n",
        "    {question}\n",
        "\n",
        "    Output Code :\n",
        "    \"\"\"\n",
        "\n",
        "prompt_prompt_zero_shot = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=prompt_zero_shot,\n",
        ")\n",
        "\n",
        "\n",
        "# RAG template\n",
        "prompt_RAG = \"\"\"\n",
        "    You are a proficient python developer. Respond with the syntactically correct code for to the question below. Make sure you follow these rules:\n",
        "    1. Use context to understand the APIs and how to use it & apply.\n",
        "    2. Do not add license information to the output code.\n",
        "    3. Do not include Colab code in the output.\n",
        "    4. Ensure all the requirements in the question are met.\n",
        "\n",
        "    Question:\n",
        "    {question}\n",
        "\n",
        "    Context:\n",
        "    {context}\n",
        "\n",
        "    Helpful Response :\n",
        "    \"\"\"\n",
        "\n",
        "prompt_RAG_template = PromptTemplate(\n",
        "    template=prompt_RAG, input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "\n",
        "qa_chain = RetrievalQA.from_llm(\n",
        "    llm=code_llm,\n",
        "    prompt=prompt_RAG_template,\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NBaObAQSlIv"
      },
      "source": [
        "### 5. Try zero-shot prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1svTVwtBS0zP"
      },
      "outputs": [],
      "source": [
        "response = code_llm.invoke(input=user_question, max_output_tokens=2048, temperature=0.1)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPm8qdxzwPM0"
      },
      "source": [
        "### 6. Run prompt using RAG Chain & compare results\n",
        "To generate response we use code-bison however can also use code-gecko and codechat-bison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMz3nPMyVoj_"
      },
      "outputs": [],
      "source": [
        "results = qa_chain.invoke(input={\"query\": user_question})\n",
        "print(results[\"result\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HF3lVWK1wjxe"
      },
      "source": [
        "### Let's try another prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jel0ON68XiU7"
      },
      "outputs": [],
      "source": [
        "user_question = \"Create python function that takes text input and returns embeddings using LangChain with Vertex AI textembedding-gecko model\"\n",
        "\n",
        "\n",
        "response = code_llm.invoke(input=user_question, max_output_tokens=2048, temperature=0.1)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9bIkqE8sO6P"
      },
      "outputs": [],
      "source": [
        "results = qa_chain.invoke(input={\"query\": user_question})\n",
        "print(results[\"result\"])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "code_retrieval_augmented_generation.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

```

`/Users/sauravverma/programs/pyano/sage/tests/test_vector_store.py`:

```py
import os
from argparse import Namespace
from unittest.mock import MagicMock, patch

import pytest

from sage.vector_store import (
    MarqoVectorStore,
    PineconeVectorStore,
    build_vector_store_from_args,
)

mock_vectors = [({"id": "1", "text": "example"}, [0.1, 0.2, 0.3])]
mock_namespace = "test_namespace"


class TestVectorStore:
    @pytest.fixture
    def pinecone_store(self):
        with patch("sage.vector_store.Pinecone"):
            store = PineconeVectorStore(index_name="test_index", dimension=128, alpha=0.5)
            yield store

    @pytest.fixture
    def marqo_store(self):
        with patch("marqo.Client"):
            store = MarqoVectorStore(url="http://localhost:8882", index_name="test_index")
            yield store

    @pytest.fixture
    def mock_data_manager(self):
        data_manager = MagicMock()
        data_manager.walk.return_value = [("sample content", {})]
        return data_manager

    @pytest.fixture
    def mock_nltk(self):
        with patch("nltk.data.find") as mock_find:
            mock_find.side_effect = LookupError
            yield mock_find

    @pytest.fixture
    def mock_bm25_encoder(self):
        with patch("sage.vector_store.BM25Encoder") as MockBM25Encoder:
            mock_instance = MockBM25Encoder.return_value
            mock_instance.encode_documents.return_value = [0.1, 0.2, 0.3]
            mock_instance.fit = MagicMock()
            mock_instance.dump = MagicMock()
            yield mock_instance

    def test_pinecone_vector_store_initialization(self, pinecone_store):
        assert pinecone_store.index_name == "test_index"
        assert pinecone_store.dimension == 128
        assert pinecone_store.alpha == 0.5

    def test_pinecone_vector_store_ensure_exists(self, pinecone_store):
        pinecone_store.ensure_exists()
        pinecone_store.client.create_index.assert_called_once()

    def test_pinecone_vector_store_upsert_batch(self, pinecone_store):
        pinecone_store.upsert_batch(mock_vectors, mock_namespace)
        pinecone_store.client.Index().upsert.assert_called_once()

    def test_marqo_vector_store_initialization(self, marqo_store):
        assert marqo_store.index_name == "test_index"

    def test_marqo_vector_store_ensure_exists(self, marqo_store):
        # No specific assertion as ensure_exists is a no-op
        marqo_store.ensure_exists()

    def test_marqo_vector_store_upsert_batch(self, marqo_store):
        # No specific assertion as upsert_batch is a no-op
        marqo_store.upsert_batch(mock_vectors, mock_namespace)

    def build_args(self, provider, alpha=1.0):
        if provider == "pinecone":
            return Namespace(
                vector_store_provider="pinecone",
                pinecone_index_name="test_index",
                embedding_size=128,
                retrieval_alpha=alpha,
                index_namespace="test_namespace",
            )
        elif provider == "marqo":
            return Namespace(
                vector_store_provider="marqo", marqo_url="http://localhost:8882", index_namespace="test_index"
            )

    def build_bm25_cache_path(self):
        return os.path.join(".bm25_cache", "test_namespace", "bm25_encoder.json")

    def test_builds_pinecone_vector_store_with_default_bm25_encoder(
        self, pinecone_store, mock_bm25_encoder, mock_data_manager, mock_nltk
    ):
        args = self.build_args("pinecone", alpha=0.5)
        store = build_vector_store_from_args(args, data_manager=mock_data_manager)
        assert isinstance(store, PineconeVectorStore)
        assert store.bm25_encoder is not None
        mock_bm25_encoder.fit.assert_called_once()
        mock_bm25_encoder.dump.assert_called_once_with(self.build_bm25_cache_path())

    def test_builds_pinecone_vector_store_with_cached_bm25_encoder(self, pinecone_store, mock_bm25_encoder):
        with patch("os.path.exists", return_value=True):
            args = self.build_args("pinecone", alpha=0.5)
            store = build_vector_store_from_args(args)
            assert isinstance(store, PineconeVectorStore)
            assert store.bm25_encoder is not None
            mock_bm25_encoder.load.assert_called_once_with(path=self.build_bm25_cache_path())

    def test_builds_pinecone_vector_store_without_bm25_encoder(self, pinecone_store):
        args = self.build_args("pinecone", alpha=1.0)
        store = build_vector_store_from_args(args)
        assert isinstance(store, PineconeVectorStore)
        assert store.bm25_encoder is None

    def test_builds_marqo_vector_store(self):
        args = self.build_args("marqo")
        store = build_vector_store_from_args(args)
        assert isinstance(store, MarqoVectorStore)

    def test_raises_value_error_for_unrecognized_provider(self):
        args = Namespace(vector_store_provider="unknown")
        with pytest.raises(ValueError, match="Unrecognized vector store type unknown"):
            build_vector_store_from_args(args)

    if __name__ == "__main__":
        pytest.main()

```

`/Users/sauravverma/programs/pyano/sage/MANIFEST.in`:

```in
include sage/sample-exclude.txt
include sage/configs/local.yaml
include sage/configs/remote.yaml
```

`/Users/sauravverma/programs/pyano/sage/README.md`:

```md
<div align="center">
    <a name="readme-top"></a>
    <img src="assets/storia-logo.png" alt="Logo" width="50" style="border-radius: 15px;">
    <h1 align="center">Sage: Chat with any codebase</h1>
    <div>
        <a href="https://discord.gg/zbtZe7GcVU" target=="_blank"><img alt="Discord" src="https://img.shields.io/discord/1286056351264407643?logo=discord&label=discord&link=https%3A%2F%2Fdiscord.gg%2FzbtZe7GcVU"></a>
        <a href="https://x.com/StoriaAI" target=="_blank"><img alt="X (formerly Twitter) Follow" src="https://img.shields.io/twitter/follow/Storia-AI?logo=x&link=https%3A%2F%2Fx.com%2FStoriaAI"></a>
        <a href="https://github.com/Storia-AI/sage/stargazers" target=="_blank"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/Storia-AI/sage?logo=github&link=https%3A%2F%2Fgithub.com%2FStoria-AI%2Fsage%2Fstargazers"></a>
        <a href="https://github.com/Storia-AI/sage/blob/main/LICENSE" target=="_blank"><img alt="GitHub License" src="https://img.shields.io/github/license/Storia-AI/sage" /></a>
    </div>
    <div>
        <a href="https://sage-docs.storia.ai">Documentation</a>
        <span>&#183;</span>
        <a href="https://sage.storia.ai">Hosted app</a>
    </div>
    <br />
    <figure>
        <!-- The <kbd> and <sub> tags are work-arounds for styling, since GitHub doesn't take into account inline styles. Note it might display awkwardly on other Markdown editors. -->
        <kbd><img src="assets/chat_screenshot2.png" alt="screenshot" /></kbd>
        <sub><figcaption align="center">Our chat window, showing a conversation with the Transformers library. 🚀</sub></figcaption>
    </figure>
</div>

***

**Sage** is like an open-source GitHub Copilot that helps you learn how a codebase works and how to integrate it into your project without spending hours sifting through the code.

# Main features
- **Dead-simple setup**. Follow our [quickstart guide](https://sage-docs.storia.ai/quickstart) to get started.
- **Runs locally or on the cloud**. When privacy is your priority, you can run the entire pipeline locally using [Ollama](https://ollama.com) for LLMs and [Marqo](https://github.com/marqo-ai/marqo) as a vector store. When optimizing for quality, you can use third-party LLM providers like OpenAI and Anthropic.
- **Wide range of built-in retrieval mechanisms**. We support both lightweight retrieval strategies (with nothing more but an LLM API key required) and more traditional RAG (which requires indexing the codebase). There are many knobs you can tune for retrieval to work well on your codebase.
- **Well-documented experiments**. We profile various strategies (for embeddings, retrieval etc.) on our own benchmark and thoroughly [document the results](benchmarks/retrieval/README.md).

# Want your repository hosted?

We're working to make all code on the internet searchable and understandable for devs. You can check out [hosted app](https://sage.storia.ai). We pre-indexed a slew of OSS repos, and you can index your desired ones by simply pasting a GitHub URL.

If you're the maintainer of an OSS repo and would like a dedicated page on Code Sage (e.g. `sage.storia.ai/your-repo`), then send us a message at [founders@storia.ai](mailto:founders@storia.ai). We'll do it for free!

![](assets/sage.gif)

# Extensions & Contributions

We built the code purposefully modular so that you can plug in your desired embeddings, LLM and vector stores providers by simply implementing the relevant abstract classes.

Feel free to send feature requests to [founders@storia.ai](mailto:founders@storia.ai) or make a pull request!

# Contributors

<a href="https://github.com/Storia-AI/sage/graphs/contributors">
  <img alt="contributors" src="https://contrib.rocks/image?repo=Storia-AI/sage"/>
</a>

<p align="right" style="font-size: 14px; color: #555; margin-top: 20px;">
    <a href="#readme-top" style="text-decoration: none; color: #007bff; font-weight: bold;">
        ↑ Back to Top ↑
    </a>
</p>

```

`/Users/sauravverma/programs/pyano/sage/setup.py`:

```py
# Work-around the fact that `pip install -e .` doesn't work with `pyproject.toml` files.
from setuptools import setup

setup()

```

`/Users/sauravverma/programs/pyano/sage/CONTRIBUTING.md`:

```md
# Contributing to Sage
Thank you for considering contributing to Sage! We welcome all kinds of contributions. You don't need to be an AI expert to have meaningful impact. If you have any questions, reach out to us via [Discord](https://discord.gg/zbtZe7GcVU).

If you are looking for an internship or full-time job, becoming a contributor is the best way to surface to the top of our resume stack!

## Ways to contribute
We welcome your help in multiple directions. Feel free to pick the one that best matches your skills and interest:

- **Improving documentation**. All our documentation is currently in the project [README](README.md). If you find the instructions incomplete, unclear or confusing, let us know via [Discord](https://discord.gg/zbtZe7GcVU), submit an issue, or send a PR with improved verbiage.
- **Simplifying the setup**. All setup instructions are documented in the [README](README.md). Currently, setting up the environment is more cumbersome than we'd like. For instance, the local setup requires installing Docker and [Marqo](https://www.marqo.ai/), while the remote setup requires you to create accounts with various providers like [Pinecone](https://www.pinecone.io/). We welcome suggestions for streamlining it.
- **Plumbing tasks**. Under the "issues" tab, you will find feature requests marked with difficulty tags between 1 and 5. As a rule of thumb, difficulty labels 1-2 refer to easy coding tasks that don't require any knowledge of AI. They are meant to improve the engineering pipeline without affecting the quality of code retrieval or chat responses.
- **AI quality improvements**. For contributors familiar with AI, these are the most fun feature requests, marked with difficulty 3-5: they encourage you to experiment with state-of-the art techniques for code retrieval and text generation. We welcome code contributions but also suggestions for relevant papers we should check out.
- **Evaluation**. We are currently using our own benchmark for retrieval (more details [here](benchmarks/retrieval/README.md)), but we welcome suggestions for additional datasets, evaluation metrics or evaluation pipelines.
- **UI/UX**. The chat experience is surfaced via a barebones Gradio app. We want to hear your suggestions for more practical or prettier form factors.
- **Branding and marketing**. If you have suggestions for better imagery (logos / header image), messaging or social media presence, we're all ears!

### Code contributions
For code contributions in particular, we suggest the following workflow:
- Fork the repository
- Clone the repository locally to your machine
- Make changes and commit them
- Push the branch to your local fork
- Submit a pull request with the described changes.
- If you are addressing an existing issue or feature request, make sure to reference it under the "Development" section of the pull request.

### Non-code contributions
- If you simply have suggestions for improvement or marketing materials, let us know via [Discord](https://discord.gg/zbtZe7GcVU).

## Hacktoberfest 2024
We welcome Hacktoberfest contributions for all the items above. Issues that we consider particularly beginner-friendly are marked with the "hacktoberfest" label.
```

`/Users/sauravverma/programs/pyano/sage/benchmarks/retrieval/requirements.txt`:

```txt
dotenv
ir_measures

```

`/Users/sauravverma/programs/pyano/sage/benchmarks/retrieval/retrieve_kaggle.py`:

```py
"""Script to call retrieval on the Kaggle dataset.

Steps:
1. Make sure that your repository is already indexed. You can find instructions in the README for how to run the `sage-index` command.
2. Download the test file from the Kaggle competition (https://www.kaggle.com/competitions/code-retrieval-for-hugging-face-transformers/data). You will pass the path to this file via the --benchmark flag below.
3. Run this script:
```
# After you cloned the repository:
cd sage
pip install -e .

# Run the actual retrieval script. Your flags may vary, but this is one example:
python benchmarks/retrieval/retrieve_kaggle.py --benchmark=/path/to/kaggle/test/file.csv --mode=remote --pinecone-index-name=your-index --index-namespace=your-namespace
```
To see a full list of flags, checkout config.py (https://github.com/Storia-AI/sage/blob/main/sage/config.py).
"""

import csv
import json
import logging

import configargparse

import sage.config
from sage.retriever import build_retriever_from_args

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger()
logger.setLevel(logging.INFO)


def main():
    parser = configargparse.ArgParser(
        description="Runs retrieval on the Kaggle dataset.", ignore_unknown_config_file_keys=True
    )
    parser.add("--benchmark", required=True, help="Path to the Kaggle dataset.")
    parser.add("--output-file", required=True, help="Path to the output file with predictions.")

    sage.config.add_config_args(parser)
    sage.config.add_llm_args(parser)  # Necessary for --multi-query-retriever, which calls an LLM.
    sage.config.add_embedding_args(parser)
    sage.config.add_vector_store_args(parser)
    sage.config.add_reranking_args(parser)
    args = parser.parse_args()
    sage.config.validate_vector_store_args(args)

    retriever = build_retriever_from_args(args)

    with open(args.benchmark, "r") as f:
        benchmark = csv.DictReader(f)
        benchmark = [row for row in benchmark]

    outputs = []
    for question_idx, item in enumerate(benchmark):
        print(f"Processing question {question_idx}...")

        retrieved = retriever.invoke(item["question"])
        # Sort by score in descending order.
        retrieved = sorted(
            retrieved, key=lambda doc: doc.metadata.get("score", doc.metadata.get("relevance_score")), reverse=True
        )
        # Keep top 3, since the Kaggle competition only evaluates the top 3.
        retrieved = retrieved[:3]
        retrieved_filenames = [doc.metadata["file_path"] for doc in retrieved]
        outputs.append((item["id"], json.dumps(retrieved_filenames)))

    with open(args.output_file, "w") as f:
        csv_writer = csv.writer(f)
        csv_writer.writerow(["id", "documents"])
        csv_writer.writerows(outputs)


if __name__ == "__main__":
    main()

```

`/Users/sauravverma/programs/pyano/sage/benchmarks/retrieval/README.md`:

```md
# Chat-with-your-codebase: Retrieval Benchmark
When using this repository (which allows you to chat with your codebase in two commands), you are indirectly making a series of choices that greatly influence the quality of your AI copilot: chunking strategy, embeddings, retrieval algorithm, rerankers, etc.

Our role as maintainers is two-fold: to give you options/flexibility, but also to find good defaults. We're not here just to dump code on the Internet. We're here to *make it work*.

To make progress, we need a ladder to climb. That's why we partnered with our friends at [Morph Labs](https://morph.so) to produce a benchmark that will allow us to make informed decisions and measure progress. We will make it public soon, but if you really really can't wait, let us know at [founders@storia.ai](mailto:founders@storia.ai).

Here you will find our first learnings enabled by this dataset. We focused on proprietary APIs, but we're planning on extending experiments to open-source models as well.

#### TL;DR
- OpenAI's `text-embedding-3-small` embeddings perform best.
- NVIDIA's reranker outperforms Cohere, Voyage and Jina.
- Sparse retrieval (e.g. BM25) is actively hurting code retrieval if you have natural language files in your index (e.g. Markdown).
- Chunks of size 800 are ideal; going smaller has very marginal gains.
- Going beyond `top_k=25` for retrieval has diminishing returns.

And now, if you want to nerd out, here's a bunch of plots and stats.

## Dataset
Our dataset consists of 1,000 `<question, answer, relevant_documents>` pairs that focus on Hugging Face's [Transformers](https://github.com/huggingface/transformers) library.

The dataset was generated artificially and checked for quality by humans (we collaborated with [Morph Labs](https://morph.so)). The questions were designed to require context from 1-3 different Python files in order to be answered correctly.

A sample of 10 instances is provided in [sample.json](sample.json).

### Code Retrieval Benchmark
Here, we will be using `<question, relevant_documents>` pairs as a code retrieval benchmark. For instance:
```
- Question:
When developing a specialized model class in the Transformers library, how does `auto_class_update` ensure that the new class's methods are tailored specifically for its requirements while preserving the functionality of the original methods from the base class?

- Relevant documents:
huggingface/transformers/src/transformers/models/auto/auto_factory.py
huggingface/transformers/src/transformers/utils/doc.py
```

#### Why not use an already-established code retrieval benchmark?
Indeed, there are already comprehensive code retrieval benchmarks like [CoIR](https://arxiv.org/abs/2407.02883). In fact, the [CosQA](https://arxiv.org/abs/2105.13239) subset of this benchmark has a similar format to ours (text-to-code retrieval for web queries).

However, we designed our document space to be *an entire codebase*, as opposed to a set of isolated Python functions. A real-world codebase contains a variety of files, including ones that are distracting and get undeservedly selected by the retriever. For instance, dense retrievers tend to prefer short files. READMEs also tend to score high even when irrelevant, since they're written in natural language. Our benchmark is able to surface such behaviors. It also allows us to experiment with a variety of strategies like file chunking.

In the rest of this document, we'll be sharing a few initial learnings enabled by our benchmark.

### Metrics

Throughout this report, we will use the following evaluation metrics, as implemented by the [ir-measures](https://ir-measur.es/en/latest/) library.
- [R-Precision](https://ir-measur.es/en/latest/measures.html#rprec): The precision at R, where R is the number of relevant documents for a given query. Since our queries have a variable number of relevant documents (1-3), this is a convenient metric.
- [Precision@1 (P@1)](https://ir-measur.es/en/latest/measures.html#p): Reflects how many of the documents retrieved on the first position are actually golden documents. Note that P@3 would be a misleading metric: since not all queries have 3 relevant documents, not even the golden dataset would score 100%.
- [Recall@3 (R@3)](https://ir-measur.es/en/latest/measures.html#r): Reflects how many of the golden documents were retrieved by the system. Note that R@1 would be a misleading metric: since a query can have multiple equally-relevant documents, not even the golden dataset would score 100%.
- [Mean Reciprocal Rank (MRR)](https://ir-measur.es/en/latest/measures.html#rr): For each query, takes the first golden document and looks up its rank in the retrieved documents. For instance, if the first golden document is retrieved second, the score for this query is 1/2. Note this metric is somewhat incomplete for our benchmark, because we might have multiple relevant documents.

## Embeddings
:classical_building: **Verdict**: Use OpenAI's `text-embedding-3-small` embeddings.

Today, most retrieval systems are *dense*. They pre-compute document *embeddings* and store them in an index. At inference time, queries are also mapped to the same embedding space. In this world, retrieval is equivalent to finding the nearest neighbors of the query embedding in the index.

To this end, the [MTEB leaderboard](https://huggingface.co/spaces/mteb/leaderboard) (Massive Text Embeddings Benchmark) offers a comprehensive comparison for open-source embeddings.

To complement this, we compared proprietary embedding APIs from [OpenAI](https://platform.openai.com/docs/guides/embeddings), [Gemini](https://ai.google.dev/gemini-api/docs/embeddings) and [Voyage](https://docs.voyageai.com/docs/embeddings). The main advantage of using these providers (in addition to quality) is that they provide *batch* embedding APIs, so you can get an entire repository indexed relatively quickly without the headache of hosting your own embedding models (you can do so with a simple `sage-index $GITHUB_REPO` command).

![embeddings-plot](assets/embeddings.png)

The plot above shows the performance of the three types of embeddings from OpenAI (`text-embedding-3-small`, `text-embedding-3-large`, `text-embedding-ada-002`), Gemini (`text-embedding-004`) and the code-specific embeddings from Voyage (`voyage-code-2`).

#### Experiment settings

- File chunks of <= 800 tokens;
- Dense retriever (nearest neighbor according to cosine distance of embeddings);
- Retrieved `top_k=25`;
- Reranked documents using the [NVIDIA re-ranker](https://docs.nvidia.com/nim/nemo-retriever/text-reranking/latest/using-reranking.html) and selected `top_k=3`.

#### Results

- Across most evaluation metrics, OpenAI's `text-embedding-3-small` performs best, on par with Gemini's `text-embedding-004`.
- It's remarkable that the `text-embedding-3-large` embeddings don't perform better, despite having double the size (3072 vs 1536).
- The older `text-embedding-ada-002` embeddings are trailing last with a huge gap in performance, so this is your call to update your pipeline if you haven't already.

## Rerankers
:classical_building: **Verdict**: Use NVIDIA's reranker.

In a world with infinitely fast compute, we would perform retrieval by passing each `<query, document>` pair through a Transformer, allowing all the query tokens to attend to all the document tokens. However, this is prohibitively expensive.

In practice, all documents are embedded independently and stored in a vector database. Most retrieval systems are two-staged: (1) embed the query independently to find its top N nearest neighbor documents, and (2) re-encode all top N `<query, document>` pairs and select the top K scoring ones. The second stage is called *reranking*.

![rerankers-plot](assets/rerankers.png)

While the [MTEB leaderboard](https://huggingface.co/spaces/mteb/leaderboard) compares *open-source* embedding models based on their ability to rerank documents, we conducted experiments on the most popular *proprietary* APIs for reranking, including [NVIDIA](https://docs.nvidia.com/nim/nemo-retriever/text-reranking/latest/using-reranking.html), [Voyage](https://docs.voyageai.com/docs/reranker), [Cohere](https://cohere.com/rerank) and [Jina](https://jina.ai/reranker/).

#### Experiment settings
- File chunks of <= 800 tokens;
- Dense retriever using OpenAI's `text-embedding-3-small` model;
- Retrieved `top_k=25` documents;
- Reranked documents and selected `top_k=3`.

#### Results
- Across all evaluation metrics, the highest performing rerankers are, in this order: [NVIDIA](https://docs.nvidia.com/nim/nemo-retriever/text-reranking/latest/using-reranking.html), [Voyage](https://docs.voyageai.com/docs/reranker), [Cohere](https://cohere.com/rerank) and [Jina](https://jina.ai/reranker/).
- Not using a reranker at all completely tanks the performance.

## Retrieval: Sparse vs Dense
:classical_building: **Verdict**: Use fully dense embeddings.

So far, we've been experimenting with purely *dense* retrieval. That is, documents are selected solely on the cosine distance between their embedding and the query embedding.

Before the emergence of deep learning, retrievers used to be *sparse*. Such retrievers (e.g. [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) or [BM25](https://en.wikipedia.org/wiki/Okapi_BM25)) were based on vectors of word counts (the vector of a document has the length of the dictionary, with each entry showing how many times a token occurs in the document; the term *sparse* comes from the fact that most entries are 0).

Since sparse retrievers rely on exact string match, one might assume they come in handy when the query contains a relatively unique token (e.g. a class name) that occurs in a small number of documents.

At the intersection of dense and sparse retrievers, *hybrid* retrievers score documents by the weighted average of the dense and sparse scores.

![retrievers-plot](assets/retrievers.png)

In the experiment above, we compared the three types of retrievers (dense, hybrid and sparse).

#### Experiment settings
- File chunks of <= 800 tokens;
- For the dense and hybrid retrievers, we used OpenAI's `text-embedding-3-small` model for embeddings;
- Retrieved `top_k=25` documents;
- Reranked documents using the [NVIDIA re-ranker](https://docs.nvidia.com/nim/nemo-retriever/text-reranking/latest/using-reranking.html) and selected `top_k=3`.

#### Results
Somewhat surprisingly, sparse retrieval is actively hurting performance. The reason is that exact string matching will favor files that are in natural language (and therefore match the token distribution in the query).

The plot below shows what percentage of the retrieved files are in Markdown. The purely sparse retriever chooses a Markdown file 40% of the time! Remember that we designed our questions so that the required context are Python files. This doesn't preclude Markdown files from actually being helpful in answering some of the questions, but surely not to this degree.

![markdown-plot](assets/markdown.png)

## Chunk sizes
:classical_building: **Verdict**: 800 tokens per chunk works well

The [CodeRag paper](https://arxiv.org/pdf/2406.14497) suggests that the ideal chunk size is somewhere between 200-800 tokens. All our experiments above used 800 tokens per chunk. When experimenting with the other end of the spectrum, we saw very mild improvements from having smaller chunks. We believe that these marginal gains are not worth the increased indexing time (since we need to send 4x more queries to the batch embedding APIs).

![chunks-plot](assets/chunks.png)

```

`/Users/sauravverma/programs/pyano/sage/benchmarks/retrieval/sample.json`:

```json
[
    {
        "repo": "huggingface/transformers",
        "commit": "7bb1c99800d235791dace10305731f377db8077b",
        "context_files": [
            "huggingface/transformers/src/transformers/commands/serving.py",
            "huggingface/transformers/src/transformers/pipelines/__init__.py"
        ],
        "question": "With the introduction of a new translation service for \"en_to_es\", how does `serve_command_factory` ensure the server is prepared to handle this specific task efficiently?",
        "answer": "The `serve_command_factory` function is designed to dynamically configure and deploy a server that can handle a variety of tasks, including complex and parameterized tasks such as language-specific translations. When a new translation service for \"en_to_es\" is introduced, the function ensures efficient handling through several key steps and mechanisms:\n\n1. **Task Specification and Validation**: The function begins by accepting command line arguments (`args`) which specify the task among other parameters. The task, in this case \"translation_en_to_es\", is passed to the `pipeline` function. This function internally calls `check_task` to validate and process the task string. The `check_task` function checks if the task is recognized and supported, and it returns structured data that includes default configurations and any task-specific options necessary for the task.\n\n2. **Dynamic Pipeline Configuration**: The `pipeline` function uses the information returned by `check_task` to dynamically configure a pipeline suitable for the specified task. This includes loading the appropriate model, tokenizer, and any additional components like feature extractors or image processors that are required for the task. For translation tasks, this might involve loading a specific model that is optimized for the language pair \"en_to_es\".\n\n3. **Model and Tokenizer Loading**: Depending on the task and the provided arguments, the `pipeline` function loads the necessary model and tokenizer. If the model or tokenizer is not explicitly specified in the command line arguments, the function will load default models and tokenizers that are appropriate for the task. This ensures that the server is using the most suitable tools for the translation between English and Spanish.\n\n4. **Device and Performance Configuration**: The `pipeline` function also handles device allocation (e.g., CPU, GPU) based on the provided arguments, ensuring that the model runs on the optimal hardware. This is crucial for performance, especially when handling complex tasks like real-time translation.\n\n5. **Server Initialization**: Once the pipeline is configured, `serve_command_factory` initializes a `ServeCommand` with the configured pipeline and server parameters such as host, port, and number of workers. This command encapsulates all the necessary settings to run the server efficiently.\n\n6. **Adaptability and Scalability**: The design of `serve_command_factory` and the underlying `pipeline` function allows for easy adaptation to new tasks and changes in task requirements. Adding support for a new translation like \"en_to_es\" primarily involves ensuring that the `check_task` function recognizes the task and that appropriate models and tokenizers are available and correctly configured.\n\nIn summary, `serve_command_factory` ensures that the server is prepared to handle the \"en_to_es\" translation task efficiently by dynamically configuring the server based on the specified task, validating and processing the task to load appropriate models and tools, and setting up the server with optimal hardware and network settings. This process guarantees that the server can adapt to new and complex tasks with minimal manual intervention.",
        "expected_identifiers": [
            "args",
            "check_task",
            "pipeline",
            "ServeCommand"
        ]
    },
    {
        "repo": "huggingface/transformers",
        "commit": "7bb1c99800d235791dace10305731f377db8077b",
        "context_files": [
            "huggingface/transformers/src/transformers/models/esm/modeling_esmfold.py",
            "huggingface/transformers/src/transformers/models/esm/openfold_utils/feats.py",
            "huggingface/transformers/src/transformers/models/esm/openfold_utils/tensor_utils.py"
        ],
        "question": "In a high-throughput setting where multiple protein structures are processed simultaneously, how does `EsmForProteinFolding.output_to_pdb` ensure accurate and independent structural representation in the resulting PDB files?",
        "answer": "In a high-throughput setting where multiple protein structures are processed simultaneously, the function `output_to_pdb` ensures accurate and independent structural representation in the resulting PDB files through a combination of specialized tensor operations and careful indexing. This is achieved primarily through the use of the `atom14_to_atom37` function, which itself relies on the `batched_gather` function to correctly map atom positions from a simplified model output to a more detailed atomic representation.\n\n### Detailed Workflow:\n\n1. **Batch Processing and Tensor Operations**:\n   - The `output_to_pdb` function begins by converting all tensor data to the CPU and converting them to NumPy arrays for easier manipulation. This step is crucial for performance and compatibility with subsequent operations that may not be optimized for GPU tensors.\n\n2. **Mapping Atom Positions**:\n   - The function `atom14_to_atom37` is called within `output_to_pdb`. This function is responsible for expanding the reduced atom representation (14 atoms per amino acid) to a fuller representation (37 atoms per amino acid). It uses the `batched_gather` function to achieve this mapping accurately across potentially multiple proteins in a batch.\n\n3. **Complex Indexing with `batched_gather`**:\n   - `batched_gather` plays a critical role in ensuring that the atom positions are mapped correctly. It constructs a complex indexing tuple that combines batch indices with the provided indices for gathering (`inds`). This tuple (`ranges`) includes both batch dimensions and the specific indices where atoms need to be gathered from the `atom14` tensor.\n   - The use of `ranges` in `batched_gather` ensures that each protein's data is handled independently, preventing any cross-contamination or mixing of data between different proteins in the batch. This is crucial for maintaining the structural integrity of each protein.\n\n4. **Application of Mask and Final Adjustments**:\n   - After mapping the positions, `atom14_to_atom37` applies a mask (`batch[\"atom37_atom_exists\"]`) to ensure that only existing atoms are considered. This step further ensures the accuracy of the structural data by zeroing out positions of non-existent atoms, preventing any erroneous data from affecting the structural representation.\n\n5. **Generation of PDB Data**:\n   - Back in `output_to_pdb`, for each protein in the batch, an instance of `OFProtein` is created with the mapped atom positions, types, and other relevant data. The `to_pdb` function is then used to convert these protein data into the PDB format, ready for downstream applications like molecular dynamics simulations.\n\n### Conclusion:\n\nThrough the careful use of tensor operations, complex indexing, and data masking, `output_to_pdb` ensures that each protein's structural data is accurately and independently represented in the PDB outputs. This methodical approach is essential in high-throughput settings, where the accuracy and integrity of structural data are paramount for subsequent scientific analysis and applications.",
        "expected_identifiers": [
            "atom14_to_atom37",
            "batched_gather",
            "batch[\"atom37_atom_exists\"]",
            "OFProtein"
        ]
    },
    {
        "repo": "huggingface/transformers",
        "commit": "7bb1c99800d235791dace10305731f377db8077b",
        "context_files": [
            "huggingface/transformers/src/transformers/models/auto/auto_factory.py",
            "huggingface/transformers/src/transformers/dynamic_module_utils.py"
        ],
        "question": "Following a security update in the production environment that limits internet connectivity, how does `_BaseAutoModelClass.from_pretrained` guarantee that the loaded model adheres strictly to the predefined version and settings?",
        "answer": "In the updated production environment with restricted internet connectivity, `_BaseAutoModelClass.from_pretrained` ensures that the model loaded adheres strictly to the predefined version and settings through several key mechanisms, primarily involving the management of model files and code via a version control system and secure access to private repositories.\n\n### Version Control and Revision Specification\n\nThe function leverages a version control system that allows users to specify exact revisions of the model or code they wish to use. This is evident in the handling of the `revision` parameter in functions like `get_cached_module_file` and `get_class_from_dynamic_module`. The `revision` parameter can accept any identifier allowed by git, such as a branch name, a tag name, or a commit id. This ensures that the exact version of the model or code that was tested and approved in other environments (like development or staging) is the same version being deployed in production.\n\nFor example, in the `get_cached_module_file` function, the `revision` parameter is used to fetch the specific version of a module file from a repository:\n```python\nresolved_module_file = cached_file(\n    pretrained_model_name_or_path,\n    module_file,\n    cache_dir=cache_dir,\n    force_download=force_download,\n    proxies=proxies,\n    resume_download=resume_download,\n    local_files_only=local_files_only,\n    token=token,\n    revision=revision,\n    repo_type=repo_type,\n    _commit_hash=_commit_hash,\n)\n```\n\n### Secure Access to Private Repositories\n\nThe function can authenticate access to private repositories using tokens, which is crucial when operating in environments with strict security protocols. The `token` parameter, which can be set to a string or `True` (to use the token generated by `huggingface-cli login`), is used to authenticate HTTP requests for remote files. This is handled securely in both `get_cached_module_file` and `get_class_from_dynamic_module`, ensuring that only authorized users can access private model files or code.\n\nFor instance, in `get_class_from_dynamic_module`, the `token` parameter is used to authenticate and download the necessary module file:\n```python\nfinal_module = get_cached_module_file(\n    repo_id,\n    module_file + \".py\",\n    cache_dir=cache_dir,\n    force_download=force_download,\n    resume_download=resume_download,\n    proxies=proxies,\n    token=token,\n    revision=code_revision,\n    local_files_only=local_files_only,\n    repo_type=repo_type,\n)\n```\n\n### Handling Restricted Internet Connectivity\n\nIn environments with limited internet access, the `local_files_only` parameter becomes particularly important. This parameter, when set to `True`, forces the function to only look for model files locally and not attempt to download them from the internet. This is crucial for ensuring that the model loading process does not fail due to lack of internet access and adheres to strict security protocols that might block external internet connections.\n\n### Conclusion\n\nBy utilizing these mechanisms, `_BaseAutoModelClass.from_pretrained` ensures that the model loaded in a production environment with restricted internet access is exactly the version specified, using secure and authenticated access where necessary. This approach guarantees consistency, reproducibility, and adherence to security protocols across different environments.",
        "expected_identifiers": [
            "revision",
            "token",
            "local_files_only"
        ]
    },
    {
        "repo": "huggingface/transformers",
        "commit": "7bb1c99800d235791dace10305731f377db8077b",
        "context_files": [
            "huggingface/transformers/src/transformers/models/auto/auto_factory.py",
            "huggingface/transformers/src/transformers/utils/doc.py"
        ],
        "question": "When developing a specialized model class in the Transformers library, how does `auto_class_update` ensure that the new class's methods are tailored specifically for its requirements while preserving the functionality of the original methods from the base class?",
        "answer": "In the Transformers library, the `auto_class_update` function plays a crucial role in dynamically creating specialized model classes that inherit functionalities from a base class but also have unique customizations. This is particularly important when different model classes need specific configurations or preprocessing steps that are not shared across all models.\n\nThe core mechanism that allows `auto_class_update` to achieve this functionality without altering the behavior of the base class methods lies in its use of the `copy_func` function. Here's how it works step-by-step:\n\n1. **Copying the Function**: `copy_func` is used to create an exact copy of the methods `from_config` and `from_pretrained` from the base class `_BaseAutoModelClass`. This is done by duplicating the `__code__` object of these methods. The `__code__` object contains the compiled executable code that the Python interpreter runs. By copying this code object, the new function retains the exact behavior and logic of the original function.\n\n2. **Customization of the Copied Function**: After copying, `auto_class_update` modifies the docstrings of these methods to tailor them to the specific subclass. This involves inserting a specific `head_doc`, replacing placeholders like `\"BaseAutoModelClass\"` with the subclass's name, and updating example checkpoints specific to the model type (e.g., `\"google-bert/bert-base-cased\"`). These modifications are crucial for providing accurate and relevant documentation and guidance specific to each subclass.\n\n3. **Re-assignment as Class Methods**: Once the functions are copied and customized, they are re-assigned to the subclass as class methods. This is done using `classmethod(from_config)` and `classmethod(from_pretrained)`. This step ensures that these methods, now tailored and documented specifically for the subclass, are callable on the subclass itself.\n\n4. **Preservation of Base Class Functionality**: Since the original methods are copied before being modified, the base class `_BaseAutoModelClass` retains its original `from_config` and `from_pretrained` methods without any changes. This isolation ensures that modifications specific to one subclass do not impact the behavior or documentation of these methods in the base class or any other subclasses.\n\nBy following this process, `auto_class_update` ensures that each subclass in the Transformers library can have methods that are specifically tailored to its requirements, both in terms of functionality and documentation, while preserving the integrity and functionality of the original methods from the base class. This approach enhances modularity and flexibility in the library, allowing developers to easily extend and customize model classes for various use cases.",
        "expected_identifiers": [
            "__code__",
            "copy_func",
            "from_config",
            "from_pretrained"
        ]
    },
    {
        "repo": "huggingface/transformers",
        "commit": "7bb1c99800d235791dace10305731f377db8077b",
        "context_files": [
            "huggingface/transformers/src/transformers/models/megatron_gpt2/checkpoint_reshaping_and_interoperability.py",
            "huggingface/transformers/src/transformers/modeling_utils.py"
        ],
        "question": "Given a system limitation of 5GB per file, how does `convert_checkpoint_from_megatron_to_transformers` manage the storage of a large model's data to comply with this restriction?",
        "answer": "The `convert_checkpoint_from_megatron_to_transformers` function manages the storage of a large model's data to comply with a system limitation of 5GB per file by utilizing the `shard_checkpoint` function to split the model's state dictionary into multiple sub-checkpoints, each of which does not exceed the specified maximum size.\n\nHere's a detailed breakdown of how this is achieved:\n\n1. **Sharding Process**: The `shard_checkpoint` function is called within `convert_checkpoint_from_megatron_to_transformers` to handle the division of the model's weights into smaller parts or shards. This function takes the entire state dictionary of the model (`output_state_dict`) and a maximum shard size as inputs.\n\n2. **Size Calculation**: The function calculates the byte size of each tensor in the state dictionary using the `dtype_byte_size` function. This function determines the number of bytes each element of a tensor occupies in memory, based on the tensor's data type (`dtype`). This calculation is crucial as it helps in accurately assessing how much space each tensor will take when saved as part of a shard.\n\n3. **Iterative Sharding**: The `shard_checkpoint` iterates through each tensor in the state dictionary and adds them to the current shard until adding another tensor would exceed the maximum shard size (5GB in this scenario). When this limit is reached, a new shard is started. This ensures that no individual shard file exceeds the specified size limit.\n\n4. **Handling Oversized Tensors**: If a single tensor is larger than the maximum shard size, it is placed in its own shard. This is a necessary exception to prevent the function from failing due to an inability to split a tensor.\n\n5. **Saving Shards**: Each shard is saved as a separate file. The naming convention and indexing ensure that each part of the model can be identified and accessed correctly. The function also generates an index file if the model is split into multiple shards, detailing where each parameter is stored.\n\n6. **Parameter Mapping**: The function maintains a mapping (`weight_map`) of model parameters to their respective shard files. This mapping is crucial for efficiently loading the model from its sharded state.\n\nBy following these steps, the `convert_checkpoint_from_megatron_to_transformers` function ensures that each shard of the converted model adheres to the 5GB file size limit imposed by the system. This methodical sharding allows for efficient storage and handling of large models without exceeding system file size limitations.",
        "expected_identifiers": [
            "shard_checkpoint",
            "dtype_byte_size",
            "output_state_dict",
            "weight_map"
        ]
    },
    {
        "repo": "huggingface/transformers",
        "commit": "7bb1c99800d235791dace10305731f377db8077b",
        "context_files": [
            "huggingface/transformers/src/transformers/quantizers/quantizer_hqq.py",
            "huggingface/transformers/src/transformers/integrations/hqq.py"
        ],
        "question": "In a scenario where a neural network model is being optimized for deployment, how does `HqqHfQuantizer._process_model_before_weight_loading` ensure that each linear module is appropriately and uniquely quantized?",
        "answer": "In the scenario where a neural network model is being optimized for deployment using the `HqqHfQuantizer._process_model_before_weight_loading` function, the process of ensuring that each linear module is appropriately and uniquely quantized involves several key steps and functions.\n\n1. **Tagging Modules with Unique Identifiers**: The process begins with the `get_linear_tags` function, which is responsible for identifying and tagging all linear modules within the model. This function uses a `set` to collect the names of these modules, which inherently ensures that each tag is unique (since sets do not allow duplicates). This is crucial because it prevents any confusion or errors in later stages when quantization parameters are applied to these tags.\n\n2. **Applying Quantization Configuration**: Once the linear modules are tagged, the `prepare_for_hqq_linear` function takes over. This function receives a `quantization_config` and a list of modules not to convert. It first calls `autoname_modules` to ensure each module in the model has a unique name, and then retrieves the linear tags using `get_linear_tags`. The function then filters these tags to exclude any specified in `skip_modules` or `modules_to_not_convert`, ensuring that the quantization process is applied only to the relevant modules.\n\n3. **Mapping Quantization Parameters**: The core of the quantization process happens when `prepare_for_hqq_linear` maps the quantization parameters to each linear tag. This is done by creating a dictionary (`patch_params`) where each key is a linear tag and the value is the corresponding quantization parameter. If specific quantization parameters are not provided for a tag, a default configuration is applied. This mapping ensures that each linear module (identified uniquely by its tag) receives a tailored set of quantization parameters.\n\n4. **Updating Model Configuration**: After mapping the quantization parameters, the `prepare_for_hqq_linear` function updates the model's configuration to include these parameters, ensuring that each linear module's configuration reflects its unique quantization settings. This step is crucial for the actual quantization process, where linear modules might be replaced with their quantized counterparts (`HQQLinear`), depending on the configuration.\n\n5. **Final Verification and Logging**: The function checks if any linear modules have been replaced and logs a warning if no modules were found for quantization. This serves as a final check to ensure that the quantization process has been applied as expected.\n\nIn summary, the `HqqHfQuantizer._process_model_before_weight_loading` function ensures that each linear module is uniquely and appropriately quantized by meticulously tagging each module, applying a tailored quantization configuration, and updating the model to reflect these settings. This process is designed to optimize the model's performance for deployment, ensuring that each module operates efficiently and accurately under the constraints of quantization.",
        "expected_identifiers": [
            "get_linear_tags",
            "autoname_modules",
            "prepare_for_hqq_linear",
            "patch_params"
        ]
    },
    {
        "repo": "huggingface/transformers",
        "commit": "7bb1c99800d235791dace10305731f377db8077b",
        "context_files": [
            "huggingface/transformers/src/transformers/models/esm/modeling_esmfold.py",
            "huggingface/transformers/src/transformers/models/esm/openfold_utils/loss.py"
        ],
        "question": "When analyzing a protein sequence with low complexity using `EsmForProteinFolding.forward`, how is the stability and definition of the output ensured?",
        "answer": "When analyzing a protein sequence with low complexity using the `EsmForProteinFolding.forward` function, the stability and definition of the output are ensured through several key mechanisms embedded within the function's implementation, particularly in how it handles normalization and potential numerical instabilities.\n\n1. **Normalization of Residue Weights**: In the `compute_tm` function, residue weights are normalized by their sum, with the addition of a small constant `eps` (epsilon) to prevent division by zero. This is crucial when dealing with sequences of low complexity where certain residues might be overrepresented or underrepresented. The normalization step is represented in the code as:\n   ```python\n   normed_residue_mask = residue_weights / (eps + residue_weights.sum())\n   ```\n   Here, `eps` acts as a safeguard against division by zero, ensuring that the function remains numerically stable and produces defined outputs even when the sum of residue weights is extremely small or zero.\n\n2. **Weighted Average Calculation**: The function calculates a weighted average of the Template Modeling (TM) scores across different bins, which is critical for obtaining a reliable TM score. This is done using the normalized residue weights, ensuring that each residue's contribution is proportionate to its presence, thus maintaining accuracy and stability in the final score calculation:\n   ```python\n   per_alignment = torch.sum(predicted_tm_term * normed_residue_mask, dim=-1)\n   ```\n   This step aggregates the TM scores across all residues, factoring in their normalized weights, which is particularly important in low complexity sequences where certain residues might dominate.\n\n3. **Handling of Edge Cases**: The use of `eps` in the normalization process is a direct method to handle edge cases, such as sequences with low complexity or unusual amino acid distributions. By ensuring that the denominator in the normalization step is never zero, the function avoids potential runtime errors (like NaN or infinite values), which could disrupt the analysis process.\n\n4. **Integration within `EsmForProteinFolding.forward`**: The stability and definition of outputs from the `EsmForProteinFolding.forward` function are further supported by how `compute_tm` integrates with other components of the model. The TM scores computed are used alongside other structural predictions, contributing to a comprehensive evaluation of the predicted protein structures. This integration ensures that the outputs are not only stable and defined but also meaningful in the context of protein structure prediction.\n\nIn summary, the `EsmForProteinFolding.forward` function ensures stable and defined outputs for protein structure predictions, particularly in scenarios involving low complexity sequences, by employing robust normalization techniques and handling potential numerical instabilities through the careful addition of a small epsilon value in critical calculations. This approach guarantees that the function can reliably process a wide range of input data without encountering computational errors.",
        "expected_identifiers": [
            "normed_residue_mask",
            "eps",
            "residue_weights / (eps + residue_weights.sum())",
            "torch.sum(predicted_tm_term * normed_residue_mask, dim=-1)"
        ]
    },
    {
        "repo": "huggingface/transformers",
        "commit": "7bb1c99800d235791dace10305731f377db8077b",
        "context_files": [
            "huggingface/transformers/src/transformers/pipelines/question_answering.py",
            "huggingface/transformers/src/transformers/data/processors/squad.py"
        ],
        "question": "In a scenario where the textual data includes unusually lengthy paragraphs, how does `QuestionAnsweringPipeline.preprocess` ensure comprehensive coverage of all context tokens in the model's input sequences?",
        "answer": "In scenarios where the textual data includes unusually lengthy paragraphs that exceed the model's maximum input length, the `QuestionAnsweringPipeline.preprocess` function ensures comprehensive coverage of all context tokens in the model's input sequences through a meticulous management of tokenization and handling of overflow tokens. This process is crucial for maintaining the integrity and continuity of the context information, which is essential for the model to accurately answer questions based on the provided context.\n\n### Step-by-Step Explanation:\n\n1. **Tokenization and Pairing**:\n   The function begins by tokenizing the question and context separately. Depending on the tokenizer's configuration (`tokenizer.padding_side`), the question and context are arranged in a specific order (either question first or context first). This is handled in the lines where `encoded_inputs` is defined using `self.tokenizer(text, text_pair, ...)`. \n\n2. **Handling Long Contexts with Overflow Tokens**:\n   The key parameter here is `return_overflowing_tokens=True` within the tokenizer call. This setting ensures that when the combined length of the question and context exceeds `max_seq_len`, the tokenizer automatically generates additional input sequences that contain the \"overflow\" tokens from the context. These sequences overlap by a number of tokens defined by `doc_stride`, which is calculated as `min(max_seq_len // 2, 128)`.\n\n3. **Creating Overlapping Spans**:\n   The overlapping spans are crucial for ensuring that tokens near the boundaries of a sequence are also seen in different contextual surroundings, enhancing the model's ability to understand and answer questions about tokens that appear near the maximum sequence length limit. This overlap is managed by the `stride` parameter in the tokenizer, which is set to `doc_stride`.\n\n4. **Feature Construction**:\n   For each span generated from the overflowing tokens, the function constructs a feature object that includes not only the token ids (`input_ids`) but also attention masks, token type ids, and a special mask (`p_mask`) which indicates which tokens can be part of an answer. The `p_mask` is particularly important as it helps the model distinguish between context tokens (potential answer locations) and non-context tokens (like those belonging to the question or special tokens).\n\n5. **Yielding Processed Features**:\n   Each feature constructed from the spans is then yielded one by one, with additional metadata such as whether it is the last feature of the example. This is handled in the loop `for i, feature in enumerate(features):` where each feature is prepared according to the model's requirements, potentially converting them into tensors suitable for the model's computation framework (PyTorch or TensorFlow).\n\n### Conclusion:\n\nBy managing the tokenization and overflow tokens effectively, `QuestionAnsweringPipeline.preprocess` ensures that every token from a lengthy context is included in at least one input sequence to the model. This comprehensive coverage is achieved through the creation of multiple, overlapping input sequences that ensure no contextual information is lost, thereby enabling the model to perform accurately even with contexts that far exceed its maximum input length. This methodical approach is crucial for handling real-world data where lengthy paragraphs are common, ensuring robust model performance across varied datasets.",
        "expected_identifiers": [
            "return_overflowing_tokens=True",
            "doc_stride",
            "p_mask"
        ]
    },
    {
        "repo": "huggingface/transformers",
        "commit": "7bb1c99800d235791dace10305731f377db8077b",
        "context_files": [
            "huggingface/transformers/examples/research_projects/movement-pruning/masked_run_squad.py",
            "huggingface/transformers/src/transformers/data/processors/squad.py"
        ],
        "question": "Given the challenge of training models on datasets with varying context lengths, how does `load_and_cache_examples` in `examples/research_projects/movement-pruning/masked_run_squad.py` ensure the production of a dataset that supports accurate answer position prediction by the models?",
        "answer": "The `load_and_cache_examples` function in `examples/research_projects/movement-pruning/masked_run_squad.py` is designed to ensure that the dataset produced supports accurate answer position prediction by models, even when dealing with varying context lengths. This is achieved through a series of steps that involve careful handling and processing of the data, particularly when contexts exceed the model's maximum sequence length. Here's how the function manages this:\n\n1. **Data Loading and Caching**: Initially, the function checks if a cached version of the processed data exists. If it does, it loads the features, dataset, and examples directly from the cache, bypassing the need for reprocessing. This step ensures efficiency and consistency in data usage across different training runs.\n\n2. **Dynamic Data Processing**: If no cached data is available, the function processes the raw data to generate features suitable for model training. This involves tokenizing the text and converting the SQuAD examples into features that models can understand and learn from.\n\n3. **Handling Extended Contexts**: The core of handling varying context lengths lies in the `squad_convert_examples_to_features` function, which is called within `load_and_cache_examples`. This function uses `squad_convert_example_to_features` to process each example individually.\n\n4. **Segmentation and Token Index Adjustment**: In `squad_convert_example_to_features`, the context is potentially split into multiple spans if its length exceeds the model's maximum sequence length. This is crucial because it allows the model to handle long contexts by breaking them down into manageable parts. Each span is processed to ensure that the start and end positions of answers are correctly adjusted within the tokenized context. This adjustment is handled by the `_improve_answer_span` function, which ensures that the answer spans are accurately placed within the tokens, even if the context is segmented.\n\n5. **Feature Construction**: Each span is then converted into a set of features, including input IDs, attention masks, token type IDs, and the positions of the answers. Special care is taken to mark tokens that cannot be part of the answers (using a p_mask), and to identify the maximum context for each token, which is critical for understanding which part of the split context a token belongs to.\n\n6. **Dataset Compilation**: After processing, the features are compiled into a dataset format (either PyTorch or TensorFlow, based on the configuration). This dataset includes all necessary information for the model to learn from, including the context, the question, and the correct positions of the answers.\n\nBy carefully managing the tokenization, segmentation, and feature construction processes, `load_and_cache_examples` ensures that the dataset it produces allows models to accurately predict answer positions, regardless of the length of the context. This capability is essential for training robust question-answering models that can handle real-world data, where context lengths can vary significantly.",
        "expected_identifiers": [
            "squad_convert_examples_to_features",
            "squad_convert_example_to_features",
            "_improve_answer_span",
            "p_mask"
        ]
    },
    {
        "repo": "huggingface/transformers",
        "commit": "7bb1c99800d235791dace10305731f377db8077b",
        "context_files": [
            "huggingface/transformers/src/transformers/modeling_flax_utils.py",
            "huggingface/transformers/src/transformers/utils/hub.py"
        ],
        "question": "In a scenario where network conditions are suboptimal, how does `FlaxPreTrainedModel.from_pretrained` manage to reduce the model loading time?",
        "answer": "In scenarios where network conditions are suboptimal, the `FlaxPreTrainedModel.from_pretrained` function effectively reduces model loading time by leveraging a sophisticated caching mechanism. This mechanism is crucial for managing the download and storage of model shards, ensuring efficient and faster model initialization.\n\n### Caching Mechanism:\nThe function first checks if the required model shards are already available in the local cache before attempting any network requests. This is achieved through the `try_to_load_from_cache` function, which inspects the cache for the presence of the last shard of the model. If the last shard is found in the cache, it is likely that all previous shards are also cached, thus avoiding the need for further network requests.\n\n### Download and Cache Management:\nIf the shards are not found in the cache, `FlaxPreTrainedModel.from_pretrained` proceeds to download them. Each shard's presence is verified using the `cached_file` function, which handles the downloading and caching of the shard if it is not already present. This function also supports resuming downloads, which is particularly useful in suboptimal network conditions where downloads might be interrupted.\n\n### Efficient Shard Handling:\nThe function `get_checkpoint_shard_files` is specifically designed to manage sharded model files. It reads the checkpoint index file to determine all the necessary shards for the model and then ensures each shard is either fetched from the cache or downloaded. This process is streamlined by the use of a progress bar (managed by `tqdm`), which provides visual feedback on the download process, enhancing user experience especially in network-constrained environments.\n\n### Impact of Caching on Model Loading Time:\nBy prioritizing cached shards, `FlaxPreTrainedModel.from_pretrained` significantly reduces the dependency on network bandwidth and stability. This is particularly beneficial in scenarios with limited network resources, as it minimizes the time spent in downloading model components. The caching mechanism ensures that once a model shard is downloaded and stored locally, subsequent loads of the same model will utilize the cached versions, thereby bypassing the network entirely and leading to much faster model initialization times.\n\n### Conclusion:\nThe caching strategy employed by `FlaxPreTrainedModel.from_pretrained` not only optimizes the use of network resources but also ensures consistent and reduced model loading times, regardless of network conditions. This approach is instrumental in scenarios where models need to be switched frequently or reloaded, providing a seamless and efficient user experience.",
        "expected_identifiers": [
            "try_to_load_from_cache",
            "cached_file",
            "get_checkpoint_shard_files",
            "tqdm"
        ]
    },
    {
        "repo": "huggingface/transformers",
        "commit": "7bb1c99800d235791dace10305731f377db8077b",
        "context_files": [
            "huggingface/transformers/examples/research_projects/information-gain-filtration/run_clm_igf.py",
            "huggingface/transformers/examples/research_projects/information-gain-filtration/igf/igf.py"
        ],
        "question": "In light of recent dataset size restrictions for training purposes, how does `generate_n_pairs` maintain compliance by ensuring the objective set adheres to the specified size and article length requirements?",
        "answer": "The `generate_n_pairs` function ensures compliance with dataset size restrictions by meticulously managing the creation of the objective set through its subordinate function `generate_datasets`. This process is governed by specific parameters and conditions set within the code to meet the required criteria of size and article length.\n\n1. **Size of the Objective Set**: The function `generate_datasets` is designed to create an objective set that contains exactly the number of articles specified by the `number` parameter, which is passed from `generate_n_pairs` as `size_objective_set`. In the provided code, this value is set to 100. The loop within `generate_datasets` that populates the `objective_set` list includes a condition to break once the length of this list reaches the specified `number` (see the line `if len(objective_set) >= number: break`). This ensures that no more than 100 articles are added to the objective set, directly adhering to the dataset size restrictions.\n\n2. **Article Length Management**: The function also manages the length of each article in the objective set based on the `context_len` parameter. If `trim` is set to `True`, the function trims the articles to ensure they do not exceed the specified `context_len`. This is achieved by selecting a starting point randomly within the article and then slicing the article to obtain a segment of the specified `context_len` (see the line `objective_set.append(example[0, start : start + context_len])`). This ensures that each article in the objective set adheres to the length restrictions.\n\n3. **Compliance with Regulations**: By strictly controlling both the number of articles and their lengths as described, `generate_n_pairs` ensures that the objective set complies with new regulations requiring training datasets to contain no more than 100 articles, each of a specified maximum length. This compliance is crucial for ethical review and adherence to training dataset standards.\n\nIn summary, `generate_n_pairs` maintains compliance with dataset size and article length restrictions through careful implementation in `generate_datasets`, which explicitly controls the size of the objective set and trims articles to the required length based on the parameters provided. This methodical approach ensures that the objective set meets specified criteria, crucial for adhering to regulatory standards.",
        "expected_identifiers": [
            "generate_n_pairs",
            "generate_datasets",
            "size_objective_set",
            "context_len"
        ]
    }
]
```

`/Users/sauravverma/programs/pyano/sage/benchmarks/retrieval/retrieve.py`:

```py
"""Script to call retrieval on a benchmark dataset.

Make sure to `pip install ir_measures` before running this script.
"""

import json
import logging
import os
import time

import configargparse
from dotenv import load_dotenv
from ir_measures import MAP, MRR, P, Qrel, R, Rprec, ScoredDoc, calc_aggregate, nDCG

import sage.config
from sage.data_manager import GitHubRepoManager
from sage.retriever import build_retriever_from_args

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger()
logger.setLevel(logging.INFO)

load_dotenv()


def main():
    parser = configargparse.ArgParser(
        description="Runs retrieval on a benchmark dataset.", ignore_unknown_config_file_keys=True
    )
    parser.add("--benchmark", required=True, help="Path to the benchmark dataset.")
    parser.add(
        "--gold-field", default="context_files", help="Field in the benchmark dataset that contains the golden answers."
    )
    parser.add(
        "--question-field", default="question", help="Field in the benchmark dataset that contains the questions."
    )
    parser.add(
        "--logs-dir",
        default=None,
        help="Path where to output predictions and metrics. Optional, since metrics are also printed to console.",
    )

    parser.add("--max-instances", default=None, type=int, help="Maximum number of instances to process.")

    validator = sage.config.add_all_args(parser)
    args = parser.parse_args()
    validator(args)

    repo_manager = GitHubRepoManager.from_args(args)
    retriever = build_retriever_from_args(args, repo_manager)

    with open(args.benchmark, "r") as f:
        benchmark = json.load(f)
    if args.max_instances is not None:
        benchmark = benchmark[: args.max_instances]

    golden_docs = []  # List of ir_measures.Qrel objects
    retrieved_docs = []  # List of ir_measures.ScoredDoc objects

    for question_idx, item in enumerate(benchmark):
        print(f"Processing question {question_idx}...")

        query_id = str(question_idx)  # Solely needed for ir_measures library.

        for golden_filepath in item[args.gold_field]:
            # All the file paths in the golden answer are equally relevant for the query (i.e. the order is irrelevant),
            # so we set relevance=1 for all of them.
            golden_docs.append(Qrel(query_id=query_id, doc_id=golden_filepath, relevance=1))

        # Make a retrieval call for the current question.
        retrieved = retriever.invoke(item[args.question_field])
        item["retrieved"] = []
        for doc_idx, doc in enumerate(retrieved):
            # The absolute value of the scores below does not affect the metrics; it merely determines the ranking of
            # the retrieved documents. The key of the score varies depending on the underlying retriever. If there's no
            # score, we use 1/(doc_idx+1) since it preserves the order of the documents.
            score = doc.metadata.get("score", doc.metadata.get("relevance_score", 1 / (doc_idx + 1)))
            retrieved_docs.append(ScoredDoc(query_id=query_id, doc_id=doc.metadata["file_path"], score=score))
            # Update the output dictionary with the retrieved documents.
            item["retrieved"].append({"file_path": doc.metadata["file_path"], "score": score})

        if "answer" in item:
            item.pop("answer")  # Makes the output file harder to read.

    print("Calculating metrics...")
    results = calc_aggregate([Rprec, P @ 1, R @ 3, nDCG @ 3, MAP, MRR], golden_docs, retrieved_docs)
    results = {str(key): value for key, value in results.items()}
    if args.logs_dir:
        if not os.path.exists(args.logs_dir):
            os.makedirs(args.logs_dir)

        out_data = {
            "data": benchmark,
            "metrics": results,
            "flags": vars(args),  # For reproducibility.
        }

        output_file = os.path.join(args.logs_dir, f"{time.time()}.json")
        with open(output_file, "w") as f:
            json.dump(out_data, f, indent=4)

    for key in sorted(results.keys()):
        print(f"{key}: {results[key]}")
    print(f"Predictions and metrics saved to {output_file}")


if __name__ == "__main__":
    main()

```

`/Users/sauravverma/programs/pyano/sage/sage/config.py`:

```py
"""Utility methods to define and validate flags."""

import argparse
import importlib.resources as resources
import logging
import os
import re
from typing import Callable

from configargparse import ArgumentParser

from sage.reranker import RerankerProvider

# Limits defined here: https://ai.google.dev/gemini-api/docs/models/gemini
GEMINI_MAX_TOKENS_PER_CHUNK = 2048

MARQO_MAX_CHUNKS_PER_BATCH = 64
# The ADA embedder from OpenAI has a maximum of 8192 tokens.
OPENAI_MAX_TOKENS_PER_CHUNK = 8192
# The OpenAI batch embedding API enforces a maximum of 2048 chunks per batch.
OPENAI_MAX_CHUNKS_PER_BATCH = 2048
# The OpenAI batch embedding API enforces a maximum of 3M tokens processed at once.
OPENAI_MAX_TOKENS_PER_JOB = 3_000_000

# Note that OpenAI embedding models have fixed dimensions, however, taking a slice of them is possible.
# See "Reducing embedding dimensions" under https://platform.openai.com/docs/guides/embeddings/use-cases and
# https://platform.openai.com/docs/api-reference/embeddings/create#embeddings-create-dimensions
OPENAI_DEFAULT_EMBEDDING_SIZE = {
    "text-embedding-ada-002": 1536,
    "text-embedding-3-small": 1536,
    "text-embedding-3-large": 3072,
}

VOYAGE_MAX_CHUNKS_PER_BATCH = 128


def get_voyage_max_tokens_per_batch(model: str) -> int:
    """Returns the maximum number of tokens per batch for the Voyage model.
    See https://docs.voyageai.com/reference/embeddings-api."""
    if model == "voyage-3-lite":
        return 1_000_000
    if model in ["voyage-3", "voyage-2"]:
        return 320_000
    return 120_000


def get_voyage_embedding_size(model: str) -> int:
    """Returns the embedding size for the Voyage model. See https://docs.voyageai.com/docs/embeddings#model-choices."""
    if model == "voyage-3-lite":
        return 512
    if model == "voyage-2-code":
        return 1536
    return 1024


def add_config_args(parser: ArgumentParser):
    """Adds configuration-related arguments to the parser."""
    parser.add(
        "--mode",
        choices=["local", "remote"],
        default="remote",
        help="Whether to use local-only resources or call third-party providers (remote).",
    )
    parser.add(
        "--config",
        is_config_file=True,
        help="Path to .yaml configuration file.",
    )
    args, _ = parser.parse_known_args()
    config_file = resources.files("sage").joinpath(f"configs/{args.mode}.yaml")
    parser.set_defaults(config=str(config_file))
    return lambda _: True


def add_repo_args(parser: ArgumentParser) -> Callable:
    """Adds repository-related arguments to the parser and returns a validator."""
    parser.add("repo_id", help="The ID of the repository to index")
    parser.add("--commit-hash", help="Optional commit hash to checkout. When not provided, defaults to HEAD.")
    parser.add(
        "--local-dir",
        default="repos",
        help="The local directory to store the repository",
    )
    return validate_repo_args


def add_embedding_args(parser: ArgumentParser) -> Callable:
    """Adds embedding-related arguments to the parser and returns a validator."""
    parser.add("--embedding-provider", default="marqo", choices=["openai", "voyage", "marqo", "gemini"])
    parser.add(
        "--embedding-model",
        type=str,
        default=None,
        help="The embedding model. Defaults to `text-embedding-ada-002` for OpenAI and `hf/e5-base-v2` for Marqo.",
    )
    parser.add(
        "--embedding-size",
        type=int,
        default=None,
        help="The embedding size to use for OpenAI text-embedding-3* models. Defaults to 1536 for small and 3072 for "
        "large. Note that no other OpenAI models support a dynamic embedding size, nor do models used with Marqo.",
    )
    parser.add(
        "--tokens-per-chunk",
        type=int,
        default=800,
        help="https://arxiv.org/pdf/2406.14497 recommends a value between 200-800.",
    )
    parser.add(
        "--chunks-per-batch",
        type=int,
        help="Maximum chunks per batch. We recommend 2000 for the OpenAI embedder. Marqo enforces a limit of 64.",
    )
    parser.add(
        "--max-embedding-jobs",
        type=int,
        help="Maximum number of embedding jobs to run. Specifying this might result in "
        "indexing only part of the repository, but prevents you from burning through OpenAI credits.",
    )
    return validate_embedding_args


def add_vector_store_args(parser: ArgumentParser) -> Callable:
    """Adds vector store-related arguments to the parser and returns a validator."""
    parser.add(
        "--vector-store-provider", default="marqo", choices=["pinecone", "marqo", "chroma", "faiss", "milvus", "qdrant"]
    )
    parser.add("--index-name", default="sage", help="Index name for the vector store index.")
    parser.add(
        "--milvus-uri",
        default="milvus_sage.db",
        help="URI for milvus. We default it to milvus_sage.db",
    )
    parser.add(
        "--index-namespace",
        default=None,
        help="Index namespace for this repo. When not specified, we default it to a derivative of the repo name.",
    )
    parser.add(
        "--marqo-url",
        default="http://localhost:8882",
        help="URL for the Marqo server. Required if using Marqo as embedder or vector store.",
    )
    parser.add(
        "--retrieval-alpha",
        default=1.0,
        type=float,
        help="Takes effect for Pinecone retriever only. The weight of the dense (embeddings-based) vs sparse (BM25) "
        "encoder in the final retrieval score. A value of 0.0 means BM25 only, 1.0 means embeddings only.",
    )
    parser.add(
        "--retriever-top-k", default=25, type=int, help="The number of top documents to retrieve from the vector store."
    )
    parser.add(
        "--multi-query-retriever",
        action=argparse.BooleanOptionalAction,
        default=False,
        help="When set to True, we rewrite the query 5 times, perform retrieval for each rewrite, and take the union "
        "of retrieved documents. See https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/MultiQueryRetriever/.",
    )
    parser.add(
        "--llm-retriever",
        action=argparse.BooleanOptionalAction,
        default=True,
        help="When set to True, we use an LLM for retrieval: we pass the repository file hierarchy together with the "
        "user query and ask the LLM to choose relevant files solely based on their paths. No indexing will be done, so "
        "all the vector store / embedding arguments will be ignored.",
    )
    return validate_vector_store_args


def add_indexing_args(parser: ArgumentParser) -> Callable:
    """Adds indexing-related arguments to the parser and returns a validator."""
    parser.add(
        "--include",
        help="Path to a file containing a list of extensions to include. One extension per line.",
    )
    parser.add(
        "--exclude",
        help="Path to a file containing a list of extensions to exclude. One extension per line.",
    )
    # Pass --no-index-repo in order to not index the repository.
    parser.add(
        "--index-repo",
        action=argparse.BooleanOptionalAction,
        default=True,
        help="Whether to index the repository. At least one of --index-repo and --index-issues must be True.",
    )
    # Pass --no-index-issues in order to not index the issues.
    parser.add(
        "--index-issues",
        action=argparse.BooleanOptionalAction,
        default=False,
        help="Whether to index GitHub issues. At least one of --index-repo and --index-issues must be True. When "
        "--index-issues is set, you must also set a GITHUB_TOKEN environment variable.",
    )
    # Pass --no-index-issue-comments in order to not index the comments of GitHub issues.
    parser.add(
        "--index-issue-comments",
        action=argparse.BooleanOptionalAction,
        default=False,
        help="Whether to index the comments of GitHub issues. This is only relevant if --index-issues is set. "
        "GitHub's API for downloading comments is quite slow. Indexing solely the body of an issue seems to bring most "
        "of the gains anyway.",
    )
    return validate_indexing_args


def add_reranking_args(parser: ArgumentParser) -> Callable:
    """Adds reranking-related arguments to the parser."""
    parser.add("--reranker-provider", default="huggingface", choices=[r.value for r in RerankerProvider])
    parser.add(
        "--reranker-model",
        help="The reranker model name. When --reranker-provider=huggingface, we suggest choosing a model from the "
        "SentenceTransformers Cross-Encoders library https://huggingface.co/cross-encoder?sort_models=downloads#models",
    )
    parser.add("--reranker-top-k", default=5, help="The number of top documents to return after reranking.")
    # Trivial validator (nothing to check).
    return lambda _: True


def add_llm_args(parser: ArgumentParser) -> Callable:
    """Adds language model-related arguments to the parser."""
    parser.add("--llm-provider", default="ollama", choices=["openai", "anthropic", "ollama", "together"])
    parser.add(
        "--llm-model",
        help="The LLM name. Must be supported by the provider specified via --llm-provider.",
    )
    # Trivial validator (nothing to check).
    return lambda _: True


def add_all_args(parser: ArgumentParser) -> Callable:
    """Adds all arguments to the parser and returns a validator."""
    arg_validators = [
        add_config_args(parser),
        add_repo_args(parser),
        add_embedding_args(parser),
        add_vector_store_args(parser),
        add_reranking_args(parser),
        add_indexing_args(parser),
        add_llm_args(parser),
    ]

    def validate_all(args):
        for validator in arg_validators:
            validator(args)

    return validate_all


def validate_repo_args(args):
    """Validates the configuration of the repository."""
    if not re.match(r"^[^/]+/[^/]+$", args.repo_id):
        raise ValueError("repo_id must be in the format 'owner/repo'")


def _validate_openai_embedding_args(args):
    """Validates the configuration of the OpenAI batch embedder and sets defaults."""
    if args.embedding_provider == "openai" and not os.getenv("OPENAI_API_KEY"):
        raise ValueError("Please set the OPENAI_API_KEY environment variable.")

    if not args.embedding_model:
        args.embedding_model = "text-embedding-3-small"

    if args.embedding_model not in OPENAI_DEFAULT_EMBEDDING_SIZE.keys():
        raise ValueError(f"Unrecognized embeddings.model={args.embedding_model}")

    if not args.embedding_size:
        args.embedding_size = OPENAI_DEFAULT_EMBEDDING_SIZE.get(args.embedding_model)

    if not args.tokens_per_chunk:
        # https://arxiv.org/pdf/2406.14497 recommends a value between 200-800.
        args.tokens_per_chunk = 800
    elif args.tokens_per_chunk > OPENAI_MAX_TOKENS_PER_CHUNK:
        args.tokens_per_chunk = OPENAI_MAX_TOKENS_PER_CHUNK
        logging.warning(
            f"OpenAI enforces a limit of {OPENAI_MAX_TOKENS_PER_CHUNK} tokens per chunk. "
            "Overwriting embeddings.tokens_per_chunk."
        )

    if not args.chunks_per_batch:
        args.chunks_per_batch = OPENAI_MAX_CHUNKS_PER_BATCH
    elif args.chunks_per_batch > OPENAI_MAX_CHUNKS_PER_BATCH:
        args.chunks_per_batch = OPENAI_MAX_CHUNKS_PER_BATCH
        logging.warning(
            f"OpenAI enforces a limit of {OPENAI_MAX_CHUNKS_PER_BATCH} chunks per batch. "
            "Overwriting embeddings.chunks_per_batch."
        )

    chunks_per_job = args.tokens_per_chunk * args.chunks_per_batch
    if chunks_per_job >= OPENAI_MAX_TOKENS_PER_JOB:
        raise ValueError(f"The maximum number of chunks per job is {OPENAI_MAX_TOKENS_PER_JOB}. Got {chunks_per_job}")


def _validate_voyage_embedding_args(args):
    """Validates the configuration of the Voyage batch embedder and sets defaults."""
    if args.embedding_provider == "voyage" and not os.getenv("VOYAGE_API_KEY"):
        raise ValueError("Please set the VOYAGE_API_KEY environment variable.")

    if not args.embedding_model:
        args.embedding_model = "voyage-code-2"

    if not args.tokens_per_chunk:
        # https://arxiv.org/pdf/2406.14497 recommends a value between 200-800.
        args.tokens_per_chunk = 800

    if not args.chunks_per_batch:
        args.chunks_per_batch = VOYAGE_MAX_CHUNKS_PER_BATCH
    elif args.chunks_per_batch > VOYAGE_MAX_CHUNKS_PER_BATCH:
        args.chunks_per_batch = VOYAGE_MAX_CHUNKS_PER_BATCH
        logging.warning(f"Voyage enforces a limit of {VOYAGE_MAX_CHUNKS_PER_BATCH} chunks per batch. Overwriting.")

    max_tokens = get_voyage_max_tokens_per_batch(args.embedding_model)
    if args.tokens_per_chunk * args.chunks_per_batch > max_tokens:
        raise ValueError(
            f"Voyage enforces a limit of {max_tokens} tokens per batch. "
            "Reduce either --tokens-per-chunk or --chunks-per-batch."
        )

    if not args.embedding_size:
        args.embedding_size = get_voyage_embedding_size(args.embedding_model)


def _validate_marqo_embedding_args(args):
    """Validates the configuration of the Marqo batch embedder and sets defaults."""
    if not args.embedding_model:
        args.embedding_model = "hf/e5-base-v2"

    if not args.chunks_per_batch:
        args.chunks_per_batch = MARQO_MAX_CHUNKS_PER_BATCH
    elif args.chunks_per_batch > MARQO_MAX_CHUNKS_PER_BATCH:
        args.chunks_per_batch = MARQO_MAX_CHUNKS_PER_BATCH
        logging.warning(
            f"Marqo enforces a limit of {MARQO_MAX_CHUNKS_PER_BATCH} chunks per batch. "
            "Overwriting embeddings.chunks_per_batch."
        )


def _validate_gemini_embedding_args(args):
    """Validates the configuration of the Gemini batch embedder and sets defaults."""
    if not args.embedding_model:
        args.embedding_model = "models/text-embedding-004"
    assert os.environ[
        "GOOGLE_API_KEY"
    ], "Please set the GOOGLE_API_KEY environment variable if using `gemini` embeddings."
    if not args.chunks_per_batch:
        # This value is reasonable but arbitrary (i.e. Gemini does not explicitly enforce a limit).
        args.chunks_per_batch = 2000

    if not args.tokens_per_chunk:
        args.tokens_per_chunk = GEMINI_MAX_TOKENS_PER_CHUNK
    if not args.embedding_size:
        args.embedding_size = 768


def validate_embedding_args(args):
    """Validates the configuration of the batch embedder and sets defaults."""
    if args.llm_retriever:
        # When using an LLM to retrieve, we are not running the embedder.
        return True
    if args.embedding_provider == "openai":
        _validate_openai_embedding_args(args)
    elif args.embedding_provider == "voyage":
        _validate_voyage_embedding_args(args)
    elif args.embedding_provider == "marqo":
        _validate_marqo_embedding_args(args)
    elif args.embedding_provider == "gemini":
        _validate_gemini_embedding_args(args)
    else:
        raise ValueError(f"Unrecognized --embedding-provider={args.embedding_provider}")


def validate_vector_store_args(args):
    """Validates the configuration of the vector store and sets defaults."""
    if args.llm_retriever:
        if not os.getenv("ANTHROPIC_API_KEY"):
            print(
                "going to use TOGETHER_API_KEY environment variable to use the LLM retriever. "
            )
        
            if not os.getenv("TOGETHER_API_KEY"):
                raise ValueError(
                    "Please set the TOGETHER_API_KEY environment variable to use the LLM retriever. "
                    "(We're constrained to Claude because we need prompt caching.)"
                )

        if args.index_issues:
            # The LLM retriever only makes sense on the code repository, since it passes file paths to the LLM.
            raise ValueError("Cannot use --index-issues with --llm-retriever.")

        # When using an LLM retriever, all the vector store arguments are ignored.
        return

    if not args.index_namespace:
        # Attempt to derive a default index namespace from the repository information.
        if "repo_id" not in args:
            raise ValueError("Please set a value for --index-namespace.")
        args.index_namespace = args.repo_id
        if "commit_hash" in args and args.commit_hash:
            args.index_namespace += "/" + args.commit_hash
        if args.vector_store_provider == "marqo":
            # Marqo namespaces must match this pattern: [a-zA-Z_-][a-zA-Z0-9_-]*
            args.index_namespace = re.sub(r"[^a-zA-Z0-9_-]", "_", args.index_namespace)

    if args.vector_store_provider == "marqo":
        if not args.marqo_url:
            args.marqo_url = "http://localhost:8882"
        if "/" in args.index_namespace:
            raise ValueError(f"Marqo doesn't allow slashes in --index-namespace={args.index_namespace}.")

    elif args.vector_store_provider == "pinecone":
        if not os.getenv("PINECONE_API_KEY"):
            raise ValueError("Please set the PINECONE_API_KEY environment variable.")
        if not args.index_name:
            raise ValueError(f"Please set the vector_store.index_name value.")


def validate_indexing_args(args):
    """Validates the indexing configuration and sets defaults."""
    if args.include and args.exclude:
        raise ValueError("At most one of indexing.include and indexing.exclude can be specified.")
    if not args.include and not args.exclude:
        args.exclude = str(resources.files("sage").joinpath("sample-exclude.txt"))
    if args.include and not os.path.exists(args.include):
        raise ValueError(f"Path --include={args.include} does not exist.")
    if args.exclude and not os.path.exists(args.exclude):
        raise ValueError(f"Path --exclude={args.exclude} does not exist.")
    if not args.index_repo and not args.index_issues:
        raise ValueError("Either --index_repo or --index_issues must be set to true.")
    if args.index_issues and not os.getenv("GITHUB_TOKEN"):
        raise ValueError("Please set the GITHUB_TOKEN environment variable.")

```

`/Users/sauravverma/programs/pyano/sage/sage/reranker.py`:

```py
import os
from enum import Enum
from typing import Optional

from langchain.retrievers.document_compressors import CrossEncoderReranker
from langchain_cohere import CohereRerank
from langchain_community.cross_encoders import HuggingFaceCrossEncoder
from langchain_community.document_compressors import JinaRerank
from langchain_core.documents import BaseDocumentCompressor
from langchain_nvidia_ai_endpoints import NVIDIARerank
from langchain_voyageai import VoyageAIRerank


class RerankerProvider(Enum):
    NONE = "none"
    HUGGINGFACE = "huggingface"
    COHERE = "cohere"
    NVIDIA = "nvidia"
    JINA = "jina"
    VOYAGE = "voyage"


def build_reranker(provider: str, model: Optional[str] = None, top_k: int = 5) -> Optional[BaseDocumentCompressor]:
    if provider == RerankerProvider.NONE.value:
        return None

    api_key_env_vars = {
        RerankerProvider.COHERE.value: "COHERE_API_KEY",
        RerankerProvider.NVIDIA.value: "NVIDIA_API_KEY",
        RerankerProvider.JINA.value: "JINA_API_KEY",
        RerankerProvider.VOYAGE.value: "VOYAGE_API_KEY",
    }

    provider_defaults = {
        RerankerProvider.HUGGINGFACE.value: "cross-encoder/ms-marco-MiniLM-L-6-v2",
        RerankerProvider.COHERE.value: "rerank-english-v3.0",
        RerankerProvider.NVIDIA.value: "nvidia/nv-rerankqa-mistral-4b-v3",
        RerankerProvider.VOYAGE.value: "rerank-1",
    }

    model = model or provider_defaults.get(provider)

    if provider == RerankerProvider.HUGGINGFACE.value:
        encoder_model = HuggingFaceCrossEncoder(model_name=model)
        return CrossEncoderReranker(model=encoder_model, top_n=top_k)

    if provider in api_key_env_vars:
        api_key = os.getenv(api_key_env_vars[provider])
        if not api_key:
            raise ValueError(f"Please set the {api_key_env_vars[provider]} environment variable")

        if provider == RerankerProvider.COHERE.value:
            return CohereRerank(model=model, cohere_api_key=api_key, top_n=top_k)

        if provider == RerankerProvider.NVIDIA.value:
            return NVIDIARerank(model=model, api_key=api_key, top_n=top_k, truncate="END")

        if provider == RerankerProvider.JINA.value:
            return JinaRerank(top_n=top_k)

        if provider == RerankerProvider.VOYAGE.value:
            return VoyageAIRerank(model=model, api_key=api_key, top_k=top_k)

    raise ValueError(f"Invalid reranker provider: {provider}")

```

`/Users/sauravverma/programs/pyano/sage/sage/index.py`:

```py
"""Runs a batch job to compute embeddings for an entire repo and stores them into a vector store."""

import logging
import os
import time

import configargparse

import sage.config as sage_config
from sage.chunker import UniversalFileChunker
from sage.data_manager import GitHubRepoManager
from sage.embedder import build_batch_embedder_from_flags
from sage.github import GitHubIssuesChunker, GitHubIssuesManager
from sage.vector_store import VectorStoreProvider, build_vector_store_from_args

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger()
logger.setLevel(logging.INFO)


def main():
    parser = configargparse.ArgParser(
        description="Batch-embeds a GitHub repository and its issues.", ignore_unknown_config_file_keys=True
    )
    sage_config.add_config_args(parser)

    arg_validators = [
        sage_config.add_repo_args(parser),
        sage_config.add_embedding_args(parser),
        sage_config.add_vector_store_args(parser),
        sage_config.add_indexing_args(parser),
    ]

    args = parser.parse_args()

    for validator in arg_validators:
        validator(args)

    if args.llm_retriever:
        logging.warning("The LLM retriever does not require indexing, so this script is a no-op.")
        return

    # Additionally validate embedder and vector store compatibility.
    vector_store_providers = [member.value for member in VectorStoreProvider]
    if args.embedding_provider == "openai" and args.vector_store_provider not in vector_store_providers:
        parser.error(
            f"When using OpenAI embedder, the vector store type must be from the list {vector_store_providers}."
        )
    if args.embedding_provider == "marqo" and args.vector_store_provider != "marqo":
        parser.error("When using the marqo embedder, the vector store type must also be marqo.")

    ######################
    # Step 1: Embeddings #
    ######################

    # Index the repository.
    repo_embedder = None
    if args.index_repo:
        logging.info("Cloning the repository...")
        repo_manager = GitHubRepoManager.from_args(args)
        logging.info("Embedding the repo...")
        chunker = UniversalFileChunker(max_tokens=args.tokens_per_chunk)
        repo_embedder = build_batch_embedder_from_flags(repo_manager, chunker, args)
        repo_jobs_file = repo_embedder.embed_dataset(args.chunks_per_batch, args.max_embedding_jobs)

    # Index the GitHub issues.
    issues_embedder = None
    if args.index_issues:
        logging.info("Issuing embedding jobs for GitHub issues...")
        issues_manager = GitHubIssuesManager(
            args.repo_id, access_token=os.getenv("GITHUB_TOKEN"), index_comments=args.index_issue_comments
        )
        issues_manager.download()
        logging.info("Embedding GitHub issues...")
        chunker = GitHubIssuesChunker(max_tokens=args.tokens_per_chunk)
        issues_embedder = build_batch_embedder_from_flags(issues_manager, chunker, args)
        issues_jobs_file = issues_embedder.embed_dataset(args.chunks_per_batch, args.max_embedding_jobs)

    ########################
    # Step 2: Vector Store #
    ########################

    if args.vector_store_provider == "marqo":
        # Marqo computes embeddings and stores them in the vector store at once, so we're done.
        logging.info("Done!")
        return

    if repo_embedder is not None:
        logging.info("Waiting for repo embeddings to be ready...")
        while not repo_embedder.embeddings_are_ready(repo_jobs_file):
            logging.info("Sleeping for 30 seconds...")
            time.sleep(30)

        logging.info("Moving embeddings to the repo vector store...")
        repo_vector_store = build_vector_store_from_args(args, repo_manager)
        repo_vector_store.ensure_exists()
        repo_vector_store.upsert(repo_embedder.download_embeddings(repo_jobs_file), namespace=args.index_namespace)

    if issues_embedder is not None:
        logging.info("Waiting for issue embeddings to be ready...")
        while not issues_embedder.embeddings_are_ready(issues_jobs_file):
            logging.info("Sleeping for 30 seconds...")
            time.sleep(30)

        logging.info("Moving embeddings to the issues vector store...")
        issues_vector_store = build_vector_store_from_args(args, issues_manager)
        issues_vector_store.ensure_exists()
        issues_vector_store.upsert(
            issues_embedder.download_embeddings(issues_jobs_file), namespace=args.index_namespace
        )

    logging.info("Done!")


if __name__ == "__main__":
    main()

```

`/Users/sauravverma/programs/pyano/sage/sage/chunker.py`:

```py
"""Chunker abstraction and implementations."""

import logging
import os
from abc import ABC, abstractmethod
from dataclasses import dataclass
from functools import cached_property
from typing import Any, Dict, List, Optional

import nbformat
import pygments
import tiktoken
from semchunk import chunk as chunk_via_semchunk
from tree_sitter import Node
from tree_sitter_language_pack import get_parser

from sage.constants import TEXT_FIELD

logger = logging.getLogger(__name__)
tokenizer = tiktoken.get_encoding("cl100k_base")


class Chunk:
    @abstractmethod
    def content(self) -> str:
        """The content of the chunk to be indexed."""

    @abstractmethod
    def metadata(self) -> Dict:
        """Metadata for the chunk to be indexed."""


@dataclass
class FileChunk(Chunk):
    """A chunk of code or text extracted from a file in the repository."""

    file_content: str  # The content of the entire file, not just this chunk.
    file_metadata: Dict  # Metadata of the entire file, not just this chunk.
    start_byte: int
    end_byte: int

    @cached_property
    def filename(self):
        if not "file_path" in self.file_metadata:
            raise ValueError("file_metadata must contain a 'file_path' key.")
        return self.file_metadata["file_path"]

    @cached_property
    def content(self) -> Optional[str]:
        """The text content to be embedded. Might contain information beyond just the text snippet from the file."""
        return self.filename + "\n\n" + self.file_content[self.start_byte : self.end_byte]

    @cached_property
    def metadata(self):
        """Converts the chunk to a dictionary that can be passed to a vector store."""
        # Some vector stores require the IDs to be ASCII.
        filename_ascii = self.filename.encode("ascii", "ignore").decode("ascii")
        chunk_metadata = {
            # Some vector stores require the IDs to be ASCII.
            "id": f"{filename_ascii}_{self.start_byte}_{self.end_byte}",
            "start_byte": self.start_byte,
            "end_byte": self.end_byte,
            "length": self.end_byte - self.start_byte,
            # Note to developer: When choosing a large chunk size, you might exceed the vector store's metadata
            # size limit. In that case, you can simply store the start/end bytes above, and fetch the content
            # directly from the repository when needed.
            TEXT_FIELD: self.content,
        }
        chunk_metadata.update(self.file_metadata)
        return chunk_metadata

    @cached_property
    def num_tokens(self):
        """Number of tokens in this chunk."""
        return len(tokenizer.encode(self.content, disallowed_special=()))

    def __eq__(self, other):
        if isinstance(other, Chunk):
            return (
                self.filename == other.filename
                and self.start_byte == other.start_byte
                and self.end_byte == other.end_byte
            )
        return False

    def __hash__(self):
        return hash((self.filename, self.start_byte, self.end_byte))


class Chunker(ABC):
    """Abstract class for chunking a datum into smaller pieces."""

    @abstractmethod
    def chunk(self, content: Any, metadata: Dict) -> List[Chunk]:
        """Chunks a datum into smaller pieces."""


class CodeFileChunker(Chunker):
    """Splits a code file into chunks of at most `max_tokens` tokens each."""

    def __init__(self, max_tokens: int):
        self.max_tokens = max_tokens
        self.text_chunker = TextFileChunker(max_tokens)

    @staticmethod
    def _get_language_from_filename(filename: str):
        """Returns a canonical name for the language of the file, based on its extension.
        Returns None if the language is unknown to the pygments lexer.
        """
        # pygments doesn't recognize .tsx files and returns None. So we need to special-case them.
        extension = os.path.splitext(filename)[1]
        if extension == ".tsx":
            return "tsx"

        try:
            lexer = pygments.lexers.get_lexer_for_filename(filename)
            return lexer.name.lower()
        except pygments.util.ClassNotFound:
            return None

    def _chunk_node(self, node: Node, file_content: str, file_metadata: Dict) -> List[FileChunk]:
        """Splits a node in the parse tree into a flat list of chunks."""
        node_chunk = FileChunk(file_content, file_metadata, node.start_byte, node.end_byte)

        if node_chunk.num_tokens <= self.max_tokens:
            return [node_chunk]

        if not node.children:
            # This is a leaf node, but it's too long. We'll have to split it with a text tokenizer.
            return self.text_chunker.chunk(file_content[node.start_byte : node.end_byte], file_metadata)

        chunks = []
        for child in node.children:
            chunks.extend(self._chunk_node(child, file_content, file_metadata))

        for chunk in chunks:
            # This should always be true. Otherwise there must be a bug in the code.
            assert chunk.num_tokens <= self.max_tokens

        # Merge neighboring chunks if their combined size doesn't exceed max_tokens. The goal is to avoid pathologically
        # small chunks that end up being undeservedly preferred by the retriever.
        merged_chunks = []
        for chunk in chunks:
            if not merged_chunks:
                merged_chunks.append(chunk)
            elif merged_chunks[-1].num_tokens + chunk.num_tokens < self.max_tokens - 50:
                # There's a good chance that merging these two chunks will be under the token limit. We're not 100% sure
                # at this point, because tokenization is not necessarily additive.
                merged = FileChunk(
                    file_content,
                    file_metadata,
                    merged_chunks[-1].start_byte,
                    chunk.end_byte,
                )
                if merged.num_tokens <= self.max_tokens:
                    merged_chunks[-1] = merged
                else:
                    merged_chunks.append(chunk)
            else:
                merged_chunks.append(chunk)
        chunks = merged_chunks

        for chunk in merged_chunks:
            # This should always be true. Otherwise there's a bug worth investigating.
            assert chunk.num_tokens <= self.max_tokens

        return merged_chunks

    @staticmethod
    def is_code_file(filename: str) -> bool:
        """Checks whether pygment & tree_sitter can parse the file as code."""
        language = CodeFileChunker._get_language_from_filename(filename)
        return language and language not in ["text only", "None"]

    @staticmethod
    def parse_tree(filename: str, content: str) -> List[str]:
        """Parses the code in a file and returns the parse tree."""
        language = CodeFileChunker._get_language_from_filename(filename)

        if not language or language in ["text only", "None"]:
            logging.debug("%s doesn't seem to be a code file.", filename)
            return None

        try:
            parser = get_parser(language)
        except LookupError:
            logging.debug("%s doesn't seem to be a code file.", filename)
            return None
        # This should never happen unless there's a bug in the code, but we'd rather not crash.
        except Exception as e:
            logging.warn("Failed to get parser for %s: %s", filename, e)
            return None

        tree = parser.parse(bytes(content, "utf8"))

        if not tree.root_node.children or tree.root_node.children[0].type == "ERROR":
            logging.warning("Failed to parse code in %s.", filename)
            return None
        return tree

    def chunk(self, content: Any, metadata: Dict) -> List[Chunk]:
        """Chunks a code file into smaller pieces."""
        file_content = content
        file_metadata = metadata
        file_path = metadata["file_path"]

        if not file_content.strip():
            return []

        tree = self.parse_tree(file_path, file_content)
        if tree is None:
            return []

        file_chunks = self._chunk_node(tree.root_node, file_content, file_metadata)
        for chunk in file_chunks:
            # Make sure that the chunk has content and doesn't exceed the max_tokens limit. Otherwise there must be
            # a bug in the code.
            assert (
                chunk.num_tokens <= self.max_tokens
            ), f"Chunk size {chunk.num_tokens} exceeds max_tokens {self.max_tokens}."

        return file_chunks


class TextFileChunker(Chunker):
    """Wrapper around semchunk: https://github.com/umarbutler/semchunk."""

    def __init__(self, max_tokens: int):
        self.max_tokens = max_tokens
        self.count_tokens = lambda text: len(tokenizer.encode(text, disallowed_special=()))

    def chunk(self, content: Any, metadata: Dict) -> List[Chunk]:
        """Chunks a text file into smaller pieces."""
        file_content = content
        file_metadata = metadata
        file_path = file_metadata["file_path"]

        # We need to allocate some tokens for the filename, which is part of the chunk content.
        extra_tokens = self.count_tokens(file_path + "\n\n")
        text_chunks = chunk_via_semchunk(file_content, self.max_tokens - extra_tokens, self.count_tokens)

        file_chunks = []
        start = 0
        for text_chunk in text_chunks:
            # This assertion should always be true. Otherwise there's a bug worth finding.
            assert self.count_tokens(text_chunk) <= self.max_tokens - extra_tokens

            # Find the start/end positions of the chunks.
            start = file_content.index(text_chunk, start)
            if start == -1:
                logging.warning("Couldn't find semchunk in content: %s", text_chunk)
            else:
                end = start + len(text_chunk)
                file_chunks.append(FileChunk(file_content, file_metadata, start, end))

            start = end

        return file_chunks


class IpynbFileChunker(Chunker):
    """Extracts the python code from a Jupyter notebook, removing all the boilerplate.

    Based on https://github.com/GoogleCloudPlatform/generative-ai/blob/main/language/code/code_retrieval_augmented_generation.ipynb
    """

    def __init__(self, code_chunker: CodeFileChunker):
        self.code_chunker = code_chunker

    def chunk(self, content: Any, metadata: Dict) -> List[Chunk]:
        filename = metadata["file_path"]

        if not filename.lower().endswith(".ipynb"):
            logging.warn("IPYNBChunker is only for .ipynb files.")
            return []

        notebook = nbformat.reads(content, as_version=nbformat.NO_CONVERT)
        python_code = "\n".join([cell.source for cell in notebook.cells if cell.cell_type == "code"])

        tmp_metadata = {"file_path": filename.replace(".ipynb", ".py")}
        chunks = self.code_chunker.chunk(python_code, tmp_metadata)

        for chunk in chunks:
            # Update filenames back to .ipynb
            chunk.metadata["file_path"] = filename
        return chunks


class UniversalFileChunker(Chunker):
    """Chunks a file into smaller pieces, regardless of whether it's code or text."""

    def __init__(self, max_tokens: int):
        self.max_tokens = max_tokens
        self.code_chunker = CodeFileChunker(max_tokens)
        self.ipynb_chunker = IpynbFileChunker(self.code_chunker)
        self.text_chunker = TextFileChunker(max_tokens)

    def chunk(self, content: Any, metadata: Dict) -> List[Chunk]:
        if not "file_path" in metadata:
            raise ValueError("metadata must contain a 'file_path' key.")
        file_path = metadata["file_path"]

        # Figure out the appropriate chunker to use.
        if file_path.lower().endswith(".ipynb"):
            chunker = self.ipynb_chunker
        elif CodeFileChunker.is_code_file(file_path):
            chunker = self.code_chunker
        else:
            chunker = self.text_chunker

        return chunker.chunk(content, metadata)

```

`/Users/sauravverma/programs/pyano/sage/sage/vector_store.py`:

```py
"""Vector store abstraction and implementations."""

import logging
import os
from abc import ABC, abstractmethod
from enum import Enum
from functools import cached_property
from typing import Dict, Generator, List, Optional, Tuple
from uuid import uuid4

import chromadb
import faiss
import marqo
import nltk
from langchain.retrievers import EnsembleRetriever
from langchain_chroma import Chroma as LangChainChroma
from langchain_community.docstore.in_memory import InMemoryDocstore
from langchain_community.retrievers import BM25Retriever
from langchain_community.vectorstores import FAISS, Marqo
from langchain_community.vectorstores import Pinecone as LangChainPinecone
from langchain_core.documents import Document
from langchain_core.embeddings import Embeddings
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_milvus import Milvus
from langchain_openai import OpenAIEmbeddings
from langchain_qdrant import QdrantVectorStore as LangChainQdrant
from langchain_voyageai import VoyageAIEmbeddings
from nltk.data import find
from pinecone import Pinecone, ServerlessSpec
from pinecone_text.sparse import BM25Encoder
from qdrant_client import QdrantClient
from qdrant_client.http.models import Distance, VectorParams

from sage.constants import TEXT_FIELD
from sage.data_manager import DataManager

Vector = Tuple[Dict, List[float]]  # (metadata, embedding)


class VectorStoreProvider(Enum):
    PINECONE = "pinecone"
    MARQO = "marqo"
    CHROMA = "chroma"
    FAISS = "faiss"
    MILVUS = "milvus"
    QDRANT = "qdrant"


def is_punkt_downloaded():
    try:
        find("tokenizers/punkt_tab")
        return True
    except LookupError:
        return False


class VectorStore(ABC):
    """Abstract class for a vector store."""

    @abstractmethod
    def ensure_exists(self):
        """Ensures that the vector store exists. Creates it if it doesn't."""

    @abstractmethod
    def upsert_batch(self, vectors: List[Vector], namespace: str):
        """Upserts a batch of vectors."""

    def upsert(self, vectors: Generator[Vector, None, None], namespace: str):
        """Upserts in batches of 100, since vector stores have a limit on upsert size."""
        batch = []
        for metadata, embedding in vectors:
            batch.append((metadata, embedding))
            if len(batch) == 100:
                self.upsert_batch(batch, namespace)
                batch = []
        if batch:
            self.upsert_batch(batch, namespace)

    @abstractmethod
    def as_retriever(self, top_k: int, embeddings: Embeddings, namespace: str):
        """Converts the vector store to a LangChain retriever object."""


class PineconeVectorStore(VectorStore):
    """Vector store implementation using Pinecone."""

    def __init__(self, index_name: str, dimension: int, alpha: float, bm25_cache: Optional[str] = None):
        """
        Args:
            index_name: The name of the Pinecone index to use. If it doesn't exist already, we'll create it.
            dimension: The dimension of the vectors.
            alpha: The alpha parameter for hybrid search: alpha == 1.0 means pure dense search, alpha == 0.0 means pure
                BM25, and 0.0 < alpha < 1.0 means a hybrid of the two.
            bm25_cache: The path to the BM25 encoder file. If not specified, we'll use the default BM25 (fitted on the
                MS MARCO dataset).
        """
        self.index_name = index_name
        self.dimension = dimension
        self.client = Pinecone()
        self.alpha = alpha
        if alpha < 1.0:
            if bm25_cache and os.path.exists(bm25_cache):
                logging.info("Loading BM25 encoder from cache.")
                # We need nltk tokenizers for bm25 tokenization
                if is_punkt_downloaded():
                    print("punkt is already downloaded")
                else:
                    print("punkt is not downloaded")
                    # Optionally download it
                    nltk.download("punkt_tab")
                self.bm25_encoder = BM25Encoder()
                self.bm25_encoder.load(path=bm25_cache)
            else:
                logging.info("Using default BM25 encoder (fitted to MS MARCO).")
                self.bm25_encoder = BM25Encoder.default()
        else:
            self.bm25_encoder = None

    @cached_property
    def index(self):
        self.ensure_exists()
        index = self.client.Index(self.index_name)

        # Hack around the fact that PineconeRetriever expects the content of the chunk to be in a "text" field,
        # while PineconeHybridSearchRetrieve expects it to be in a "context" field.
        original_query = index.query

        def patched_query(*args, **kwargs):
            result = original_query(*args, **kwargs)
            for res in result["matches"]:
                if TEXT_FIELD in res["metadata"]:
                    res["metadata"]["context"] = res["metadata"][TEXT_FIELD]
            return result

        index.query = patched_query
        return index

    def ensure_exists(self):
        if self.index_name not in self.client.list_indexes().names():
            self.client.create_index(
                name=self.index_name,
                dimension=self.dimension,
                # See https://www.pinecone.io/learn/hybrid-search-intro/
                metric="dotproduct" if self.bm25_encoder else "cosine",
                spec=ServerlessSpec(cloud="aws", region="us-east-1"),
            )

    def upsert_batch(self, vectors: List[Vector], namespace: str):
        pinecone_vectors = []
        for i, (metadata, embedding) in enumerate(vectors):
            vector = {"id": metadata.get("id", str(i)), "values": embedding, "metadata": metadata}
            if self.bm25_encoder:
                vector["sparse_values"] = self.bm25_encoder.encode_documents(metadata[TEXT_FIELD])
            pinecone_vectors.append(vector)

        self.index.upsert(vectors=pinecone_vectors, namespace=namespace)

    def as_retriever(self, top_k: int, embeddings: Embeddings, namespace: str):
        bm25_retriever = (
            BM25Retriever(
                embeddings=embeddings,
                sparse_encoder=self.bm25_encoder,
                index=self.index,
                namespace=namespace,
                top_k=top_k,
            )
            if self.bm25_encoder
            else None
        )

        dense_retriever = LangChainPinecone.from_existing_index(
            index_name=self.index_name, embedding=embeddings, namespace=namespace
        ).as_retriever(search_kwargs={"k": top_k})

        if bm25_retriever:
            return EnsembleRetriever(retrievers=[dense_retriever, bm25_retriever], weights=[self.alpha, 1 - self.alpha])
        else:
            return dense_retriever


class ChromaVectorStore(VectorStore):
    """Vector store implementation using ChromaDB"""

    def __init__(self, index_name: str, alpha: float = None, bm25_cache: Optional[str] = None):
        """
        Args:
            index_name: The name of the Chroma collection/index to use. If it doesn't exist already, we'll create it.
            alpha: The alpha parameter for hybrid search: alpha == 1.0 means pure dense search, alpha == 0.0 means pure
                BM25, and 0.0 < alpha < 1.0 means a hybrid of the two.
        """
        self.index_name = index_name
        self.alpha = alpha
        self.client = chromadb.PersistentClient()

    @cached_property
    def index(self):
        index = self.client.get_or_create_collection(self.index_name)
        return index

    def ensure_exists(self):
        pass

    def upsert_batch(self, vectors: List[Vector], namespace: str):
        del namespace

        ids = []
        embeddings = []
        metadatas = []
        documents = []

        for i, (metadata, embedding) in enumerate(vectors):
            ids.append(metadata.get("id", str(i)))
            embeddings.append(embedding)
            metadatas.append(metadata)
            documents.append(metadata[TEXT_FIELD])

        self.index.upsert(ids=ids, embeddings=embeddings, metadatas=metadatas, documents=documents)

    def as_retriever(self, top_k: int, embeddings: Embeddings = None, namespace: str = None):
        vector_store = LangChainChroma(
            collection_name=self.index_name, embedding_function=embeddings, client=self.client
        )

        return vector_store.as_retriever(search_kwargs={"k": top_k})


class FAISSVectorStore(VectorStore):
    """Vector store implementation using FAISS"""

    def __init__(self, index_name: str, dimension: int, embeddings: Embeddings = None):
        """
        Args:
            index_name: The name of the FAISS index to use. If it doesn't exist already, we'll create it.
            dimension: The dimension of the vectors.
            embeddings: The embedding function used to generate embeddings
        """
        self.index_name = index_name
        self.dimension = dimension
        self.embeddings = embeddings

        # check if the index exists
        if os.path.exists(self.index_name):
            # load the existing index
            self.vector_store = FAISS.load_local(
                folder_path=self.index_name, embeddings=self.embeddings, allow_dangerous_deserialization=True
            )
        # else create a new index
        else:
            self.vector_store = FAISS(
                embedding_function=self.embeddings,
                index=self.index,
                docstore=InMemoryDocstore(),
                index_to_docstore_id={},
            )

    @cached_property
    def index(self):
        index = faiss.IndexFlatL2(self.dimension)
        return index

    def ensure_exists(self):
        pass

    def upsert_batch(self, vectors: List[Vector], namespace: str):
        del namespace

        ids = []
        documents = []

        for i, (meta_data, embedding) in enumerate(vectors):
            ids.append(meta_data.get("id", str(i)))
            document = Document(page_content=meta_data[TEXT_FIELD], metadata=meta_data)
            documents.append(document)

        self.vector_store.add_documents(documents=documents, ids=ids)

        # saving the index after every batch upsert
        self.vector_store.save_local(self.index_name)
        print("Save Local Executed")
        logging.error("Save Local Got Executed")

    def as_retriever(self, top_k, embeddings, namespace):
        del embeddings
        del namespace

        return self.vector_store.as_retriever(search_kwards={"k": top_k})


class MilvusVectorStore(VectorStore):
    """Vector store implementation using Milvus"""

    def __init__(self, uri: str, index_name: str, embeddings: Embeddings = None):
        """
        Args:
            index_name: The name of the Milvus collection to use. If it doesn't exist already, we'll create it.
            embeddings: The embedding function used to generate embeddings
        """
        self.uri = uri
        self.index_name = index_name
        self.embeddings = embeddings

        self.vector_store = Milvus(
            embedding_function=embeddings, connection_args={"uri": self.uri}, collection_name=self.index_name
        )

    def ensure_exists(self):
        pass

    def upsert_batch(self, vectors: List[Vector], namespace: str):
        del namespace

        ids = []
        documents = []

        for i, (meta_data, embedding) in enumerate(vectors):
            ids.append(meta_data.get("id", str(i)))
            # "text" is a reserved keyword. So removing it
            page_content = meta_data[TEXT_FIELD]
            meta_data["content"] = meta_data[TEXT_FIELD]
            del meta_data[TEXT_FIELD]

            document = Document(page_content=page_content, metadata=meta_data)
            documents.append(document)

        self.vector_store.add_documents(documents=documents, ids=ids)

    def as_retriever(self, top_k, embeddings, namespace):
        del embeddings
        del namespace

        return self.vector_store.as_retriever(search_kwards={"k": top_k})


class QdrantVectorStore(VectorStore):
    """Vector store implementation using Qdrant"""

    def __init__(self, index_name: str, dimension: int, embeddings: Embeddings = None):
        """
        Args:
            index_name: The name of the Qdrant collection to use. If it doesn't exist already, we'll create it.
            embeddings: The embedding function used to generate embeddings
        """
        self.index_name = index_name
        self.dimension = dimension
        self.embeddings = embeddings
        self.client = QdrantClient(path="qdrantdb")
        self.vector_store = self.index

    @cached_property
    def index(self):
        self.ensure_exists()
        vector_store = LangChainQdrant(client=self.client, collection_name=self.index_name, embedding=self.embeddings)
        return vector_store

    def ensure_exists(self):
        if not self.client.collection_exists(self.index_name):
            self.client.create_collection(
                collection_name=self.index_name,
                vectors_config=VectorParams(size=self.dimension, distance=Distance.COSINE),
            )

    def upsert_batch(self, vectors: List[Vector], namespace: str):
        del namespace

        ids = []
        documents = []

        for i, (meta_data, embedding) in enumerate(vectors):
            ids.append(str(uuid4()))
            document = Document(page_content=meta_data[TEXT_FIELD], metadata=meta_data)
            documents.append(document)

        self.vector_store.add_documents(documents=documents, ids=ids)

    def as_retriever(self, top_k, embeddings, namespace):
        del embeddings
        del namespace

        return self.vector_store.as_retriever(search_kwards={"k": top_k})


class MarqoVectorStore(VectorStore):
    """Vector store implementation using Marqo."""

    def __init__(self, url: str, index_name: str):
        self.client = marqo.Client(url=url)
        self.index_name = index_name

    def ensure_exists(self):
        pass

    def upsert_batch(self, vectors: List[Vector], namespace: str):
        # Since Marqo is both an embedder and a vector store, the embedder is already doing the upsert.
        pass

    def as_retriever(self, top_k: int, embeddings: Embeddings = None, namespace: str = None):
        del embeddings  # Unused; The Marqo vector store is also an embedder.
        del namespace  # Unused; Unlike Pinecone, Marqo doesn't differentiate between index name and namespace.

        vectorstore = Marqo(client=self.client, index_name=self.index_name)

        # Monkey-patch the _construct_documents_from_results_without_score method to not expect a "metadata" field in
        # the result, and instead take the "filename" directly from the result.
        def patched_method(self, results):
            documents: List[Document] = []
            for result in results["hits"]:
                content = result.pop(TEXT_FIELD)
                documents.append(Document(page_content=content, metadata=result))
            return documents

        vectorstore._construct_documents_from_results_without_score = patched_method.__get__(
            vectorstore, vectorstore.__class__
        )
        return vectorstore.as_retriever(search_kwargs={"k": top_k})


def build_vector_store_from_args(
    args: dict,
    data_manager: Optional[DataManager] = None,
) -> VectorStore:
    """Builds a vector store from the given command-line arguments.

    When `data_manager` is specified and hybrid retrieval is requested, we'll use it to fit a BM25 encoder on the corpus
    of documents.
    """
    if args.embedding_provider == "openai":
        embeddings = OpenAIEmbeddings(model=args.embedding_model)
    elif args.embedding_provider == "voyage":
        embeddings = VoyageAIEmbeddings(model=args.embedding_model)
    elif args.embedding_provider == "gemini":
        embeddings = GoogleGenerativeAIEmbeddings(model=args.embedding_model)

    if args.vector_store_provider == "pinecone":
        bm25_cache = os.path.join(".bm25_cache", args.index_namespace, "bm25_encoder.json")
        if args.retrieval_alpha < 1.0 and not os.path.exists(bm25_cache) and data_manager:
            logging.info("Fitting BM25 encoder on the corpus...")
            if is_punkt_downloaded():
                print("punkt is already downloaded")
            else:
                print("punkt is not downloaded")
                # Optionally download it
                nltk.download("punkt_tab")
            corpus = [content for content, _ in data_manager.walk()]
            bm25_encoder = BM25Encoder()
            bm25_encoder.fit(corpus)
            # Make sure the folder exists, before we dump the encoder.
            bm25_folder = os.path.dirname(bm25_cache)
            if not os.path.exists(bm25_folder):
                os.makedirs(bm25_folder)
            bm25_encoder.dump(bm25_cache)

        return PineconeVectorStore(
            index_name=args.index_name,
            dimension=args.embedding_size if "embedding_size" in args else None,
            alpha=args.retrieval_alpha,
            bm25_cache=bm25_cache,
        )
    elif args.vector_store_provider == "chroma":
        return ChromaVectorStore(
            index_name=args.index_name,
        )
    elif args.vector_store_provider == "faiss":
        return FAISSVectorStore(index_name=args.index_name, dimension=args.embedding_size, embeddings=embeddings)
    elif args.vector_store_provider == "milvus":
        return MilvusVectorStore(uri=args.milvus_uri, index_name=args.index_name, embeddings=embeddings)
    elif args.vector_store_provider == "qdrant":
        return QdrantVectorStore(index_name=args.index_name, dimension=args.embedding_size, embeddings=embeddings)
    elif args.vector_store_provider == "marqo":
        return MarqoVectorStore(url=args.marqo_url, index_name=args.index_namespace)
    else:
        raise ValueError(f"Unrecognized vector store type {args.vector_store_provider}")

```

`/Users/sauravverma/programs/pyano/sage/sage/constants.py`:

```py
# This is the key in the metadata that points to the actual text content of a document or chunk.
# It can mostly be an arbitrary string, but certain classes in LangChain do expect it to be "text" specifically.
TEXT_FIELD = "text"

```

`/Users/sauravverma/programs/pyano/sage/sage/retriever.py`:

```py
import logging
import os
from typing import Dict, List, Optional

import anthropic
import Levenshtein
from anytree import Node, RenderTree
from langchain.callbacks.manager import CallbackManagerForRetrieverRun
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.multi_query import MultiQueryRetriever
from langchain.schema import BaseRetriever, Document
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_openai import OpenAIEmbeddings
from langchain_voyageai import VoyageAIEmbeddings
from pydantic import Field
from sage.code_symbols import get_code_symbols
from sage.data_manager import DataManager, GitHubRepoManager
from sage.llm import build_llm_via_langchain
from sage.reranker import build_reranker
from sage.vector_store import build_vector_store_from_args

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger()
logger.setLevel(logging.INFO)

CLAUDE_MODEL = "claude-3-5-sonnet-20241022"
CLAUDE_MODEL_CONTEXT_SIZE = 200_000


class LLMRetriever(BaseRetriever):
    """Custom Langchain retriever based on an LLM.

    Builds a representation of the folder structure of the repo, feeds it to an LLM, and asks the LLM for the most
    relevant files for a particular user query, expecting it to make decisions based solely on file names.

    Only works with Claude/Anthropic, because it's very slow (e.g. 15s for a mid-sized codebase) and we need prompt
    caching to make it usable.
    """

    repo_manager: GitHubRepoManager = Field(...)
    top_k: int = Field(...)

    cached_repo_metadata: List[Dict] = Field(...)
    cached_repo_files: List[str] = Field(...)
    cached_repo_hierarchy: str = Field(...)

    def __init__(self, repo_manager: GitHubRepoManager, top_k: int):
        super().__init__()
        self.repo_manager = repo_manager
        self.top_k = top_k

        # We cached these fields manually because:
        # 1. Pydantic doesn't work with functools's @cached_property.
        # 2. We can't use Pydantic's @computed_field because these fields depend on each other.
        # 3. We can't use functools's @lru_cache because LLMRetriever needs to be hashable.
        self.cached_repo_metadata = None
        self.cached_repo_files = None
        self.cached_repo_hierarchy = None

        if not os.environ.get("ANTHROPIC_API_KEY"):
            raise ValueError("Please set the ANTHROPIC_API_KEY environment variable for the LLMRetriever.")

    @property
    def repo_metadata(self):
        if not self.cached_repo_metadata:
            self.cached_repo_metadata = [metadata for metadata in self.repo_manager.walk(get_content=False)]

            # Extracting code symbols takes quite a while, since we need to read each file from disk.
            # As a compromise, we do it for small codebases only.
            small_codebase = len(self.repo_files) <= 200
            if small_codebase:
                for metadata in self.cached_repo_metadata:
                    file_path = metadata["file_path"]
                    content = self.repo_manager.read_file(file_path)
                    metadata["code_symbols"] = get_code_symbols(file_path, content)

        return self.cached_repo_metadata

    @property
    def repo_files(self):
        if not self.cached_repo_files:
            self.cached_repo_files = set(metadata["file_path"] for metadata in self.repo_metadata)
        return self.cached_repo_files

    @property
    def repo_hierarchy(self):
        """Produces a string that describes the structure of the repository. Depending on how big the codebase is, it
        might include class and method names."""
        if self.cached_repo_hierarchy is None:
            render = LLMRetriever._render_file_hierarchy(self.repo_metadata, include_classes=True, include_methods=True)
            max_tokens = CLAUDE_MODEL_CONTEXT_SIZE - 50_000  # 50,000 tokens for other parts of the prompt.
            client = anthropic.Anthropic()

            def count_tokens(x):
                count = client.beta.messages.count_tokens(model=CLAUDE_MODEL, messages=[{"role": "user", "content": x}])
                return count.input_tokens

            if count_tokens(render) > max_tokens:
                logging.info("File hierarchy is too large; excluding methods.")
                render = LLMRetriever._render_file_hierarchy(
                    self.repo_metadata, include_classes=True, include_methods=False
                )
                if count_tokens(render) > max_tokens:
                    logging.info("File hierarchy is still too large; excluding classes.")
                    render = LLMRetriever._render_file_hierarchy(
                        self.repo_metadata, include_classes=False, include_methods=False
                    )
                    if count_tokens(render) > max_tokens:
                        logging.info("File hierarchy is still too large; truncating.")
                        tokenizer = anthropic.Tokenizer()
                        tokens = tokenizer.tokenize(render)[:max_tokens]
                        render = tokenizer.detokenize(tokens)
            self.cached_repo_hierarchy = render
        return self.cached_repo_hierarchy

    def _get_relevant_documents(self, query: str, *, run_manager: CallbackManagerForRetrieverRun) -> List[Document]:
        """Retrieve relevant documents for a given query."""
        filenames = self._ask_llm_to_retrieve(user_query=query, top_k=self.top_k)
        documents = []
        for filename in filenames:
            document = Document(
                page_content=self.repo_manager.read_file(filename),
                metadata={"file_path": filename, "url": self.repo_manager.url_for_file(filename)},
            )
            documents.append(document)
        return documents

    def _ask_llm_to_retrieve(self, user_query: str, top_k: int) -> List[str]:
        """Feeds the file hierarchy and user query to the LLM and asks which files might be relevant."""
        repo_hierarchy = str(self.repo_hierarchy)
        sys_prompt = f"""
You are a retriever system. You will be given a user query and a list of files in a GitHub repository, together with the class names in each file.

For instance:
folder1
    folder2
        folder3
            file123.py
                ClassName1
                ClassName2
                ClassName3
means that there is a file with path folder1/folder2/folder3/file123.py, which contains classes ClassName1, ClassName2, and ClassName3.

Your task is to determine the top {top_k} files that are most relevant to the user query.
DO NOT RESPOND TO THE USER QUERY DIRECTLY. Instead, respond with full paths to relevant files that could contain the answer to the query. Say absolutely nothing else other than the file paths.

Here is the file hierarchy of the GitHub repository, together with the class names in each file:

{repo_hierarchy}
"""

        # We are deliberately repeating the "DO NOT RESPOND TO THE USER QUERY DIRECTLY" instruction here.
        augmented_user_query = f"""
User query: {user_query}

DO NOT RESPOND TO THE USER QUERY DIRECTLY. Instead, respond with full paths to relevant files that could contain the answer to the query. Say absolutely nothing else other than the file paths.
"""
        response = LLMRetriever._call_via_anthropic_with_prompt_caching(sys_prompt, augmented_user_query)

        files_from_llm = response.content[0].text.strip().split("\n")
        validated_files = []

        for filename in files_from_llm:
            if filename not in self.repo_files:
                if "/" not in filename:
                    # This is most likely some natural language excuse from the LLM; skip it.
                    continue
                # Try a few heuristics to fix the filename.
                filename = LLMRetriever._fix_filename(filename, self.repo_manager.repo_id)
                if filename not in self.repo_files:
                    # The heuristics failed; try to find the closest filename in the repo.
                    filename = LLMRetriever._find_closest_filename(filename, self.repo_files)
            if filename in self.repo_files:
                validated_files.append(filename)
        return validated_files

    @staticmethod
    def _call_via_anthropic_with_prompt_caching(system_prompt: str, user_prompt: str) -> str:
        """Calls the Anthropic API with prompt caching for the system prompt.

        See https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching.

        We're circumventing LangChain for now, because the feature is < 1 week old at the time of writing and has no
        documentation: https://github.com/langchain-ai/langchain/pull/27087
        """
        system_message = {"type": "text", "text": system_prompt, "cache_control": {"type": "ephemeral"}}
        user_message = {"role": "user", "content": user_prompt}

        response = anthropic.Anthropic().beta.prompt_caching.messages.create(
            model=CLAUDE_MODEL,
            max_tokens=1024,  # The maximum number of *output* tokens to generate.
            system=[system_message],
            messages=[user_message],
        )
        # Caching information will be under `cache_creation_input_tokens` and `cache_read_input_tokens`.
        # Note that, for prompts shorter than 1024 tokens, Anthropic will not do any caching.
        logging.info("Anthropic prompt caching info: %s", response.usage)
        return response

    @staticmethod
    def _render_file_hierarchy(
        repo_metadata: List[Dict], include_classes: bool = True, include_methods: bool = True
    ) -> str:
        """Given a list of files, produces a visualization of the file hierarchy. This hierarchy optionally includes
        class and method names, if available.

        For large codebases, including both classes and methods might exceed the token limit of the LLM. In that case,
        try setting `include_methods=False` first. If that's still too long, try also setting `include_classes=False`.

        As a point of reference, the Transformers library requires setting `include_methods=False` to fit within
        Claude's 200k context.

        Example:
        folder1
            folder11
                file111.md
                file112.py
                    ClassName1
                        method_name1
                        method_name2
                        method_name3
            folder12
                file121.py
                    ClassName2
                    ClassName3
        folder2
            file21.py
        """
        # The "nodepath" is the path from root to the node (e.g. huggingface/transformers/examples)
        nodepath_to_node = {}

        for metadata in repo_metadata:
            path = metadata["file_path"]
            paths = [path]

            if include_classes or include_methods:
                # Add the code symbols to the path. For instance, "folder/myfile.py/ClassName/method_name".
                for class_name, method_name in metadata.get("code_symbols", []):
                    if include_classes and class_name:
                        paths.append(path + "/" + class_name)
                    # We exclude private methods to save tokens.
                    if include_methods and method_name and not method_name.startswith("_"):
                        paths.append(
                            path + "/" + class_name + "/" + method_name if class_name else path + "/" + method_name
                        )

            for path in paths:
                items = path.split("/")
                nodepath = ""
                parent_node = None
                for item in items:
                    nodepath = f"{nodepath}/{item}"
                    if nodepath in nodepath_to_node:
                        node = nodepath_to_node[nodepath]
                    else:
                        node = Node(item, parent=parent_node)
                        nodepath_to_node[nodepath] = node
                    parent_node = node

        root_path = "/" + repo_metadata[0]["file_path"].split("/")[0]
        full_render = ""
        root_node = nodepath_to_node[root_path]
        for pre, fill, node in RenderTree(root_node):
            render = "%s%s\n" % (pre, node.name)
            # Replace special lines with empty strings to save on tokens.
            render = render.replace("└", " ").replace("├", " ").replace("│", " ").replace("─", " ")
            full_render += render
        return full_render

    @staticmethod
    def _fix_filename(filename: str, repo_id: str) -> str:
        """Attempts to "fix" a filename output by the LLM.

        Common issues with LLM-generated filenames:
        - The LLM prepends an extraneous "/".
        - The LLM omits the name of the org (e.g. "transformers/README.md" instead of "huggingface/transformers/README.md").
        - The LLM omits the name of the repo (e.g. "huggingface/README.md" instead of "huggingface/transformers/README.md").
        - The LLM omits the org/repo prefix (e.g. "README.md" instead of "huggingface/transformers/README.md").
        """
        if filename.startswith("/"):
            filename = filename[1:]
        org_name, repo_name = repo_id.split("/")
        items = filename.split("/")
        if filename.startswith(org_name) and not filename.startswith(repo_id):
            new_items = [org_name, repo_name] + items[1:]
            return "/".join(new_items)
        if not filename.startswith(org_name) and filename.startswith(repo_name):
            return f"{org_name}/{filename}"
        if not filename.startswith(org_name) and not filename.startswith(repo_name):
            return f"{org_name}/{repo_name}/{filename}"
        return filename

    @staticmethod
    def _find_closest_filename(filename: str, repo_filenames: List[str], max_edit_distance: int = 10) -> Optional[str]:
        """Returns the path in the repo with smallest edit distance from `filename`. Helpful when the `filename` was
        generated by an LLM and parts of it might have been hallucinated. Returns None if the closest path is more than
        `max_edit_distance` away. In case of a tie, returns an arbitrary closest path.
        """
        distances = [(path, Levenshtein.distance(filename, path)) for path in repo_filenames]
        distances.sort(key=lambda x: x[1])
        if distances[0][1] <= max_edit_distance:
            closest_path = distances[0][0]
            return closest_path
        return None



# class DeepseekRetriever(BaseRetriever):
#     """Custom Langchain retriever that uses Deepseek VSCode model hosted on Together.ai"""

#     repo_manager: GitHubRepoManager = Field(...)
#     top_k: int = Field(...)
#     model_name: str = Field(default="deepseek-ai/deepseek-coder-6.7b-instruct")
#     temperature: float = Field(default=0.1)
#     max_tokens: int = Field(default=1024)

#     def __init__(self, repo_manager: GitHubRepoManager, top_k: int):
#         """Initialize the retriever with repository manager and configuration.
        
#         Args:
#             repo_manager: Manager for the GitHub repository
#             top_k: Number of files to retrieve
#         """
#         super().__init__()
#         self.repo_manager = repo_manager
#         self.top_k = top_k

#         # Ensure Together API key is set
#         if not together.api_key:
#             raise ValueError("Please set the TOGETHER_API_KEY environment variable")

#         # Cache repo metadata and files
#         self.cached_repo_metadata = None
#         self.cached_repo_files = None
#         self.cached_repo_hierarchy = None

#     def _get_relevant_documents(
#         self, 
#         query: str, 
#         *, 
#         run_manager: CallbackManagerForRetrieverRun
#     ) -> List[Document]:
#         """Retrieve relevant documents for a given query.
        
#         Args:
#             query: The user query
#             run_manager: Callback manager
            
#         Returns:
#             List of relevant documents
#         """
#         filenames = self._ask_model_to_retrieve(user_query=query, top_k=self.top_k)
#         documents = []
        
#         for filename in filenames:
#             if filename in self.repo_files:
#                 document = Document(
#                     page_content=self.repo_manager.read_file(filename),
#                     metadata={
#                         "file_path": filename,
#                         "url": self.repo_manager.url_for_file(filename)
#                     }
#                 )
#                 documents.append(document)
                
#         return documents

#     def _ask_model_to_retrieve(self, user_query: str, top_k: int) -> List[str]:
#         """Query the Deepseek model to identify relevant files.
        
#         Args:
#             user_query: The user's question
#             top_k: Number of files to retrieve
            
#         Returns:
#             List of relevant file paths
#         """
#         prompt = f"""You are a code search assistant. Given a file hierarchy and a query, your task is to return the {top_k} most relevant file paths that would help answer the query.
# Only return the file paths, one per line. Do not include any other text or explanations.

# File hierarchy:
# {self.repo_hierarchy}

# Query: {user_query}

# Relevant files:"""

#         # Call Together API
#         response = together.Complete.create(
#             prompt=prompt,
#             model=self.model_name,
#             max_tokens=self.max_tokens,
#             temperature=self.temperature
#         )

#         # Extract file paths from response
#         file_paths = []
#         if response.output and response.output.text:
#             # Split response into lines and clean up
#             paths = response.output.text.strip().split('\n')
#             paths = [p.strip() for p in paths if p.strip()]
            
#             # Validate and fix paths
#             for path in paths[:top_k]:
#                 if path not in self.repo_files:
#                     fixed_path = self._fix_filename(path, self.repo_manager.repo_id)
#                     if fixed_path and fixed_path in self.repo_files:
#                         file_paths.append(fixed_path)
#                 else:
#                     file_paths.append(path)

#         return file_paths

#     @property
#     def repo_metadata(self):
#         """Get repository metadata, computing it if not cached."""
#         if not self.cached_repo_metadata:
#             self.cached_repo_metadata = [
#                 metadata for metadata in self.repo_manager.walk(get_content=False)
#             ]
#         return self.cached_repo_metadata

#     @property
#     def repo_files(self):
#         """Get repository files, computing them if not cached."""
#         if not self.cached_repo_files:
#             self.cached_repo_files = set(
#                 metadata["file_path"] for metadata in self.repo_metadata
#             )
#         return self.cached_repo_files

#     @property 
#     def repo_hierarchy(self):
#         """Get repository hierarchy string, computing if not cached."""
#         if not self.cached_repo_hierarchy:
#             # Render full hierarchy
#             render = self._render_file_hierarchy(
#                 self.repo_metadata,
#                 include_classes=True,
#                 include_methods=True
#             )
#             self.cached_repo_hierarchy = render
#         return self.cached_repo_hierarchy

#     def _fix_filename(self, filename: str, repo_id: str) -> str:
#         """Fix common issues with model-generated filenames.
        
#         Args:
#             filename: The filename to fix
#             repo_id: Repository identifier
            
#         Returns:
#             Fixed filename or None if unfixable
#         """
#         if filename.startswith("/"):
#             filename = filename[1:]
            
#         org_name, repo_name = repo_id.split("/")
#         items = filename.split("/")
        
#         if filename.startswith(org_name) and not filename.startswith(repo_id):
#             new_items = [org_name, repo_name] + items[1:]
#             return "/".join(new_items)
            
#         if not filename.startswith(org_name) and filename.startswith(repo_name):
#             return f"{org_name}/{filename}"
            
#         if not filename.startswith(org_name) and not filename.startswith(repo_name):
#             return f"{org_name}/{repo_name}/{filename}"
            
#         return filename






class RerankerWithErrorHandling(BaseRetriever):
    """Wraps a `ContextualCompressionRetriever` to catch errors during inference.

    In practice, we see occasional `requests.exceptions.ReadTimeout` from the NVIDIA reranker, which crash the entire
    pipeline. This wrapper catches such exceptions by simply returning the documents in the original order.
    """

    def __init__(self, reranker: ContextualCompressionRetriever):
        self.reranker = reranker

    def _get_relevant_documents(self, query: str, *, run_manager=None) -> List[Document]:
        try:
            return self.reranker._get_relevant_documents(query, run_manager=run_manager)
        except Exception as e:
            logging.error(f"Error in reranker; preserving original document order from retriever. {e}")
            return self.reranker.base_retriever._get_relevant_documents(query, run_manager=run_manager)


def build_retriever_from_args(args, data_manager: Optional[DataManager] = None):
    """Builds a retriever (with optional reranking) from command-line arguments."""
    if args.llm_retriever:
        retriever = DeepseekRetriever(GitHubRepoManager.from_args(args), top_k=args.retriever_top_k)
    else:
        if args.embedding_provider == "openai":
            embeddings = OpenAIEmbeddings(model=args.embedding_model)
        elif args.embedding_provider == "voyage":
            embeddings = VoyageAIEmbeddings(model=args.embedding_model)
        elif args.embedding_provider == "gemini":
            embeddings = GoogleGenerativeAIEmbeddings(model=args.embedding_model)
        else:
            embeddings = None

        retriever = build_vector_store_from_args(args, data_manager).as_retriever(
            top_k=args.retriever_top_k, embeddings=embeddings, namespace=args.index_namespace
        )

    if args.multi_query_retriever:
        retriever = MultiQueryRetriever.from_llm(
            retriever=retriever, llm=build_llm_via_langchain(args.llm_provider, args.llm_model)
        )

    reranker = build_reranker(args.reranker_provider, args.reranker_model, args.reranker_top_k)
    if reranker:
        retriever = ContextualCompressionRetriever(base_compressor=reranker, base_retriever=retriever)
    return retriever

```

`/Users/sauravverma/programs/pyano/sage/sage/llm.py`:

```py
import os
import yaml
from langchain_anthropic import ChatAnthropic
from langchain_ollama import ChatOllama
from langchain_openai import ChatOpenAI
from langchain_together import ChatTogether

def build_llm_via_langchain(provider: str, model: str):
    """Builds a language model via LangChain."""
    if provider == "openai":
        if "OPENAI_API_KEY" not in os.environ:
            raise ValueError("Please set the OPENAI_API_KEY environment variable.")
        return ChatOpenAI(model=model or "gpt-4")
    elif provider == "anthropic":
        if "ANTHROPIC_API_KEY" not in os.environ:
            raise ValueError("Please set the ANTHROPIC_API_KEY environment variable.")
        return ChatAnthropic(model=model or "claude-3-opus-20240229")
    elif provider == "ollama":
        return ChatOllama(model=model or "llama3.1")
    elif provider == "together":
        if "TOGETHER_API_KEY" not in os.environ:
            raise ValueError("Please set the TOGETHER_API_KEY environment variable.")
        return ChatTogether(model=model or "deepseek-ai/DeepSeek-V3")
    else:
        raise ValueError(f"Unrecognized LLM provider {provider}. Contributons are welcome!")

```

`/Users/sauravverma/programs/pyano/sage/sage/sample-exclude.txt`:

```txt
# This list tends to be overly-aggressive. We're assuming by default devs are most interested in code files, not configs.
dir:_build
dir:alembic
dir:build
dir:deprecated
dir:docker
dir:downgrades
dir:fixtures
dir:integration-tests
dir:legacy
dir:library-tests
dir:logo
dir:logs
dir:migrations
dir:node_modules
dir:old-change-notes
dir:test
dir:testdata
dir:tests
dir:third_party
dir:upgrades
dir:vendor
ext:.Packages
ext:.avi
ext:.bazel
ext:.bin
ext:.binpb
ext:.bmp
ext:.crt
ext:.css
ext:.csv
ext:.dat
ext:.db
ext:.duckdb
ext:.eot
ext:.exe
ext:.gguf
ext:.gif
ext:.glb
ext:.gz
ext:.icns
ext:.ico
ext:.inp
ext:.isl
ext:.jar
ext:.jpeg
ext:.jpg
ext:.json
ext:.key
ext:.lock
ext:.mo
ext:.model
ext:.mov
ext:.mp3
ext:.mp4
ext:.otf
ext:.out
ext:.pb
ext:.pdf
ext:.pem
ext:.pickle
ext:.png
ext:.pt
ext:.ptl
ext:.s
ext:.so
ext:.sql
ext:.sqlite
ext:.stl
ext:.sum
ext:.svg
ext:.tar
ext:.tgz
ext:.th
ext:.toml
ext:.ts-fixture
ext:.tsv
ext:.ttf
ext:.wav
ext:.webp
ext:.wmv
ext:.woff
ext:.woff2
ext:.xml
ext:.yaml
ext:.yml
ext:.zip
file:CODE_OF_CONDUCT.md
file:CONTRIBUTING.md
file:Dockerfile
file:__init__.py
file:code-of-conduct.md
file:conftest.py
file:package-lock.json

```

`/Users/sauravverma/programs/pyano/sage/sage/chat.py`:

```py
"""A gradio app that enables users to chat with their codebase.

You must run `sage-index $GITHUB_REPO` first in order to index the codebase into a vector store.
"""

import logging

import configargparse
import gradio as gr
from dotenv import load_dotenv
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.schema import AIMessage, HumanMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

import sage.config as sage_config
from sage.llm import build_llm_via_langchain
from sage.retriever import build_retriever_from_args

load_dotenv()


def build_rag_chain(args):
    """Builds a RAG chain via LangChain."""
    llm = build_llm_via_langchain(args.llm_provider, args.llm_model)
    retriever = build_retriever_from_args(args)

    # Prompt to contextualize the latest query based on the chat history.
    contextualize_q_system_prompt = (
        "Given a chat history and the latest user question which might reference context in the chat history, "
        "formulate a standalone question which can be understood without the chat history. Do NOT answer the question, "
        "just reformulate it if needed and otherwise return it as is."
    )
    contextualize_q_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", contextualize_q_system_prompt),
            MessagesPlaceholder("chat_history"),
            ("human", "{input}"),
        ]
    )
    contextualize_q_llm = llm.with_config(tags=["contextualize_q_llm"])
    history_aware_retriever = create_history_aware_retriever(contextualize_q_llm, retriever, contextualize_q_prompt)

    qa_system_prompt = (
        f"You are my coding buddy, helping me quickly understand a GitHub repository called {args.repo_id}."
        "Assume I am an advanced developer and answer my questions in the most succinct way possible."
        "\n\n"
        "Here are some snippets from the codebase."
        "\n\n"
        "{context}"
    )
    qa_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", qa_system_prompt),
            MessagesPlaceholder("chat_history"),
            ("human", "{input}"),
        ]
    )

    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)
    return rag_chain


def main():
    parser = configargparse.ArgParser(
        description="Batch-embeds a GitHub repository and its issues.", ignore_unknown_config_file_keys=True
    )
    parser.add(
        "--share",
        default=False,
        help="Whether to make the gradio app publicly accessible.",
    )

    validator = sage_config.add_all_args(parser)
    args = parser.parse_args()
    validator(args)

    rag_chain = build_rag_chain(args)

    def source_md(file_path: str, url: str) -> str:
        """Formats a context source in Markdown."""
        return f"[{file_path}]({url})"

    async def _predict(message, history):
        """Performs one RAG operation."""
        history_langchain_format = []
        for human, ai in history:
            history_langchain_format.append(HumanMessage(content=human))
            history_langchain_format.append(AIMessage(content=ai))
        history_langchain_format.append(HumanMessage(content=message))

        query_rewrite = ""
        response = ""
        async for event in rag_chain.astream_events(
            {
                "input": message,
                "chat_history": history_langchain_format,
            },
            version="v1",
        ):
            if event["name"] == "retrieve_documents" and "output" in event["data"]:
                sources = [(doc.metadata["file_path"], doc.metadata["url"]) for doc in event["data"]["output"]]
                # Deduplicate while preserving the order.
                sources = list(dict.fromkeys(sources))
                response += "## Sources:\n" + "\n".join([source_md(s[0], s[1]) for s in sources]) + "\n\n"

            elif event["event"] == "on_chat_model_stream":
                chunk = event["data"]["chunk"].content

                if "contextualize_q_llm" in event["tags"]:
                    query_rewrite += chunk
                else:
                    # This is the actual response to the user query.
                    if not response:
                        logging.info(f"Query rewrite: {query_rewrite}")
                    response += chunk
                    yield response

    gr.Markdown("# Pyano ai16z eliza agent")
    gr.ChatInterface(
        _predict,
        title=args.repo_id,
        examples=["What does this repo do?", "Give me some sample code."],
    ).launch(share=args.share)


if __name__ == "__main__":
    main()

```

`/Users/sauravverma/programs/pyano/sage/sage/configs/remote.yaml`:

```yaml
llm-retriever: true
llm-provider: anthropic
# Here we optimize for ease of setup, so we skip the reranker which would require an extra API key.
reranker-provider: none
# Since we skipped the reranker, we can't afford to feed the retriever with too many candidates.
retriever-top-k: 5

# The settings below (embeddings and vector store) are only relevant when setting --no-llm-retriever

# Embeddings
embedding-provider: openai
embedding-model: text-embedding-3-small
tokens-per-chunk: 800
chunks-per-batch: 2000
# Vector store
vector-store-provider: pinecone
pinecone-index-name: sage
hybrid-retrieval: true

```

`/Users/sauravverma/programs/pyano/sage/sage/configs/local.yaml`:

```yaml

# Embeddings
embedding-provider: marqo
embedding-model: hf/e5-base-v2
tokens-per-chunk: 800
chunks-per-batch: 64

# Vector store
vector-store-provider: marqo

# LLM
llm-provider: together
llm-model: deepseek-ai/DeepSeek-V3

# Reranking
reranking-provider: huggingface
reranking-model: cross-encoder/ms-marco-MiniLM-L-6-v2
```

`/Users/sauravverma/programs/pyano/sage/sage/embedder.py`:

```py
"""Batch embedder abstraction and implementations."""

import json
import logging
import os
import time
from abc import ABC, abstractmethod
from collections import Counter
from typing import Dict, Generator, List, Optional, Tuple

import google.generativeai as genai
import marqo
import requests
from openai import OpenAI
from tenacity import retry, stop_after_attempt, wait_random_exponential
from tqdm import tqdm

from sage.chunker import Chunk, Chunker
from sage.constants import TEXT_FIELD
from sage.data_manager import DataManager

Vector = Tuple[Dict, List[float]]  # (metadata, embedding)


class BatchEmbedder(ABC):
    """Abstract class for batch embedding of a dataset."""

    @abstractmethod
    def embed_dataset(self, chunks_per_batch: int, max_embedding_jobs: int = None):
        """Issues batch embedding jobs for the entire dataset."""

    @abstractmethod
    def embeddings_are_ready(self) -> bool:
        """Checks whether the batch embedding jobs are done."""

    @abstractmethod
    def download_embeddings(self) -> Generator[Vector, None, None]:
        """Yields (chunk_metadata, embedding) pairs for each chunk in the dataset."""


class OpenAIBatchEmbedder(BatchEmbedder):
    """Batch embedder that calls OpenAI. See https://platform.openai.com/docs/guides/batch/overview."""

    def __init__(
        self, data_manager: DataManager, chunker: Chunker, local_dir: str, embedding_model: str, embedding_size: int
    ):
        self.data_manager = data_manager
        self.chunker = chunker
        self.local_dir = local_dir
        self.embedding_model = embedding_model
        self.embedding_size = embedding_size
        self.client = OpenAI()

    def embed_dataset(self, chunks_per_batch: int, max_embedding_jobs: int = None) -> str:
        """Issues batch embedding jobs for the entire dataset. Returns the filename containing the job IDs."""
        batch = []
        batch_ids = {}  # job_id -> metadata
        chunk_count = 0
        dataset_name = self.data_manager.dataset_id.replace("/", "_")

        num_files = len([x for x in self.data_manager.walk(get_content=False)])
        pbar = tqdm(total=num_files, desc="Processing files", unit="file")

        for content, metadata in self.data_manager.walk():
            chunks = self.chunker.chunk(content, metadata)
            chunk_count += len(chunks)
            batch.extend(chunks)
            pbar.update(1)

            if len(batch) > chunks_per_batch:
                for i in range(0, len(batch), chunks_per_batch):
                    sub_batch = batch[i : i + chunks_per_batch]
                    openai_batch_id = self._issue_job_for_chunks(sub_batch, batch_id=f"{dataset_name}/{len(batch_ids)}")
                    batch_ids[openai_batch_id] = [chunk.metadata for chunk in sub_batch]
                    if max_embedding_jobs and len(batch_ids) >= max_embedding_jobs:
                        logging.info("Reached the maximum number of embedding jobs. Stopping.")
                        return
                batch = []

        # Finally, commit the last batch.
        if batch:
            openai_batch_id = self._issue_job_for_chunks(batch, batch_id=f"{dataset_name}/{len(batch_ids)}")
            batch_ids[openai_batch_id] = [chunk.metadata for chunk in batch]

        logging.info("Issued %d jobs for %d chunks.", len(batch_ids), chunk_count)

        timestamp = int(time.time())
        metadata_file = os.path.join(self.local_dir, f"{dataset_name}_openai_batch_ids_{timestamp}.json")
        with open(metadata_file, "w") as f:
            json.dump(batch_ids, f)
        logging.info("Job metadata saved at %s", metadata_file)
        pbar.close()
        return metadata_file

    def embeddings_are_ready(self, metadata_file: str) -> bool:
        """Checks whether the embeddings jobs are done (either completed or failed).

        Args:
            metadata_file: Path to the file containing the job metadata (output of self.embed_dataset).
        """
        with open(metadata_file, "r") as f:
            batch_ids = json.load(f)

        job_ids = batch_ids.keys()
        statuses = [self.client.batches.retrieve(job_id.strip()) for job_id in job_ids]
        are_ready = all(status.status in ["completed", "failed"] for status in statuses)
        status_counts = Counter(status.status for status in statuses)
        logging.info("Job statuses: %s", status_counts)
        return are_ready

    def download_embeddings(
        self, metadata_file: str, store_file_chunk_content: bool = True
    ) -> Generator[Vector, None, None]:
        """Yields a (chunk_metadata, embedding) pair for each chunk in the dataset.

        Args:
            metadata_file: Path to the file containing the job metadata (output of self.embed_dataset).
            store_file_chunk_content: Whether to store the text content in the metadata for file chunks. Set this to
                False if you want to save space in the vector store. After retrieval, the content of a file chunk can be
                reconstructed based on the file_path, start_byte and end_byte fields in the metadata. This will not
                affect other types of chunks (e.g. GitHub issues) for which the content is harder to reconstruct.
        """
        with open(metadata_file, "r") as f:
            batch_ids = json.load(f)

        job_ids = batch_ids.keys()
        statuses = [self.client.batches.retrieve(job_id.strip()) for job_id in job_ids]

        for idx, status in enumerate(statuses):
            if status.status == "failed":
                logging.error("Job failed: %s", status)
                continue

            if not status.output_file_id:
                error = self.client.files.content(status.error_file_id)
                logging.error("Job %s failed with error: %s", status.id, error.text)
                continue

            batch_metadata = batch_ids[status.id]
            file_response = self.client.files.content(status.output_file_id)
            data = json.loads(file_response.text)["response"]["body"]["data"]
            logging.info("Job %s generated %d embeddings.", status.id, len(data))

            for datum in data:
                idx = int(datum["index"])
                metadata = batch_metadata[idx]
                if (
                    not store_file_chunk_content
                    and "file_path" in metadata
                    and "start_byte" in metadata
                    and "end_byte" in metadata
                ):
                    metadata.pop(TEXT_FIELD, None)
                embedding = datum["embedding"]
                yield (metadata, embedding)

    def _issue_job_for_chunks(self, chunks: List[Chunk], batch_id: str) -> str:
        """Issues a batch embedding job for the given chunks. Returns the job ID."""
        logging.info("*" * 100)
        logging.info("Issuing job for batch %s with %d chunks.", batch_id, len(chunks))

        # Create a .jsonl file with the batch.
        request = OpenAIBatchEmbedder._chunks_to_request(chunks, batch_id, self.embedding_model, self.embedding_size)
        input_file = os.path.join(self.local_dir, f"batch_{batch_id}.jsonl")
        OpenAIBatchEmbedder._export_to_jsonl([request], input_file)

        # Uplaod the file and issue the embedding job.
        batch_input_file = self.client.files.create(file=open(input_file, "rb"), purpose="batch")
        batch_status = self._create_batch_job(batch_input_file.id)
        logging.info("Created job with ID %s", batch_status.id)
        return batch_status.id

    def _create_batch_job(self, input_file_id: str):
        """Creates a batch embedding job for OpenAI."""
        try:
            return self.client.batches.create(
                input_file_id=input_file_id,
                endpoint="/v1/embeddings",
                completion_window="24h",  # This is the only allowed value for now.
                timeout=3 * 60,  # 3 minutes
                metadata={},
            )
        except Exception as e:
            logging.error(f"Failed to create batch job with input_file_id={input_file_id}. Error: {e}")
            return None

    @staticmethod
    def _export_to_jsonl(list_of_dicts: List[Dict], output_file: str):
        """Exports a list of dictionaries to a .jsonl file."""
        directory = os.path.dirname(output_file)
        if not os.path.exists(directory):
            os.makedirs(directory)
        with open(output_file, "w") as f:
            for item in list_of_dicts:
                json.dump(item, f)
                f.write("\n")

    @staticmethod
    def _chunks_to_request(chunks: List[Chunk], batch_id: str, model: str, dimensions: Optional[int] = None) -> Dict:
        """Convert a list of chunks to a batch request."""
        body = {
            "model": model,
            "input": [chunk.content for chunk in chunks],
        }

        # These are the only two models that support a dynamic embedding size.
        if model in ["text-embedding-3-small", "text-embedding-3-large"] and dimensions is not None:
            body["dimensions"] = dimensions

        return {
            "custom_id": batch_id,
            "method": "POST",
            "url": "/v1/embeddings",
            "body": body,
        }


class VoyageBatchEmbedder(BatchEmbedder):
    """Batch embedder that calls Voyage. See https://docs.voyageai.com/reference/embeddings-api."""

    def __init__(self, data_manager: DataManager, chunker: Chunker, embedding_model: str):
        self.data_manager = data_manager
        self.chunker = chunker
        self.embedding_model = embedding_model
        self.embedding_data = []

    def embed_dataset(self, chunks_per_batch: int, max_embedding_jobs: int = None):
        """Issues batch embedding jobs for the entire dataset."""
        batch = []
        chunk_count = 0

        num_files = len([x for x in self.data_manager.walk(get_content=False)])
        pbar = tqdm(total=num_files, desc="Processing files", unit="file")

        for content, metadata in self.data_manager.walk():
            chunks = self.chunker.chunk(content, metadata)
            chunk_count += len(chunks)
            batch.extend(chunks)
            pbar.update(1)

            token_count = chunk_count * self.chunker.max_tokens
            if token_count % 900_000 == 0:
                logging.info("Pausing for 60 seconds to avoid rate limiting...")
                time.sleep(60)  # Voyage API rate limits to 1m tokens per minute; we'll pause every 900k tokens.

            if len(batch) > chunks_per_batch:
                for i in range(0, len(batch), chunks_per_batch):
                    sub_batch = batch[i : i + chunks_per_batch]
                    logging.info("Embedding %d chunks...", len(sub_batch))
                    result = self._make_batch_request(sub_batch)
                    for chunk, datum in zip(sub_batch, result["data"]):
                        self.embedding_data.append((chunk.metadata, datum["embedding"]))
                batch = []

        # Finally, commit the last batch.
        if batch:
            logging.info("Embedding %d chunks...", len(batch))
            result = self._make_batch_request(batch)
            for chunk, datum in zip(batch, result["data"]):
                self.embedding_data.append((chunk.metadata, datum["embedding"]))
        pbar.close()
        logging.info(f"Successfully embedded {chunk_count} chunks.")

    def embeddings_are_ready(self, *args, **kwargs) -> bool:
        """Checks whether the batch embedding jobs are done."""
        # The Voyage API is synchronous, so once embed_dataset() returns, the embeddings are ready.
        return True

    def download_embeddings(self, *args, **kwargs) -> Generator[Vector, None, None]:
        """Yields (chunk_metadata, embedding) pairs for each chunk in the dataset."""
        for chunk_metadata, embedding in self.embedding_data:
            yield (chunk_metadata, embedding)

    @retry(wait=wait_random_exponential(multiplier=1, max=60), stop=stop_after_attempt(6))
    def _make_batch_request(self, chunks: List[Chunk]) -> Dict:
        """Makes a batch request to the Voyage API with exponential backoff when we hit rate limits."""
        url = "https://api.voyageai.com/v1/embeddings"
        headers = {"Authorization": f"Bearer {os.environ['VOYAGE_API_KEY']}", "Content-Type": "application/json"}
        payload = {"input": [chunk.content for chunk in chunks], "model": self.embedding_model}

        response = requests.post(url, json=payload, headers=headers)
        if not response.status_code == 200:
            raise ValueError(f"Failed to make batch request. Response: {response.text}")

        return response.json()


class MarqoEmbedder(BatchEmbedder):
    """Embedder that uses the open-source Marqo vector search engine.

    Embeddings can be stored locally (in which case `url` the constructor should point to localhost) or in the cloud.
    """

    def __init__(self, data_manager: DataManager, chunker: Chunker, index_name: str, url: str, model="hf/e5-base-v2"):
        self.data_manager = data_manager
        self.chunker = chunker
        self.client = marqo.Client(url=url)
        self.index = self.client.index(index_name)

        all_index_names = [result["indexName"] for result in self.client.get_indexes()["results"]]
        if not index_name in all_index_names:
            self.client.create_index(index_name, model=model)

    def embed_dataset(self, chunks_per_batch: int, max_embedding_jobs: int = None):
        """Issues batch embedding jobs for the entire dataset with progress tracking."""
        if chunks_per_batch > 64:
            raise ValueError("Marqo enforces a limit of 64 chunks per batch.")

        chunk_count = 0
        batch = []
        job_count = 0

        num_files = len([x for x in self.data_manager.walk(get_content=False)])
        pbar = tqdm(total=num_files, desc="Processing files", unit="file")

        for content, metadata in self.data_manager.walk():
            chunks = self.chunker.chunk(content, metadata)
            chunk_count += len(chunks)
            batch.extend(chunks)
            pbar.update(1)
            if len(batch) > chunks_per_batch:
                for i in range(0, len(batch), chunks_per_batch):
                    sub_batch = batch[i : i + chunks_per_batch]
                    logging.info("Indexing %d chunks...", len(sub_batch))
                    self.index.add_documents(
                        documents=[chunk.metadata for chunk in sub_batch],
                        tensor_fields=[TEXT_FIELD],
                    )
                    job_count += 1

                    if max_embedding_jobs and job_count >= max_embedding_jobs:
                        logging.info("Reached the maximum number of embedding jobs. Stopping.")
                        pbar.close()
                        return
                batch = []
        if batch:
            self.index.add_documents(documents=[chunk.metadata for chunk in batch], tensor_fields=[TEXT_FIELD])

        pbar.close()
        logging.info(f"Successfully embedded {chunk_count} chunks.")

    def embeddings_are_ready(self) -> bool:
        """Checks whether the batch embedding jobs are done."""
        # Marqo indexes documents synchronously, so once embed_dataset() returns, the embeddings are ready.
        return True

    def download_embeddings(self) -> Generator[Vector, None, None]:
        """Yields (chunk_metadata, embedding) pairs for each chunk in the dataset."""
        # Marqo stores embeddings as they are created, so they're already in the vector store. No need to download them
        # as we would with e.g. OpenAI, Cohere, or some other cloud-based embedding service.
        return []


class GeminiBatchEmbedder(BatchEmbedder):
    """Batch embedder that calls Gemini."""

    def __init__(self, data_manager: DataManager, chunker: Chunker, embedding_model: str):
        self.data_manager = data_manager
        self.chunker = chunker
        self.embedding_data = []
        self.embedding_model = embedding_model
        genai.configure(api_key=os.environ["GOOGLE_API_KEY"])

    def _make_batch_request(self, chunks: List[Chunk]) -> Dict:
        return genai.embed_content(
            model=self.embedding_model, content=[chunk.content for chunk in chunks], task_type="retrieval_document"
        )

    def embed_dataset(self, chunks_per_batch: int, max_embedding_jobs: int = None):
        """Issues batch embedding jobs for the entire dataset."""
        batch = []
        chunk_count = 0

        request_count = 0
        last_request_time = time.time()

        num_files = len([x for x in self.data_manager.walk(get_content=False)])
        pbar = tqdm(total=num_files, desc="Processing files", unit="file")

        for content, metadata in self.data_manager.walk():
            chunks = self.chunker.chunk(content, metadata)
            chunk_count += len(chunks)
            batch.extend(chunks)
            pbar.update(1)

            if len(batch) > chunks_per_batch:
                for i in range(0, len(batch), chunks_per_batch):
                    sub_batch = batch[i : i + chunks_per_batch]
                    logging.info("Embedding %d chunks...", len(sub_batch))
                    result = self._make_batch_request(sub_batch)
                    for chunk, embedding in zip(sub_batch, result["embedding"]):
                        self.embedding_data.append((chunk.metadata, embedding))
                    request_count += 1

                    # Check if we've made more than 1500 requests in the last minute
                    # Rate limits here: https://ai.google.dev/gemini-api/docs/models/gemini
                    current_time = time.time()
                    elapsed_time = current_time - last_request_time
                    if elapsed_time < 60 and request_count >= 1400:
                        logging.info("Reached rate limit, pausing for 60 seconds...")
                        time.sleep(60)
                        last_request_time = current_time
                        request_count = 0
                    # Reset the last request time and request count if more than 60 sec have passed
                    elif elapsed_time > 60:
                        last_request_time = current_time
                        request_count = 0

                batch = []

        # Finally, commit the last batch.
        if batch:
            logging.info("Embedding %d chunks...", len(batch))
            result = self._make_batch_request(batch)
            for chunk, embedding in zip(batch, result["embedding"]):
                self.embedding_data.append((chunk.metadata, embedding))
        pbar.close()
        logging.info(f"Successfully embedded {chunk_count} chunks.")

    def embeddings_are_ready(self, *args, **kwargs) -> bool:
        """Checks whether the batch embedding jobs are done."""
        return True

    def download_embeddings(self, *args, **kwargs) -> Generator[Vector, None, None]:
        """Yields (chunk_metadata, embedding) pairs for each chunk in the dataset."""
        for chunk_metadata, embedding in self.embedding_data:
            yield chunk_metadata, embedding


def build_batch_embedder_from_flags(data_manager: DataManager, chunker: Chunker, args) -> BatchEmbedder:
    if args.embedding_provider == "openai":
        return OpenAIBatchEmbedder(data_manager, chunker, args.local_dir, args.embedding_model, args.embedding_size)
    elif args.embedding_provider == "voyage":
        return VoyageBatchEmbedder(data_manager, chunker, args.embedding_model)
    elif args.embedding_provider == "marqo":
        return MarqoEmbedder(
            data_manager, chunker, index_name=args.index_namespace, url=args.marqo_url, model=args.embedding_model
        )
    elif args.embedding_provider == "gemini":
        return GeminiBatchEmbedder(data_manager, chunker, embedding_model=args.embedding_model)
    else:
        raise ValueError(f"Unrecognized embedder type {args.embedding_provider}")

```

`/Users/sauravverma/programs/pyano/sage/sage/data_manager.py`:

```py
"""Utility classes to maniuplate GitHub repositories."""

import logging
import os
from abc import abstractmethod
from functools import cached_property
from typing import Any, Dict, Generator, Tuple

import requests
from git import GitCommandError, Repo


class DataManager:
    def __init__(self, dataset_id: str):
        self.dataset_id = dataset_id

    @abstractmethod
    def download(self) -> bool:
        """Downloads the data from a remote location."""

    @abstractmethod
    def walk(self) -> Generator[Tuple[Any, Dict], None, None]:
        """Yields a tuple of (data, metadata) for each data item in the dataset."""


class GitHubRepoManager(DataManager):
    """Class to manage a local clone of a GitHub repository."""

    def __init__(
        self,
        repo_id: str,
        commit_hash: str = None,
        access_token: str = None,
        local_dir: str = None,
        inclusion_file: str = None,
        exclusion_file: str = None,
    ):
        """
        Args:
            repo_id: The identifier of the repository in owner/repo format, e.g. "Storia-AI/sage".
            commit_hash: Optional commit hash to checkout. If not specified, we pull the latest version of the repo.
            access_token: A GitHub access token to use for cloning private repositories. Not needed for public repos.
            local_dir: The local directory where the repository will be cloned.
            inclusion_file: A file with a lists of files/directories/extensions to include. Each line must be in one of
                the following formats: "ext:.my-extension", "file:my-file.py", or "dir:my-directory".
            exclusion_file: A file with a lists of files/directories/extensions to exclude. Each line must be in one of
                the following formats: "ext:.my-extension", "file:my-file.py", or "dir:my-directory".
        """
        super().__init__(dataset_id=repo_id)
        self.repo_id = repo_id
        self.commit_hash = commit_hash
        self.access_token = access_token

        self.local_dir = local_dir or "/tmp/"
        if not os.path.exists(self.local_dir):
            os.makedirs(self.local_dir)
        self.local_path = os.path.join(self.local_dir, repo_id)

        self.log_dir = os.path.join(self.local_dir, "logs", repo_id)
        if not os.path.exists(self.log_dir):
            os.makedirs(self.log_dir)

        if inclusion_file and exclusion_file:
            raise ValueError("Only one of inclusion_file or exclusion_file should be provided.")

        self.inclusions = self._parse_filter_file(inclusion_file) if inclusion_file else None
        self.exclusions = self._parse_filter_file(exclusion_file) if exclusion_file else None

    @cached_property
    def is_public(self) -> bool:
        """Checks whether a GitHub repository is publicly visible."""
        response = requests.get(f"https://api.github.com/repos/{self.repo_id}", timeout=10)
        # Note that the response will be 404 for both private and non-existent repos.
        return response.status_code == 200

    @cached_property
    def default_branch(self) -> str:
        """Fetches the default branch of the repository from GitHub."""
        headers = {
            "Accept": "application/vnd.github.v3+json",
        }
        if self.access_token:
            headers["Authorization"] = f"token {self.access_token}"

        response = requests.get(f"https://api.github.com/repos/{self.repo_id}", headers=headers)
        if response.status_code == 200:
            branch = response.json().get("default_branch", "main")
        else:
            # This happens sometimes when we exceed the Github rate limit. The best bet in this case is to assume the
            # most common naming for the default branch ("main").
            logging.warn(f"Unable to fetch default branch for {self.repo_id}: {response.text}")
            branch = "main"
        return branch

    def download(self) -> bool:
        """Clones the repository to the local directory, if it's not already cloned."""
        if os.path.exists(self.local_path):
            # The repository is already cloned.
            return True

        if not self.is_public and not self.access_token:
            raise ValueError(f"Repo {self.repo_id} is private or doesn't exist.")

        if self.access_token:
            clone_url = f"https://{self.access_token}@github.com/{self.repo_id}.git"
        else:
            clone_url = f"https://github.com/{self.repo_id}.git"

        try:
            if self.commit_hash:
                repo = Repo.clone_from(clone_url, self.local_path)
                repo.git.checkout(self.commit_hash)
            else:
                Repo.clone_from(clone_url, self.local_path, depth=1, single_branch=True)
        except GitCommandError as e:
            logging.error("Unable to clone %s from %s. Error: %s", self.repo_id, clone_url, e)
            return False
        return True

    def _parse_filter_file(self, file_path: str) -> bool:
        """Parses a file with files/directories/extensions to include/exclude.

        Lines are expected to be in the format:
        # Comment that will be ignored, or
        ext:.my-extension, or
        file:my-file.py, or
        dir:my-directory
        """
        with open(file_path, "r") as f:
            lines = f.readlines()

        parsed_data = {"ext": [], "file": [], "dir": []}
        for line in lines:
            if line.startswith("#"):
                # This is a comment line.
                continue
            key, value = line.strip().split(":")
            if key in parsed_data:
                parsed_data[key].append(value)
            else:
                logging.error("Unrecognized key in line: %s. Skipping.", line)

        return parsed_data

    def _should_include(self, file_path: str) -> bool:
        """Checks whether the file should be indexed."""
        # Exclude symlinks.
        if os.path.islink(file_path):
            return False

        # Exclude hidden files and directories.
        if any(part.startswith(".") for part in file_path.split(os.path.sep)):
            return False

        if not self.inclusions and not self.exclusions:
            return True

        # Filter based on file extensions, file names and directory names.
        _, extension = os.path.splitext(file_path)
        extension = extension.lower()
        file_name = os.path.basename(file_path)
        dirs = os.path.dirname(file_path).split("/")

        if self.inclusions:
            return (
                extension in self.inclusions.get("ext", [])
                or file_name in self.inclusions.get("file", [])
                or any(d in dirs for d in self.inclusions.get("dir", []))
            )
        elif self.exclusions:
            return (
                extension not in self.exclusions.get("ext", [])
                and file_name not in self.exclusions.get("file", [])
                and all(d not in dirs for d in self.exclusions.get("dir", []))
            )
        return True

    def walk(self, get_content: bool = True) -> Generator[Tuple[Any, Dict], None, None]:
        """Walks the local repository path and yields a tuple of (content, metadata) for each file.
        The filepath is relative to the root of the repository (e.g. "org/repo/your/file/path.py").

        Args:
            get_content: When set to True, yields (content, metadata) tuples. When set to False, yields metadata only.
        """
        # We will keep appending to these files during the iteration, so we need to clear them first.
        repo_name = self.repo_id.replace("/", "_")
        included_log_file = os.path.join(self.log_dir, f"included_{repo_name}.txt")
        excluded_log_file = os.path.join(self.log_dir, f"excluded_{repo_name}.txt")
        if os.path.exists(included_log_file):
            os.remove(included_log_file)
            logging.info("Logging included files at %s", included_log_file)
        if os.path.exists(excluded_log_file):
            os.remove(excluded_log_file)
            logging.info("Logging excluded files at %s", excluded_log_file)

        for root, _, files in os.walk(self.local_path):
            file_paths = [os.path.join(root, file) for file in files]
            included_file_paths = [f for f in file_paths if self._should_include(f)]

            with open(included_log_file, "a") as f:
                for path in included_file_paths:
                    f.write(path + "\n")

            excluded_file_paths = set(file_paths).difference(set(included_file_paths))
            with open(excluded_log_file, "a") as f:
                for path in excluded_file_paths:
                    f.write(path + "\n")

            for file_path in included_file_paths:
                relative_file_path = file_path[len(self.local_dir) + 1 :]
                metadata = {
                    "file_path": relative_file_path,
                    "url": self.url_for_file(relative_file_path),
                }

                if not get_content:
                    yield metadata
                    continue

                contents = self.read_file(relative_file_path)
                if contents:
                    yield contents, metadata

    def url_for_file(self, file_path: str) -> str:
        """Converts a repository file path to a GitHub link."""
        file_path = file_path[len(self.repo_id) + 1 :]
        return f"https://github.com/{self.repo_id}/blob/{self.default_branch}/{file_path}"

    def read_file(self, relative_file_path: str) -> str:
        """Reads the contents of a file in the repository."""
        absolute_file_path = os.path.join(self.local_dir, relative_file_path)
        with open(absolute_file_path, "r") as f:
            try:
                contents = f.read()
                return contents
            except UnicodeDecodeError:
                logging.warning("Unable to decode file %s.", absolute_file_path)
                return None

    def from_args(args: Dict):
        """Creates a GitHubRepoManager from command-line arguments and clones the underlying repository."""
        repo_manager = GitHubRepoManager(
            repo_id=args.repo_id,
            commit_hash=args.commit_hash,
            access_token=os.getenv("GITHUB_TOKEN"),
            local_dir=args.local_dir,
            inclusion_file=args.include,
            exclusion_file=args.exclude,
        )
        success = repo_manager.download()
        if not success:
            raise ValueError(
                f"Unable to clone {args.repo_id}. Please check that it exists and you have access to it. "
                "For private repositories, please set the GITHUB_TOKEN variable in your environment."
            )
        return repo_manager

```

`/Users/sauravverma/programs/pyano/sage/sage/github.py`:

```py
"""GitHub-specific implementations for DataManager and Chunker."""

import logging
from dataclasses import dataclass
from typing import Any, Dict, Generator, List, Tuple

import requests
import tiktoken

from sage.chunker import Chunk, Chunker
from sage.constants import TEXT_FIELD
from sage.data_manager import DataManager

tokenizer = tiktoken.get_encoding("cl100k_base")


@dataclass
class GitHubIssueComment:
    """A comment on a GitHub issue."""

    url: str
    html_url: str
    body: str

    @property
    def pretty(self):
        return f"""## Comment: {self.body}"""


@dataclass
class GitHubIssue:
    """A GitHub issue."""

    url: str
    html_url: str
    title: str
    body: str
    comments: List[GitHubIssueComment]

    @property
    def pretty(self):
        # Do not include the comments.
        return f"# Issue: {self.title}\n{self.body}"


class GitHubIssuesManager(DataManager):
    """Class to manage the GitHub issues of a particular repository."""

    def __init__(self, repo_id: str, access_token: str, index_comments: bool = False, max_issues: int = None):
        super().__init__(dataset_id=repo_id + "/issues")
        self.repo_id = repo_id
        self.index_comments = index_comments
        self.max_issues = max_issues
        self.access_token = access_token
        if not self.access_token:
            raise ValueError("Please set the GITHUB_TOKEN environment variable when indexing GitHub issues.")
        self.issues = []

    def download(self) -> bool:
        """Downloads all open issues from a GitHub repository (including the comments)."""
        per_page = min(self.max_issues or 100, 100)  # 100 is maximum per page
        url = f"https://api.github.com/repos/{self.repo_id}/issues?per_page={per_page}"
        while url:
            logging.info(f"Fetching issues from {url}")
            response = self._get_page_of_issues(url)
            response.raise_for_status()
            for issue in response.json():
                if not "pull_request" in issue:
                    self.issues.append(
                        GitHubIssue(
                            url=issue["url"],
                            html_url=issue["html_url"],
                            title=issue["title"],
                            # When there's no body, issue["body"] is None.
                            body=issue["body"] or "",
                            comments=self._get_comments(issue["comments_url"]) if self.index_comments else [],
                        )
                    )
            if self.max_issues and len(self.issues) >= self.max_issues:
                break
            url = GitHubIssuesManager._get_next_link_from_header(response)
        return True

    def walk(self) -> Generator[Tuple[Any, Dict], None, None]:
        """Yields a tuple of (issue_content, issue_metadata) for each GitHub issue in the repository."""
        for issue in self.issues:
            yield issue, {}  # empty metadata

    @staticmethod
    def _get_next_link_from_header(response):
        """
        Given a response from a paginated request, extracts the URL of the next page.

        Example:
            response.headers.get("link") = '<https://api.github.com/repositories/2503910/issues?per_page=10&page=2>; rel="next", <https://api.github.com/repositories/2503910/issues?per_page=10&page=2>; rel="last"'
            get_next_link_from_header(response) = 'https://api.github.com/repositories/2503910/issues?per_page=10&page=2'
        """
        link_header = response.headers.get("link")
        if link_header:
            links = link_header.split(", ")
            for link in links:
                url, rel = link.split("; ")
                url = url[1:-1]  # The URL is enclosed in angle brackets
                rel = rel[5:-1]  # e.g. rel="next" -> next
                if rel == "next":
                    return url
        return None

    def _get_page_of_issues(self, url):
        """Downloads a single page of issues. Note that GitHub uses pagination for long lists of objects."""
        return requests.get(
            url,
            headers={
                "Authorization": f"Bearer {self.access_token}",
                "X-GitHub-Api-Version": "2022-11-28",
            },
        )

    def _get_comments(self, comments_url) -> List[GitHubIssueComment]:
        """Downloads all the comments associated with an issue; returns an empty list if the request times out."""
        try:
            response = requests.get(
                comments_url,
                headers={
                    "Authorization": f"Bearer {self.access_token}",
                    "X-GitHub-Api-Version": "2022-11-28",
                },
            )
        except requests.exceptions.ConnectTimeout:
            logging.warn(f"Timeout fetching comments from {comments_url}")
            return []
        comments = []
        for comment in response.json():
            comments.append(
                GitHubIssueComment(
                    url=comment["url"],
                    html_url=comment["html_url"],
                    body=comment["body"],
                )
            )
        return comments


@dataclass
class IssueChunk(Chunk):
    """A chunk form a GitHub issue with a contiguous (sub)set of comments.

    Note that, in comparison to FileChunk, its properties are not cached. We want to allow fields to be changed in place
    and have e.g. the token count be recomputed. Compared to files, GitHub issues are typically smaller, so the overhead
    is less problematic.
    """

    issue: GitHubIssue
    start_comment: int
    end_comment: int  # exclusive

    @property
    def content(self) -> str:
        """The title of the issue, followed by the comments in the chunk."""
        if self.start_comment == 0:
            # This is the first subsequence of comments. We'll include the entire body of the issue.
            issue_str = self.issue.pretty
        else:
            # This is a middle subsequence of comments. We'll only include the title of the issue.
            issue_str = f"# Issue: {self.issue.title}"
        # Now add the comments themselves.
        comments = self.issue.comments[self.start_comment : self.end_comment]
        comments_str = "\n\n".join([comment.pretty for comment in comments])
        return issue_str + "\n\n" + comments_str

    @property
    def metadata(self):
        """Converts the chunk to a dictionary that can be passed to a vector store."""
        return {
            "id": f"{self.issue.html_url}_{self.start_comment}_{self.end_comment}",
            "url": self.issue.html_url,
            "start_comment": self.start_comment,
            "end_comment": self.end_comment,
            # Note to developer: When choosing a large chunk size, you might exceed the vector store's metadata
            # size limit. In that case, you can simply store the start/end comment indices above, and fetch the
            # content of the issue on demand from the URL.
            TEXT_FIELD: self.content,
        }

    @property
    def num_tokens(self):
        """Number of tokens in this chunk."""
        return len(tokenizer.encode(self.content, disallowed_special=()))


class GitHubIssuesChunker(Chunker):
    """Chunks a GitHub issue into smaller pieces of contiguous (sub)sets of comments."""

    def __init__(self, max_tokens: int):
        self.max_tokens = max_tokens

    def chunk(self, content: Any, metadata: Dict) -> List[Chunk]:
        """Chunks a GitHub issue into subsequences of comments."""
        del metadata  # The metadata of the input issue is unused.

        issue = content  # Rename for clarity.
        if not isinstance(issue, GitHubIssue):
            raise ValueError(f"Expected a GitHubIssue, got {type(issue)}.")

        chunks = []

        # First, create a chunk for the body of the issue. If it's too long, then truncate it.
        if len(tokenizer.encode(issue.pretty, disallowed_special=())) > self.max_tokens:
            title_len = len(tokenizer.encode(issue.title, disallowed_special=()))
            target_body_len = self.max_tokens - title_len - 20  # 20 for buffer
            trimmed_body = tokenizer.decode(tokenizer.encode(issue.body, disallowed_special=())[:target_body_len])
            trimmed_issue = GitHubIssue(
                url=issue.url,
                html_url=issue.html_url,
                title=issue.title,
                body=trimmed_body,
                comments=issue.comments,
            )
            issue_body_chunk = IssueChunk(trimmed_issue, 0, 0)
        else:
            issue_body_chunk = IssueChunk(issue, 0, 0)

        chunks.append(issue_body_chunk)

        for comment_idx, comment in enumerate(issue.comments):
            # This is just approximate, because when we actually add a comment to the chunk there might be some extra
            # tokens, like a "Comment:" prefix.
            approx_comment_size = len(tokenizer.encode(comment.body, disallowed_special=())) + 20  # 20 for buffer

            if chunks[-1].num_tokens + approx_comment_size > self.max_tokens:
                # Create a new chunk starting from this comment.
                chunks.append(
                    IssueChunk(
                        issue=issue,
                        start_comment=comment_idx,
                        end_comment=comment_idx + 1,
                    )
                )
            else:
                # Add the comment to the existing chunk.
                chunks[-1].end_comment = comment_idx + 1
        return chunks

```

`/Users/sauravverma/programs/pyano/sage/sage/code_symbols.py`:

```py
"""Utilities to extract code symbols (class and method names) from code files."""

import logging
from typing import List, Tuple

from tree_sitter import Node

from sage.chunker import CodeFileChunker


def _extract_classes_and_methods(node: Node, acc: List[Tuple[str, str]], parent_class: str = None):
    """Extracts classes and methods from a tree-sitter node and places them in the `acc` accumulator."""
    if node.type in ["class_definition", "class_declaration"]:
        class_name_node = node.child_by_field_name("name")
        if class_name_node:
            class_name = class_name_node.text.decode("utf-8")
            acc.append((class_name, None))
            for child in node.children:
                _extract_classes_and_methods(child, acc, class_name)
    elif node.type in ["function_definition", "method_definition"]:
        function_name_node = node.child_by_field_name("name")
        if function_name_node:
            acc.append((parent_class, function_name_node.text.decode("utf-8")))
            # We're not going deeper into a method. This means we're missing nested functions.
    else:
        for child in node.children:
            _extract_classes_and_methods(child, acc, parent_class)


def get_code_symbols(file_path: str, content: str) -> List[Tuple[str, str]]:
    """Extracts code symbols from a file.

    Code symbols are tuples of the form (class_name, method_name). For classes, method_name is None. For methods
    that do not belong to a class, class_name is None.
    """
    if not CodeFileChunker.is_code_file(file_path):
        return []

    if not content:
        return []

    logging.info(f"Extracting code symbols from {file_path}")
    tree = CodeFileChunker.parse_tree(file_path, content)
    if not tree:
        return []

    classes_and_methods = []
    _extract_classes_and_methods(tree.root_node, classes_and_methods)
    return classes_and_methods

```